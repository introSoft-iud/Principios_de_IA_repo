# Foro: Transformers y Mecanismo de Atención

Como actividad inicial del curso, organizaremos una tertulia en nuestra primera sesión. Para participar activamente, debes ver la serie sobre Transformers del canal [3Blue1Brown](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi) y estar listo para discutir el funcionamiento de los Modelos de Lenguaje de Gran Escala (LLM) y participar en debates sobre esta arquitectura de red neuronal.
### Desarrollo o ruta metodológica

A continuación, se detallan los temas clave que deberías comprender:

- **Embeddings**: Comprende qué son los embeddings y su papel en la representación de datos en transformers.

- **Tokens**: Familiarízate con el concepto de tokens y su importancia en el procesamiento de datos secuenciales.

- **Búsquedas por Similitud**: Investiga cómo las búsquedas por similitud están relacionadas con la distancia euclidiana y la función coseno, y cómo se aplican en transformers.

- **Mecanismo de Atención y Autoatención**: Entiende el funcionamiento del mecanismo de atención, incluyendo la autoatención.

- **Capas de Perceptrones en un Transformer**: Aprende cómo funcionan las capas de perceptrones dentro de la arquitectura de un transformer.

- **Softmax**: Asegúrate de comprender qué es la función softmax y su aplicación en la normalización de las salidas de los transformers.

### Cómo entregar la actividad

No hay entrega formal; debes asistir al encuentro sincrónico y estar preparado para responder preguntas del profesor y tus compañeros. Prepárate para participar activamente en la discusión, aportando tus ideas y preguntas sobre estos temas.

¡Mucha suerte!