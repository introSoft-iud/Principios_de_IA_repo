
# M√≥dulo 4: Proyecto Integrador: construcci√≥n y depliegue de un sisitema RAG


## Introducci√≥n al m√≥dulo

Bienvenidos al √∫ltima m√≥dulo de nuestro curso sobre aplicaciones asistidas por LLM. En este m√≥dulo aprenderemos sobre document loaders y desplegaremos una aplicaci√≥n RAG funcional para conversar con archivos en PDF. ¬°Comencemos!

## Retival Aumented generation (RAG)

El desempe√±o de un modelo de lenguaje en un t√≥pico particular depende de cu√°nto haya visto datos sobre este t√≥pico en su proceso de entrenamiento. Si un tema no aparece mucho en internet, el modelo tiene una capacidad limitada para responder preguntas sobre este, mientras que si este tema es muy bien difundido, el desempe√±o del modelo en este tipo de temas ser√° mucho mejor. Por otro lado, existe informaci√≥n que el modelo nunca vio, por ejemplo, los contratos de compraventa de una empresa que se dedica a la venta de casas, los decretos y ordenanzas que los gobiernos emitieron despu√©s de la √∫ltima actualizaci√≥n del modelo, etc.

En una RAG, un LLM recupera documentos contextuales a partir de una base de datos externa como parte de su ejecuci√≥n. Esto es √∫til si queremos hacer preguntas sobre documentos espec√≠ficos. La figura muestra el esquema general de una aplicaci√≥n RAG.
![Esquema de flujo de una aplicaci√≥n RAG](../assets/images/rag1.png){ width=" " }
*Esquema de flujo de una aplicaci√≥n RAG. Fuente: [LangChain Documentation](https://python.langchain.com/docs/concepts/rag/).*

El sistema de Recuperaci√≥n (Retrieval) es el encargado de devolver los documentos relevantes para que el LMM (Modelo de Lenguaje de M√°quina) elabore la respuesta a partir de una solicitud. Este sistema est√° compuesto por tres subsistemas:

- **Cargador de Documentos (Document Loader):** carga documentos desde diversas fuentes, como archivos locales, sitios web, bases de datos, etc.
- **Divisor de Texto (Text Splitter):** divide los documentos en fragmentos manejables para su procesamiento.
- **Almac√©n Vectorial (Vector Store):** almacena representaciones vectoriales de los documentos para una recuperaci√≥n eficiente.

La Figura ilustra c√≥mo estos subsistemas interact√∫an en el pipeline de RAG:

![Esquema de los subsistemas acoplados al sistema de Recuperaci√≥n](../assets/images/retrival_subsystems.png){ #fig-retrieval-subsystems }
*Fuente: [Curso "Chat with Your Data" de DeepLearning.AI](https://www.deeplearning.ai/short-courses/chat-with-your-data/).*<!-- Nota para edici√≥n: Por favor construit ua imagen propia-->
## Cargadores de Documentos

Comenzaremos experimentando con algunas de las herramientas de carga de documentos disponibles. Es importante resaltar que existe una gran variedad, como lo ilustra la imagen:

![alt text](../assets/images/doc_loaders.png)
*Fuente: [Curso "Chat with Your Data" de DeepLearning.AI](https://www.deeplearning.ai/short-courses/chat-with-your-data/).*

En esencia, un Cargador de Documentos en LangChain es un componente que obtiene datos de una fuente especificada y los transforma en un formato estandarizado que LangChain puede entender y con el que puede trabajar. Este formato estandarizado es t√≠picamente un objeto Documento.

Una vez que un cargador de documentos ingiere datos, usualmente los transforma en uno o m√°s objetos Documento. Piensa en esto como nuestro contenedor estandarizado para datos de texto. Un objeto Documento en LangChain consiste principalmente en dos atributos clave:

- **page_content:** contiene el contenido de texto real del documento o un fragmento de √©l.

- **metadata (diccionario):** es un diccionario que contiene informaci√≥n adicional sobre el contenido. ¬°Esto es muy importante! Los metadatos pueden incluir informaci√≥n como la siguiente:
    - La fuente del documento (por ejemplo, nombre del archivo, URL, ID de base de datos)
    - Fecha de creaci√≥n, autor
    - N√∫mero de p√°gina (para PDFs)
    - T√≠tulos de secciones espec√≠ficas
    - Cualquier otra informaci√≥n contextual que consideres relevante.

El uso efectivo de metadatos puede mejorar significativamente la capacidad de tu aplicaci√≥n para filtrar, buscar y entender el contexto de la informaci√≥n.

## Categor√≠as y ejemplos de cargadores de documentos

LangChain ofrece un vasto ecosistema de cargadores de documentos, ¬°actualmente m√°s de 80! y sigue a√±adiendo m√°s. Exploremos algunos:

### Cargadores Basados en Archivos:
Estos cargadores leen datos directamente de archivos. Vemos algunos:

#### Documentos PDF

##### `PyPDFLoader` 

Una de las herramientas diponibles para carga de documentos en PDF es  `PyPDFLoader`. Este cargador extrae texto y metadatos de archivos PDF basados en texto, es decir, no es apropiado si el PDF es un escaneo de una imagen. Supongamos que nuestro texto lo tenemos en la carpeta de contenidos `./content`. En este caso, cargaremos el archivo [attention.pdf](../assets/documents/attention.pdf) de la siguiente manera:

```python
from langchain.document_loaders import PyPDFLoader

output_path = "./content/"
file_path = output_path + 'attention.pdf'
loader = PyPDFLoader(file_path)  # Instancia del cargador.
pages = loader.load()
```
La l√≠nea `pages = loader.load()` ejecuta el m√©todo `.load()`, que lee el PDF y devuelve una lista de objetos `Document`. Cada `Document` representa una p√°gina del PDF, con `page_content` (el texto extra√≠do) y `metadata` (informaci√≥n como la fuente y el n√∫mero de p√°gina).

`pages` ser√° una lista de objetos `Document`.  
Cada `Document` tendr√°:
- `page_content`: El texto extra√≠do de una p√°gina del PDF.
- `metadata`: Un diccionario con informaci√≥n como `{"source": "./content/attention.pdf", "page": n}` (donde `n` es el n√∫mero de p√°gina, comenzando desde 0).

Podemos inspeccionar el contenido cargado:

=== "C√≥digo"
    ```python
    pages
    ```
=== "Salida"
    ```bash
    [<Document page_content="Texto de la p√°gina 1" metadata={"source": "./content/attention.pdf", "page": 0}>,
    <Document page_content="Texto de la p√°gina 2" metadata={"source": "./content/attention.pdf", "page": 1}>,
    ...]
    ```
    ![alt text](../assets/images/pages.png)
As√≠, por ejemplo, podremos acceder al contenido cargado de la primera p√°gina haciendo:

=== "C√≥digo"
    ```python
    first_page = pages[0]
    print("Contenido de la primera p√°gina:")
    print(first_page.page_content)
    ```

=== "Salida"
    ```bash
    Contenido de la primera p√°gina:
    Provided proper attribution is provided, Google hereby grants permission to
    reproduce the tables and figures in this paper solely for use in journalistic or
    scholarly works.
    Attention Is All You Need
    Ashish Vaswani‚àó
    Google Brain
    avaswani@google.com
    Noam Shazeer‚àó
    Google Brain
    noam@google.com
    Niki Parmar‚àó
    Google Research
    nikip@google.com
    Jakob Uszkoreit‚àó
    Google Research
    usz@google.com
    Llion Jones‚àó
    Google Research
    llion@google.com
    Aidan N. Gomez‚àó ‚Ä†
    University of Toronto
    aidan@cs.toronto.edu
    ≈Åukasz Kaiser‚àó
    Google Brain
    ...
    ‚Ä†Work performed while at Google Brain.
    ‚Ä°Work performed while at Google Research.
    31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.
    arXiv:1706.03762v7  [cs.CL]  2 Aug 2023
    ```
!!! tip "üìñ Para aprender m√°s"
    Puedes conocer m√°s cargadores de documentos PDF consultando la documentaci√≥n de LangChain en:
    [How to: load PDF files.](https://python.langchain.com/docs/integrations/document_loaders/#pdfs)

##### Unstructured

El cargador de documentos Unstructured se utiliza para cargar archivos de muchos tipos. Actualmente, Unstructured admite la carga de archivos de texto, presentaciones de PowerPoint, HTML, PDFs, im√°genes y m√°s.

El paquete Unstructured de Unstructured.IO extrae texto limpio de documentos fuente como PDFs y documentos de Word.

!!! warning "Para tener en cuenta"
    La API de Unstructured requiere claves de API para realizar solicitudes para opciones m√°s avanzadas. Puedes solicitar una clave de API [aqu√≠](https://unstructured.io/enterprise) y comenzar a usarla.

Para ilustrar su uso, usaremos el mismo archivo PDF del ejemplo anterior:

```python
from unstructured.partition.pdf import partition_pdf

output_path = ".content/"
file_path = output_path + 'attention.pdf'
```

En su forma m√°s b√°sica, sin ninguna configuraci√≥n adicional, podemos cargar el documento como:

```python
# Carga y procesa el pdf con la configuraci√≥n b√°sica
chunks = partition_pdf(filename=file_path)
```

=== "Salida"
    ```bash
    [<unstructured.documents.elements.Text at 0x702efe8e0a60>,
    <unstructured.documents.elements.NarrativeText at 0x702efe8e0ac0>,
    ...
    ```

Visualicemos los documentos extra√≠dos:

```python
# Muestra los elementos extra√≠dos
for chunk in chunks:
    print(chunk)
```

=== "Salida"
    ```bash
    3 2 0 2
    g u A 2
    ] L C . s c [
    ...
    ```

!!! tip "üìñ Para aprender m√°s"
    Puedes explorar las diferentes funcionalidades de carga en el siguiente enlace:
    [Documentaci√≥n de Unstructured](https://docs.unstructured.io/open-source/core-functionality/chunking)

Para un mayor control sobre la forma en que podemos extraer los diferentes tipos de datos del PDF, podemos configurar la funci√≥n `partition_pdf` de la siguiente manera:

```python
chunks = partition_pdf(
    filename=file_path,
    infer_table_structure=True,            # Extraer tablas
    strategy="hi_res",                     # Necesario para inferir tablas

    extract_image_block_types=["Image"],   # Agregar 'Table' para extraer im√°genes de tablas
    # image_output_dir_path=output_path,   # Si es None, las im√°genes y tablas se guardar√°n en base64 en memoria

    extract_image_block_to_payload=True,   # Si es True, extraer√° base64 para uso en API

    chunking_strategy="by_title",          # O 'basic'
    max_characters=10000,                  # Por defecto es 500
    combine_text_under_n_chars=2000,       # Por defecto es 0
    new_after_n_chars=6000,

    # extract_images_in_pdf=True,          # Obsoleto
)
```
Aqu√≠, la funci√≥n `partition_pdf` procesa el archivo PDF extrayendo no solo texto, sino tambi√©n im√°genes y tablas. La funci√≥n divide el PDF en partes manejables, conocidas como "chunks", y permite ajustar la forma en que se extraen los diferentes tipos de contenido. En particular, se infiere la estructura de las tablas, se extraen bloques de im√°genes, y se define c√≥mo dividir el contenido en chunks bas√°ndose en t√≠tulos y l√≠mites de caracteres. Las im√°genes extra√≠das se codifican en base64 para facilitar su transmisi√≥n o almacenamiento.

!!! tip "üìñ Para aprender m√°s"
    El formato base64 es un m√©todo de codificaci√≥n que convierte datos binarios en texto ASCII, permitiendo que los datos sean f√°cilmente transmitidos a trav√©s de medios que solo soportan texto. Puedes aprender m√°s sobre el formato base64 en la [documentaci√≥n de Wikipedia](https://es.wikipedia.org/wiki/Base64).
Verifiquemos el contenido extra√≠do:

```python
chunks
```

=== "Salida"
    ```bash
    [<unstructured.documents.elements.CompositeElement at 0x702efeb07670>,
     <unstructured.documents.elements.CompositeElement at 0x702e358f2be0>,
     <unstructured.documents.elements.CompositeElement at 0x702efeb07be0>,
     <unstructured.documents.elements.CompositeElement at 0x702efeb07f40>,
     ...
    ```

Con esta configuraci√≥n, obtenemos dos tipos de elementos:

```python
set([str(type(el)) for el in chunks])
```

=== "Salida"
    ```bash
    {"<class 'unstructured.documents.elements.CompositeElement'>",
     "<class 'unstructured.documents.elements.Table'>"}
    ```

Un `CompositeElement` generalmente contiene m√∫ltiples elementos secundarios de varios tipos, como texto, im√°genes o tablas. Esto permite que la biblioteca gestione secciones de un documento compuestas por diferentes tipos de contenido como una sola entidad.

En el procesamiento de documentos, es com√∫n encontrar secciones que incluyen una mezcla de texto, im√°genes y otros elementos que l√≥gicamente son parecidos. 
Un `CompositeElement` puede representar dichas secciones. Cada objeto `CompositeElement` tiene un atributo `metadata`, que es una instancia de la clase `ElementMetadata`. Este atributo contiene informaci√≥n adicional sobre el chunk, como n√∫meros de p√°gina, detalles del archivo o datos estructurales. Por ejemplo:

```python
chunks[3].metadata.orig_elements
```

=== "Salida"
    ```bash
    [<unstructured.documents.elements.Title at 0x702e35e7ed60>,
     <unstructured.documents.elements.NarrativeText at 0x702e35e7edf0>,
     <unstructured.documents.elements.Footer at 0x702e0cedd940>,
     <unstructured.documents.elements.Image at 0x702e0ceddd00>,
     <unstructured.documents.elements.Image at 0x702e0ceddac0>,
     <unstructured.documents.elements.NarrativeText at 0x702e0cedde50>,
     <unstructured.documents.elements.NarrativeText at 0x702e0d38a730>,
     <unstructured.documents.elements.Title at 0x702e0d38aca0>,
     <unstructured.documents.elements.NarrativeText at 0x702e0d38adc0>,
     <unstructured.documents.elements.NarrativeText at 0x702e0d16a7c0>,
     <unstructured.documents.elements.Formula at 0x702e0d16a0a0>,
     <unstructured.documents.elements.NarrativeText at 0x702e0d16a610>,
     <unstructured.documents.elements.NarrativeText at 0x702e0d433d30>]
    ```

Aqu√≠, `orig_elements` es un campo espec√≠fico dentro de la metadata que almacena una referencia a los elementos originales, no divididos, a partir de los cuales se cre√≥ el chunk. Generalmente, es una lista de objetos `Element` que fueron combinados o procesados para formar el chunk actual. En este chunk particular, tenemos los siguientes elementos:

- T√≠tulo
- Texto narrativo
- Pie de p√°gina
- Imagen
- F√≥rmula

Separemos las im√°genes en el chunk:

```python
elements = chunks[3].metadata.orig_elements

chunks_images = [el for el in elements if "Image" in str(type(el))]
chunks_images
```

=== "Salida"
```bash
[<unstructured.documents.elements.Image at 0x702e0ceddd00>,
 <unstructured.documents.elements.Image at 0x702e0ceddac0>]
```

La siguiente funci√≥n extraer√° todas las im√°genes del documento en una lista:


```python
# Extraer las im√°genes de los CompositeElements

def get_images_base64(chunks):
    images_b64 = []
    for chunk in chunks:
        if "CompositeElement" in str(type(chunk)):  # Filtramos los CompositeElements
            chunks_els = chunk.metadata.orig_elements
            for el in chunks_els:
                if "Image" in str(type(el)):
                    images_b64.append(el.metadata.image_base64)
    return images_b64
```
Y podemos verificar la extracci√≥n visualiz√°ndola:

```python
import base64
from IPython.display import Image, display

def display_base64_image(base64_code):
    image_data = base64.b64decode(base64_code)
    display(Image(data=image_data))

images = get_images_base64(chunks)
display_base64_image(images[0])
```

=== "Salida"
    ![alt text](../assets/images/base64.png)


### Web Based

Veamos ahora un ejemplo corto de carga de un documento web:

```python
from langchain.document_loaders import WebBaseLoader

loader = WebBaseLoader("https://concepto.de/reino-fungi/")
docs = loader.load()
print(docs[0].page_content[:500])
```

=== "Salida"
```bash
Reino Fungi - Concepto, tipos, caracter√≠sticas y ejemplos

...

Arte
Conocimiento
C
```

Este tipo de cargadores dejan como espacios en blanco los objetos que no son texto, como im√°genes o tablas. Por lo tanto, requeriremos conectar otras herramientas que nos permitan hacer un pos-procesamiento adecuado.
Una caracter√≠stica de `WebBaseLoader` es que permite cargar varias URLs a la vez. Vamos a cargar contenido de dos p√°ginas relacionadas:

```python
# Lista de URLs a cargar
urls = [
    "https://es.wikipedia.org/wiki/Inteligencia_artificial",
    "https://es.wikipedia.org/wiki/Aprendizaje_autom%C3%A1tico"
]

# Cargamos m√∫ltiples p√°ginas
multi_loader = WebBaseLoader(urls)
multi_documents = multi_loader.load()

# Mostramos el n√∫mero de documentos cargados y un extracto de cada uno
print(f"Se cargaron {len(multi_documents)} documentos.")
for i, doc in enumerate(multi_documents):
    print(f"Documento {i + 1} (primeros 200 caracteres):")
    print(doc.page_content[:200])
```

=== "Salida"
```bash
Se cargaron 2 documentos.
Documento 1 (primeros 200 caracteres):

Inteligencia artificial - Wikipedia, la enciclopedia libre

...

El algoritmo aprende observando el mundo que le rodea. Su informaci√≥n de entrada es la retroalimentaci√≥n que obtiene del mundo exterior como respuesta a sus acciones. Por lo tanto, el sistema aprende a base de ensayo-error.

El aprendizaje por refuerzo es el m√°s general entre las tres categor√≠as. En vez de que un instructor indique al agente qu√© hacer, el agente inteligente debe aprender c√≥mo se comporta el entorno mediante recompensas (refuerzos) o castigos, derivados del √©xito o del fracaso respectivamente. El objetivo principal es aprender la funci√≥n de valor que le ayude al agente inteligente a maximizar la se√±al de recompensa y as√≠ optimizar sus pol√≠ticas de modo a comprender el comportamiento del entorno y a tomar buenas decisiones para el logro de sus objetivos formales.

Los principales algoritmos de aprendizaje por refuerzo se desarrollan dentro de los m√©todos de resoluci√≥n de problemas de decisi√≥n finitos de Markov, que incorporan las ecuaciones de Bellman y las funciones de valor. Los tres m√©todos principales son: la programaci√≥n din√°mica, los m√©todos de Monte Carlo y el aprendizaje de diferencias temporales.

Entre las implementaciones desarrolladas est√° AlphaGo, un programa de IA desarrollado por Google DeepMind para jugar el juego de mesa Go. En marzo de 2016 AlphaGo le gan√≥ una partida al jugador profesional Lee Se-Dol que tiene la categor√≠a noveno dan y 18 t√≠tulos mundiales. Entre los algoritmos que utiliza se encuentra el √°rbol de b√∫squeda Monte Carlo, tambi√©n utiliza aprendizaje profundo con redes neuronales. Puede ver lo ocurrido en el documental de Netflix ‚ÄúAlp
```
#### YouTube

La API `YouTubeTranscriptApi` es una herramienta √∫til para extraer transcripciones de videos de YouTube. Esta API permite obtener los subt√≠tulos de un video en diferentes idiomas, facilitando el an√°lisis de contenido de video de manera program√°tica. A continuaci√≥n, ilustraremos su uso utilizando el video de YouTube sobre  [retropropagaci√≥n ](https://www.youtube.com/watch?v=kbGu60QBx2o).

```python
from youtube_transcript_api import YouTubeTranscriptApi

# Extraemos el ID del video desde la URL
video_id = "kbGu60QBx2o"  # ID de https://www.youtube.com/watch?v=kbGu60QBx2o (Backpropagation: Data Science Concepts)

try:
    transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=['es', 'en'])
    print("Primeros 500 caracteres del transcrito manual:")
    print(" ".join([entry['text'] for entry in transcript])[:500])
except Exception as e:
    print(f"Error al obtener el transcrito manualmente: {str(e)}")
```

=== "Salida"
```bash
[Music] hey everyone in this video we're going to do a followup to our initial video on neural networks and this video is going to be on back propagation now I'm making this video on back propagation mostly because it's a really difficult concept to understand at least it was for me I read through multiple blog posts and watch videos and everyone seems to have their own kind of way of understanding and explaining it and it was difficult for me to match up all those understandings into what this 
```
## Text Splitters

Una vez hemos cargado el o los documentos sobre los cuales queremos realizar RAG , debemos separarlos en fragmentos sobre los cuales crearemos nuestra base de datos de embeddings, es decir, una base de datos vectorial (ver [Figura 2](#fig-retrieval-subsystems)). La herramienta que nos permite hacer esto son los llamados **Text Splitters**.
LangChain ofrece varias implementaciones de **Text Splitters** para dividir texto bas√°ndose en:

- **Caracteres**  
- **Tokens**  
- **L√≠neas**  
- **Oraciones**  

Por ejemplo, el **CharacterTextSplitter** divide el texto en funci√≥n de caracteres, mientras que el **RecursiveCharacterTextSplitter** intenta dividir por varios delimitadores (como `\n` o espacios) hasta encontrar el tama√±o adecuado.

En general, el texto completo ser√° dividido seg√∫n un par√°metro que limita el tama√±o del fragmento (`chunk_size`) y otro par√°metro que le indica al text splitter el nivel de superposici√≥n de textos en dos fragmentos contiguos (`chunk_overlap`). El solapamiento es deseable para garantizar que las ideas que no est√©n completas en un fragmento queden completas en el siguiente, como lo ilustra la figura:

![alt text](../assets/images/chunck_overlap.png)

Para usar el `RecursiveCharacterTextSplitter` y el `CharacterTextSplitter`, cargaremos el m√≥dulo:

```python
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
```

### RecursiveCharacterTextSplitter

Dividamos un texto usando un tama√±o de fragmento de 26 caracteres y un solapamiento de 4 caracteres.

```python
chunk_size = 26
chunk_overlap = 4

# Instanciamos los dos splitters
r_splitter = RecursiveCharacterTextSplitter(
    chunk_size=chunk_size,
    chunk_overlap=chunk_overlap
)

c_splitter = CharacterTextSplitter(
    chunk_size=chunk_size,
    chunk_overlap=chunk_overlap
)
```
Usemos el divisor en la cadena de texto:

```python
text1 = 'abcdefghijklmnopqrstuvwxyz'
r_splitter.split_text(text1)
```

=== "Salida"
    ```bash
    ['abcdefghijklmnopqrstuvwxyz']
    ```

Como el texto es demasiado corto, el splitter solo genera un fragmento. Un texto un poco m√°s largo, como en:

```python
text2 = 'abcdefghijklmnopqrstuvwxyznopqrstuvwxyz'
r_splitter.split_text(text2)
```

Dividir√° el texto en dos fragmentos.

=== "Salida"
    ```bash
    ['abcdefghijklmnopqrstuvwxyz', 'wxyznopqrstuvwxyz']
    ```
Ahora hag√°moslo con el splitter de caracteres:

```python
c_splitter.split_text(text3)
```

Lo que vemos es que no trata de dividirlo. ¬øQu√© est√° pasando? Debemos escoger el car√°cter separador. Por ejemplo, `separator = ' '`.

```python
c_splitter = CharacterTextSplitter(
    chunk_size=chunk_size,
    chunk_overlap=chunk_overlap,
    separator=' '
)
c_splitter.split_text(text3)
```

=== "Salida"
    ```bash
    ['a b c d e f g h i j k l m', 'l m n o p q r s t u v w x', 'w x y z']
    ```

Vemos otro ejemplo:
```python
# Reemplaza la variable some_text por otro texto del mismo tama√±o sobre lo que opinaba Einstein sobre termodin√°mica
some_text = """Albert Einstein consideraba que la termodin√°mica era 
una de las teor√≠as m√°s fundamentales y s√≥lidas de la f√≠sica. En sus
 propias palabras, dec√≠a que la termodin√°mica era la √∫nica teor√≠a 
 f√≠sica que √©l estaba convencido de que nunca ser√≠a refutada, dentro 
 del marco de aplicabilidad de sus conceptos b√°sicos. Esto resaltaba 
 su profundo respeto por la capacidad de la termodin√°mica para describir 
 fen√≥menos naturales con precisi√≥n. Einstein ve√≠a en la termodin√°mica una
  belleza que se derivaba de su simplicidad y universalidad, y la consideraba
   una piedra angular en el entendimiento cient√≠fico del mundo f√≠sico."""

len(some_text)
```

=== "Salida"
```bash
625
```

```python
c_splitter = CharacterTextSplitter(
    chunk_size=450,
    chunk_overlap=0,
    separator=' '
)
r_splitter = RecursiveCharacterTextSplitter(
    chunk_size=450,
    chunk_overlap=0,
    separators=["\n\n", "\n", " ", ""]
)
```

Cuando especificamos `separators=["\n\n", "\n", " ", ""]`, significa que el splitter usa estos separadores en ese orden hasta que encuentra un tipo de separador que le sirva.

```python
c_splitter.split_text(some_text)
from IPython.display import Markdown, display
print(c_splitter.split_text(some_text))
display(Markdown(c_splitter.split_text(some_text)[0]))
display(Markdown(c_splitter.split_text(some_text)[1]))
```

=== "Salida"
![alt text](../assets/images/recursive.png)
<div class="grid cards" markdown>

- :fontawesome-solid-gears:{ .lg .middle } **Reto formativo**  
  **Planteamiento**:
  Utiliza PyPDFLoader para cargar el documento [attention.pdf](../assets/documents/attention.pdf) u otro de tu preferencia.
  * Usa `CharacterTextSplitter` y experimenta con diferentes par√°metros, por ejemplo: 
    ```python
    separator="\n",
    chunk_size=1000,
    chunk_overlap=150,
    length_function=len
    ```
  * Visualiza el n√∫mero de documentos cargados.
  * ¬øCu√°ntas p√°ginas tiene el documento?
  
</div>
## Almacenamiento en la base de datos vectorial

El paso siguiente en la construcci√≥n de nuestro RAG (ver figura 2) consiste en crear representaciones vectoriales de los fragmentos. Las representaciones vectoriales de texto, o *embeddings*, son creadas a partir de modelos de lenguaje (LLM). Un *embedding* se genera utilizando un LLM que convierte texto en vectores num√©ricos, capturando el contexto y el significado sem√°ntico del texto. El flujo es mostrado en la figura:

![alt text](image-1.png)

!!! Tip "Para aprender m√°s"
    Las bases de datos vectoriales son sistemas especializados en el almacenamiento, indexaci√≥n y recuperaci√≥n eficiente de vectores de alta dimensi√≥n. Estos vectores, que com√∫nmente representan caracter√≠sticas num√©ricas extra√≠das de datos como texto, im√°genes, audio o video, permiten realizar b√∫squedas basadas en similitud (por ejemplo, utilizando distancia euclidiana, coseno o HNSW). A diferencia de las bases de datos tradicionales orientadas a registros, las bases de datos vectoriales est√°n optimizadas para operaciones de b√∫squeda aproximada de vecinos m√°s cercanos (Approximate Nearest Neighbor Search, ANNS), lo que las hace ideales para tareas de recuperaci√≥n sem√°ntica, sistemas de recomendaci√≥n, reconocimiento de patrones y aplicaciones en inteligencia artificial.

    Estos sistemas son clave en entornos de aprendizaje autom√°tico y procesamiento de lenguaje natural (PLN), donde se requiere comparar representaciones vectoriales de datos embebidos (embeddings). Algunas implementaciones populares incluyen FAISS (Facebook AI Similarity Search), Annoy (Spotify), Milvus y Weaviate, que ofrecen distintos enfoques y estructuras para manejar escalabilidad, latencia y precisi√≥n.

    Para profundizar en el tema, se recomienda la siguiente referencia t√©cnica:  
    **Johnson, J., Douze, M., & J√©gou, H. (2019). [Billion-scale similarity search with GPUs](https://arxiv.org/abs/1702.08734)**. *arXiv preprint arXiv:1702.08734*.
### Embeddings

Los embeddings son las representaciones vectoriales de los fragmentos de texto entregados por el `TextSplitter`. Textos similares tienen representaciones similares en el espacio de embeddings, lo que significa que podemos comparar estos vectores y encontrar fragmentos de texto que son similares o relevantes dada una consulta. Crearemos nuestros embeddings usando `OpenAIEmbeddings`.

Veamos su funcionamiento con un ejemplo sencillo:
```python
from dotenv import load_dotenv
import os
from langchain_openai import OpenAIEmbeddings
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

# Cargar variables de entorno (por ejemplo, OPENAI_API_KEY)
load_dotenv()

# Definir oraciones menos creativas con temas de comida/cocina y clima
sentence1 = "I like pasta."
sentence2 = "I like noodles."
sentence3 = "The soup I made is bad."
sentence4 = "The sky is blue."

# Lista de todas las oraciones
sentences = [sentence1, sentence2, sentence3, sentence4]

# Inicializar embeddings de OpenAI
embeddings_model = OpenAIEmbeddings() 

# Generar embeddings para todas las oraciones
embeddings = embeddings_model.embed_documents(sentences)

# Convertir embeddings a un array de numpy para el c√°lculo de similitud
embeddings_array = np.array(embeddings)

# Calcular la similitud coseno entre todos los pares de oraciones
similarity_matrix = cosine_similarity(embeddings_array)

# Imprimir puntuaciones de similitud
print("Cosine Similarity Matrix:")
print("Sentences:")
for i, sentence in enumerate(sentences):
    print(f"{i+1}. {sentence}")
print("\nSimilarity Scores:")
for i in range(len(sentences)):
    for j in range(i + 1, len(sentences)):
        print(f"Similarity between '{sentences[i][:50]}...' and '{sentences[j][:50]}...': {similarity_matrix[i][j]:.4f}")
```

=== "Salida"
```bash
Cosine Similarity Matrix:
Sentences:
1. I like pasta.
2. I like noodles.
3. The soup I made is bad.
4. The sky is blue.

Similarity Scores:
Similarity between 'I like pasta....' and 'I like noodles....': 0.9300
Similarity between 'I like pasta....' and 'The soup I made is bad....': 0.6500
Similarity between 'I like pasta....' and 'The sky is blue....': 0.6100
Similarity between 'I like noodles....' and 'The soup I made is bad....': 0.6400
Similarity between 'I like noodles....' and 'The sky is blue....': 0.6000
Similarity between 'The soup I made is bad....' and 'The sky is blue....': 0.6700
```
Con mayor similitud entre la oraci√≥n 3 y la oraci√≥n 4 debido a que ambas est√°n relacionadas con el clima, e incluso una similitud mayor entre las oraciones 1 y 2 ya que est√°n relacionadas con comidas italianas.

El modelo `text-embedding-ada-002` es un Transformer optimizado para tareas de embeddings, entrenado en grandes cantidades de datos de texto para capturar relaciones sem√°nticas.

!!! warning "Para tener en cuenta"
    El modelo `text-embedding-ada-002` de OpenAI es un Transformer optimizado para tareas de embeddings, entrenado en grandes cantidades de datos de texto para capturar relaciones sem√°nticas. El texto de entrada (por ejemplo, "I like pasta.") se tokeniza, dividiendo las palabras o subpalabras en unidades (tokens) que el modelo entiende.

    Cada token se convierte en un vector inicial (word embedding) basado en un vocabulario aprendido durante el entrenamiento.

    Un Transformer procesa los tokens a trav√©s de m√∫ltiples capas de redes neuronales.

    Las capas de atenci√≥n capturan la importancia de cada token en relaci√≥n con los dem√°s, es decir, el contexto. Por ejemplo, en "I like pasta.", "pasta" se interpreta en el contexto de "like", lo que da un significado positivo.

    Esto produce representaciones contextuales que reflejan no solo las palabras individuales, sino tambi√©n su relaci√≥n en la frase. Las capas finales del modelo combinan las representaciones contextuales en un solo vector fijo (por ejemplo, 1536 dimensiones para `text-embedding-ada-002`).

    Este vector es una representaci√≥n densa del significado sem√°ntico del texto, donde textos con significados similares (por ejemplo, "I like pasta." y "I like noodles.") tienen vectores cercanos en el espacio.

Hemos usado el producto escalar para compararlos, ya que este es proporcional a la distancia entre los vectores. Recordemos que dados dos vectores $\mathbf{A}$ y $\mathbf{B}$, el producto escalar est√° definido como $\mathbf{A} \cdot \mathbf{B} = AB \cos(\theta)$, como $\cos(0) = 1$ entonces los vectores son m√°s cercanos entre m√°s pr√≥ximo est√© este n√∫mero de 1.
### Creando la Vectorstore

Existen diversas opciones de almacenamiento vectorial, cada una con caracter√≠sticas espec√≠ficas. Algunas de las opciones m√°s populares incluyen:

- **Pinecone**: Ofrece un servicio escalable y r√°pido para almacenar y buscar vectores, ideal para aplicaciones en la nube.
- **Faiss**: Un framework de Facebook AI Research que es eficiente para b√∫squedas de similitud y clustering, especialmente √∫til para grandes cantidades de datos.
- **Annoy**: Desarrollado por Spotify, es adecuado para b√∫squedas aproximadas en grandes datasets, optimizando el uso de memoria.
- **Chroma**: Es una opci√≥n vers√°til que puede funcionar tanto localmente como en la nube, facilitando la integraci√≥n con aplicaciones que requieren b√∫squedas r√°pidas y eficientes.

En este ejemplo, utilizaremos **Chroma** debido a su flexibilidad y facilidad de uso cuando se implementa localmente. Chroma permite almacenar los embeddings generados y realizar b√∫squedas de similitud de manera eficiente, lo cual es ideal para aplicaciones que requieren procesamiento r√°pido sin depender de servicios externos.

Volvemos al ejemplo del PDF (`attention.pdf`). Lo cargaremos con `PyPDFLoader` y lo dividiremos en fragmentos usando `RecursiveCharacterTextSplitter`.

```python
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

# Cargamos el PDF
output_path = ".content/"
file_path = output_path + "attention.pdf"
loader = PyPDFLoader(file_path)
documents = loader.load()

# Dividimos en fragmentos
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,  # Tama√±o de cada fragmento
    chunk_overlap=50  # Solapamiento entre fragmentos
)
split_docs = text_splitter.split_documents(documents)

# Verificamos cu√°ntos fragmentos tenemos
print(f"Se generaron {len(split_docs)} fragmentos del PDF.")
```

```bash
Se generaron 93 fragmentos del PDF.
```

A continuaci√≥n, crearemos los embeddings.

```python
from langchain_openai import OpenAIEmbeddings

# Configuramos el modelo de embeddings de OpenAI
embedding_model = OpenAIEmbeddings()  # Usa la variable de entorno OPENAI_API_KEY

# Probamos con un fragmento para verificar
sample_chunk = split_docs[0].page_content
sample_embedding = embedding_model.embed_query(sample_chunk)
print(f"Dimensi√≥n del embedding: {len(sample_embedding)}")
print(f"Primeros 5 valores: {sample_embedding[:5]}")
```

```bash
Dimensi√≥n del embedding: 1536
Primeros 5 valores: [-0.012028052471578121, 0.01199324894696474, 0.017206797376275063, -0.027745263651013374, -0.0014739236794412136]
```
### Creando la base de datos

Guardaremos los fragmentos y sus embeddings generados por OpenAI en una vectorstore de `Chroma`.

```python
from langchain.vectorstores import Chroma

# Creamos la vectorstore con Chroma
vectorstore = Chroma.from_documents(
    documents=split_docs,  # Los fragmentos del PDF
    embedding=embedding_model,  # Embeddings de OpenAI
    persist_directory="./chroma_db_openai"  # Directorio para esta versi√≥n
)

# Verificamos cu√°ntos documentos se almacenaron
print(f"Se almacenaron {vectorstore._collection.count()} fragmentos en la vectorstore.")
```

```bash
Se almacenaron 93 fragmentos en la vectorstore.
```

Con la vectorstore lista, podemos buscar fragmentos relevantes para una consulta como "attention mechanism".

```python
# Realizamos una b√∫squeda sem√°ntica
query = "attention mechanism"
results = vectorstore.similarity_search(query, k=3)  # Top 3 fragmentos m√°s similares

# Mostramos los resultados
for i, result in enumerate(results):
    print(f"Resultado {i + 1}:")
    print(result.page_content)
    print(f"Metadatos: {result.metadata}")
    print("-" * 50)
```

```bash
Resultado 1:
tion models in various tasks, allowing modeling of dependencies without regard to their distance in
the input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms
are used in conjunction with a recurrent network.
In this work we propose the Transformer, a model architecture eschewing recurrence and instead
relying entirely on an attention mechanism to draw global dependencies between input and output.
Metadatos: {'page': 1, 'page_label': '2', 'source': '.content/attention.pdf'}
--------------------------------------------------
Resultado 2:
but
its
application
should
be
just
-
this
is
what
we
are
missing
,
in
my
...
```

### Persistencia de la Vectorstore

La base de datos se guarda en `./chroma_db_openai`. Para cargarla en otros scripts, de esta manera no tendremos que pagar por los embeddings cada vez que un script requiera realizar consultas sobre el documento.

```python
# Cargar la vectorstore existente
loaded_vectorstore = Chroma(
    persist_directory="./chroma_db_openai",
    embedding_function=embedding_model
)

# Verificamos que se carg√≥
print(f"Fragmentos cargados desde disco: {loaded_vectorstore._collection.count()}")
```

```bash
Fragmentos cargados desde disco: 93
```

Veamos:

```python
query = "embeddings"
results = loaded_vectorstore.similarity_search(query, k=3)  # Top 3 fragmentos m√°s similares

# Mostramos los resultados
for i, result in enumerate(results):
    print(f"Resultado {i + 1}:")
    print(result.page_content)
    print(f"Metadatos: {result.metadata}")
    print("-" * 50)
```

```bash
Resultado 1:
Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint
arXiv:1703.03130, 2017.
[23] Minh-Thang Luong, Quoc V . Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task
sequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.
[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-
based neural machine translation. arXiv preprint arXiv:1508.04025, 2015.
11
Metadatos: {'page': 10, 'page_label': '11', 'source': '.content/attention.pdf'}
--------------------------------------------------
Resultado 2:
2006.
[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv
preprint arXiv:1608.05859, 2016.
[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words
with subword units. arXiv preprint arXiv:1508.07909, 2015.
[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,
and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts
Metadatos: {'page': 11, 'page_label': '12', 'source': '.content/attention.pdf'}
--------------------------------------------------
...
```

## RetrievalQA

Combinaremos la vectorstore con un modelo de lenguaje de OpenAI (`gpt-3.5-turbo` por defecto) para responder preguntas. `RetrievalQA` buscar√° fragmentos relevantes en la vectorstore y los usar√° como contexto para generar respuestas.

```python
from langchain.chains import RetrievalQA
from langchain.chat_models import ChatOpenAI

# Configuramos el modelo de lenguaje
llm = ChatOpenAI(
    model_name="gpt-3.5-turbo",
    temperature=0  # Respuestas m√°s precisas y menos creativas
)

# Creamos la cadena RetrievalQA
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",  # Usa todos los fragmentos relevantes directamente
    retriever=vectorstore.as_retriever(search_kwargs={"k": 3}),  # Top 3 fragmentos m√°s similares
    return_source_documents=True  # Devuelve los fragmentos usados como fuente
)
```
## Preguntas al PDF

Ahora podemos preguntar algo sobre el contenido del PDF:

=== "C√≥digo"
    ```python
    query2 = "¬øC√≥mo se usa la atenci√≥n?"
    result2 = qa_chain({"query": query2})

    # Mostramos la respuesta
    print("Respuesta:")
    print(result2["result"])
    print("Fragmentos utilizados como fuente:")
    for i, doc in enumerate(result2["source_documents"]):
        print(f"Fuente {i + 1}:")
        print(doc.page_content)
        print(f"Metadatos: {doc.metadata}")
        print("-" * 50)
    ```
=== "Salida"
    ```bash
    Respuesta:
    La atenci√≥n se utiliza en el contexto de modelos de aprendizaje autom√°tico, como en el caso de la atenci√≥n en el mecanismo de atenci√≥n de un modelo de lenguaje. La atenci√≥n se aplica para que el modelo pueda enfocarse en partes espec√≠ficas de la entrada durante el proceso de aprendizaje y toma de decisiones.

    Fragmentos utilizados como fuente:
    Fuente 1:
    but
    its
    application
    should
    be
    just
    -
    this
    is
    what
    we
    are
    missing
    ,
    in
    my
    opinion
    .
    <EOS>
    <pad>
    Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the
    sentence. We give two such examples above, from two different heads from the encoder self-attention
    at layer 5 of 6. The heads clearly learned to perform different tasks.
    15
    Metadatos: {'page_label': '15', 'page': 14, 'source': '.content/attention.pdf'}
    --------------------------------------------------
    Fuente 2:
    .
    <EOS>
    <pad>
    <pad>
    <pad>
    <pad>
    <pad>
    <pad>
    Figure 3: An example of the attention mechanism following long-distance dependencies in the
    encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of
    the verb ‚Äòmaking‚Äô, completing the phrase ‚Äòmaking...more difficult‚Äô. Attentions here shown only for
    the word ‚Äòmaking‚Äô. Different colors represent different heads. Best viewed in color.
    13
    Metadatos: {'page_label': '13', 'page': 12, 'source': '.content/attention.pdf'}
    --------------------------------------------------
    Fuente 3:
    but
    its
    application
    should
    be
    just
    -
    this
    is
    what
    we
    are
    missing
    ,
    in
    my
    opinion
    .
    <EOS>
    <pad>
    Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:
    Full attentions for head 5. Bottom: Isolated attentions from just the word ‚Äòits‚Äô for attention heads 5
    and 6. Note that the attentions are very sharp for this word.
    14
    Metadatos: {'source': '.content/attention.pdf', 'page_label': '14', 'page': 13}
    --------------------------------------------------
    ```

## Desplegando mi RAG con Gradio

Gradio de Hugging Face es una herramienta poderosa para crear interfaces de usuario interactivas para modelos de aprendizaje autom√°tico. Para usar Gradio, lo instalaremos usando pip:

```bash
pip install gradio transformers torch
```

Una vez instalado, crearemos nuestra primera aplicaci√≥n. Generaremos un archivo con el nombre `app1.py` cuyo contenido es el siguiente c√≥digo:

```python
import gradio as gr

def analyze_sentiment(sentence):
    if "good" in sentence:
        return "positive"
    elif "bad" in sentence:
        return "negative"
    else:
        return "neutral"

app = gr.Interface(fn=analyze_sentiment, inputs="text", outputs="label")

app.launch()
```

A continuaci√≥n, ejecutamos la aplicaci√≥n con el siguiente comando:

```bash
python app1.py
```
La salida se presenta en la imagen.
![alt text](image-2.png)

Gradio genera autom√°ticamente un dise√±o de interfaz predeterminado que incluye:

- **Widget de entrada**: Un cuadro de texto, ya que se especific√≥ `inputs="text"`.
- **Widget de salida**: Una etiqueta, ya que se especific√≥ `outputs="label"`.
- **Botones de control** para la interacci√≥n b√°sica del usuario:
  - **Submit**: Llama a la funci√≥n que proporcionaste (`analyze_sentiment`).
  - **Clear**: Restablece los componentes de entrada y salida.
  - **Flag**: Permite al usuario marcar un ejemplo (para retroalimentaci√≥n o depuraci√≥n).

Estos botones son parte de la interfaz de usuario predeterminada de Gradio; no necesitas definirlos manualmente a menos que desees personalizarlos o eliminarlos.

# Usando Blocks

Si deseas eliminar tanto "Clear" como "Flag" y gestionar tus propios botones, cambia a la API de Blocks:

```python
import gradio as gr

def analyze_sentiment(sentence):
   if "good" in sentence:
       return "positive"
   elif "bad" in sentence:
       return "negative"
   else:
       return "neutral"

with gr.Blocks() as demo:
    gr.Markdown("# App de Sentimiento")
    with gr.Row():
        txt = gr.Textbox(lines=2, placeholder="Ingresa texto")
        btn = gr.Button("Analizar")
    out = gr.Textbox(label="Resultado")

    btn.click(analyze_sentiment, inputs=txt, outputs=out)

demo.launch(share=True)
```

Esto te da control total: solo un bot√≥n, y t√∫ decides exactamente qu√© hace.

![alt text](image-3.png)




¬°Felicidades por llegar hasta el final del m√≥dulo y del curso! Has aprendido a integrar plantillas de prompt con cadenas y parsers de salida, implementaste cadenas con memoria y, finalmente, has practicado c√≥mo dividir documentos en fragmentos, almacenar embeddings de los fragmentos en una base de datos vectorial y realizar RAG sobre esta base de datos. Te invito a realizar la [actividad de aprendizaje](#evidencia-de-aprendizaje), donde crear√°s y desplegar√°s tu aplicaci√≥n RAG siguiendo los pasos que acabas de estudiar.

## Glosario

- **AIMessageChunk**: Una respuesta parcial de un mensaje de IA. Se utiliza al transmitir respuestas de un modelo de chat.
- **Configurable runnables**: Creaci√≥n de Runnables configurables.
- **Context window**: El tama√±o m√°ximo de entrada que un modelo de chat puede procesar.
- **Document**: Representaci√≥n de un documento en LangChain.
- **Embedding models**: Modelos que generan embeddings vectoriales para varios tipos de datos.
- **HumanMessage**: Representa un mensaje de un usuario humano.
- **Vector stores**: Almacenes de datos especializados para almacenar y buscar eficientemente embeddings vectoriales.

## Evidencia de Aprendizaje

| **M√≥dulo 3** | **Proyecto Integrador: construcci√≥n y despliegue de un sistema RAG** |
|--------------|-------------------------------------------------------------|
| **EA3.**     | Chat con tus datos

¬°Felicidades por llegar al final del curso! En tu √∫ltima entrega, practicar√°s las siguientes habilidades:

**Instrucciones**
1. **Carga de documentos**: usa PyPDFLoader para cargar 5 documentos en PDF de tu inter√©s.

2. **Divisi√≥n de documentos**: Utiliza RecursiveCharacterTextSplitter o CharacterTextSplitter para dividir los documentos en fragmentos.

3. **Embeddings**: emplea OpenAIEmbeddings para crear embeddings para tus fragmentos.

4. **Almacenamiento vectorial**: carga los embeddings en una base de datos vectorial, como Chroma.

5. **Instrucciones**: ilustra el uso de consultas sobre tus datos cargados en la base de datos vectorial, utilizando consultas por similitud y consultas usando el algoritmo MMR.

Desarrolla tu proyecto en un Jupyter Notebook y carga tu soluci√≥n. No olvides agregar comentarios en celdas de Markdown que expliquen el c√≥digo y tus razonamientos.

**Opcional**: investiga sobre plataformas de despliegue de tu aplicaci√≥n, como Streamlit, Hugging Face, Gradio, etc., y despliega tu RAG para que otros usuarios puedan usarla. Tambi√©n, investiga sobre retrievers y escoge la estrategia de retrieval que mejor se adapte a las necesidades de tu app desplegada.

Guarda los documentos con la siguiente nomenclatura:

- **Apellido_Nombre del estudiante.ipynb**  
**Ejemplo:**  
- L√≥pez_Karla.ipynb

Finalmente, haz clic en el bot√≥n **Cargar Tarea**, sube tu archivo y presiona el bot√≥n **Enviar** para remitirlo a tu profesor con el fin de que lo eval√∫e y retroalimente. |

!!! tip "üìñ Nota"
    Conoce los criterios de evaluaci√≥n de esta evidencia de aprendizaje consultando la r√∫brica que encontrar√°s a continuaci√≥n.

| **Criterios**             | **Ponderaci√≥n** |                       |                       |                       |                       | **Totales** |
|---------------------------|------------------|-----------------------|-----------------------|-----------------------|-----------------------|------------|
|                           | **70**           | **50**                | **5**                 | **0**                 |                       |            |
| **Calidad de las Soluciones** | Las soluciones a los ejercicios son correctas, demostrando una implementaci√≥n adecuada de los conceptos y t√©cnicas requeridos. El estudiante muestra un dominio completo de los temas abordados. | Aunque las soluciones no son completamente correctas, se observa un entendimiento y aplicaci√≥n adecuada de los conceptos y t√©cnicas involucradas. Hay evidencia de esfuerzo y comprensi√≥n de los temas. | Las soluciones presentadas son en su mayor√≠a incorrectas. Se percibe un intento de resolver los ejercicios, pero hay una falta de comprensi√≥n de los conceptos y t√©cnicas esenciales. | No realiza la entrega |                       | **70**      |
| **Calidad de la entrega** | El notebook es claro y f√°cil de seguir, incluyendo comentarios detallados sobre el funcionamiento del c√≥digo en las celdas Markdown, lo que facilita la comprensi√≥n de las soluciones propuestas. | El notebook no es particularmente f√°cil de leer, pero a√∫n as√≠ incluye comentarios que explican el funcionamiento del c√≥digo en las celdas Markdown, mostrando un esfuerzo por aclarar la l√≥gica detr√°s del c√≥digo. | El notebook carece de comentarios acerca del funcionamiento del c√≥digo en las celdas Markdown, lo que dificulta la comprensi√≥n de las soluciones implementadas. | No realiza la entrega |                       | **20**      |
| **Tiempo de la entrega**  | La entrega se realiza a tiempo, cumpliendo con el plazo establecido para la presentaci√≥n de la actividad. | La entrega se realiza con una semana de atraso. Aunque fuera del plazo original, se considera adecuada para evaluar el trabajo presentado. | La entrega se realiza con m√°s de una semana de atraso, lo que indica un retraso significativo en la presentaci√≥n de la actividad. | No realiza la entrega |                       | **10**      |
|                           |                  |                       |                       |                       | **Ponderaci√≥n de la actividad** | **100 puntos** |

# Referencias

DeepLearning.AI. (2025). *LangChain: Chat with Your Data* [Curso en l√≠nea]. [https://learn.deeplearning.ai/langchain-chat-with-your-data](https://learn.deeplearning.ai/langchain-chat-with-your-data)

LangChain. (2024). Document loaders. *Python LangChain Documentation*. [https://python.langchain.com/docs/integrations/document_loaders/](https://python.langchain.com/docs/integrations/document_loaders/)

# Lecturas y material complementario

Te invitamos a explorar el siguiente material para ampliar tus conocimientos sobre Retrieval-Augmented Generation (RAG) y su implementaci√≥n con LangChain.

## üìö Lecturas recomendadas


### **T√≠tulo:** *LangChain: Chat with Your Data*  
**Autor:** DeepLearning.AI  
**URL:** [https://learn.deeplearning.ai/langchain-chat-with-your-data](https://learn.deeplearning.ai/langchain-chat-with-your-data)  
Este curso gratuito de DeepLearning.AI ofrece una introducci√≥n pr√°ctica a la creaci√≥n de aplicaciones RAG utilizando LangChain. Cubre la carga de documentos, la generaci√≥n de embeddings, la recuperaci√≥n de informaci√≥n relevante y la integraci√≥n con modelos de lenguaje.

### **T√≠tulo:** *Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks*  
**Autor:** Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., ... & Kiela, D.  
**URL:** [https://arxiv.org/abs/2005.11401](https://arxiv.org/abs/2005.11401)  
Este art√≠culo seminal introduce el concepto de Retrieval-Augmented Generation (RAG), explicando c√≥mo combina modelos de recuperaci√≥n de informaci√≥n con generaci√≥n de texto para mejorar el rendimiento en tareas intensivas en conocimiento.

### **T√≠tulo:** *LangChain Documentation: Retrieval-Augmented Generation*  
**Autor:** LangChain  
**URL:** [https://python.langchain.com/docs/use_cases/question_answering/](https://python.langchain.com/docs/use_cases/question_answering/)  
La documentaci√≥n oficial de LangChain ofrece una gu√≠a detallada sobre c√≥mo implementar flujos de trabajo RAG, incluyendo ejemplos pr√°cticos de carga de documentos, creaci√≥n de √≠ndices vectoriales y uso de retrievers para aplicaciones de preguntas y respuestas.

## üé• Videos recomendados


### **T√≠tulo:** *Building RAG Applications with LangChain*  
**Autor:** DataCamp  
**URL:** [[https://www.datacamp.com/courses/building-rag-applications-with-langchain](https://www.datacamp.com/courses/retrieval-augmented-generation-rag-with-langchain)]
Este curso en video explora paso a paso c√≥mo construir aplicaciones RAG utilizando LangChain, con ejemplos pr√°cticos de integraci√≥n de bases de datos vectoriales y modelos de lenguaje.

### **T√≠tulo:** *What is Retrieval-Augmented Generation (RAG)?*  
**Autor:** IBM Technology  
**URL:** [https://www.youtube.com/watch?v=T-D1OfcDW1M](https://www.youtube.com/watch?v=T-D1OfcDW1M)  
Este video proporciona una explicaci√≥n concisa de RAG, destacando c√≥mo combina recuperaci√≥n de informaci√≥n y generaci√≥n de texto para mejorar las respuestas de modelos de lenguaje.
