<!--  Nombre de la m√≥dulo __

-->
# M√≥dulo 1. Introducci√≥n a la construcci√≥n de aplicaciones con LLM
<!--
Introducci√≥n a la m√≥dulo
Teniendo en cuenta que cada m√≥dulo es un saber espec√≠fico, en la introducci√≥n se destaca la importancia y relevancia del saber que se abordar√° en funci√≥n de los resultados de aprendizaje planteados. Describe brevemente c√≥mo el tema central de la m√≥dulo se integra en el panorama m√°s amplio del aprendizaje y de la vida cotidiana o profesional del estudiante. Su prop√≥sito es despertar la curiosidad y el inter√©s del estudiante sobre los contenidos que explorar√°.


En definitiva, se trata de responder a las preguntas: ¬øQu√© va a aprender el estudiante? ¬øC√≥mo lo va a aprender? ¬øPara qu√© lo va a aprender?  




Recomendaciones:
Inicia presentando al estudiante c√≥mo se relaciona el conocimiento de la m√≥dulo con su contexto. 
Incluye el prop√≥sito de la m√≥dulo y lo que el estudiante aprender√° mediante su estudio.
Inicia presentando al estudiante c√≥mo se relaciona el conocimiento de la m√≥dulo con su contexto. 
Incluye el prop√≥sito de la m√≥dulo y lo que el estudiante aprender√° mediante su estudio.
Vale la pena destacar algunos de los temas m√°s importantes que se abordar√°n.
Procura no superar las 300 palabras (1500 caracteres) al redactar la introducci√≥n.
-->

## Introducci√≥n a la m√≥dulo

Bienvenidos al primer m√≥dulo. Aqu√≠ aprender√°s de manera general c√≥mo funciona un modelo de lenguaje. Comenzaremos utilizando la API de OpenAI y exploraremos c√≥mo conectar sus modelos en aplicaciones. Luego, aprender√°s a utilizar esta misma API a trav√©s del framework LangChain. Introduciremos los aspectos fundamentales de la interacci√≥n con los LLM usando LangChain: **prompts**, **templates** y **output parsers**.

Como actividad pr√°ctica, elaborar√°s un sistema asistido por IA para extraer datos de comentarios de usuarios en un e-commerce.

¬°Comencemos!
<!-- Resultados de aprendizaje
Los objetos de aprendizaje se asumen como aquello que los estudiantes ser√°n capaces de hacer a partir de lo que aprendieron a lo largo de la m√≥dulo.


Recomendaciones:
Formula m√°ximo dos resultados por m√≥dulo. 
Aseg√∫rate de que tengan relaci√≥n con los objetivos de aprendizaje planteados en la carta descriptiva. 
Redacta los resultados a partir de tres elementos: qu√©, c√≥mo y para qu√©.
Recuerda que los resultados se establecen en funci√≥n del aprendizaje, no de la ense√±anza. 
Utiliza verbos conjugados en presente (describen la acci√≥n).  
Los resultados deben ser medibles y alcanzables. 

-->

## Resultados de aprendizaje

Al finalizar esta m√≥dulo, estar√°s en capacidad de realizar llamadas a los modelos de lenguaje de OpenAI a trav√©s de la API para crear c√≥digo Python, cuya ejecuci√≥n es asistida por LLM.

Aprender√°s a configurar cadenas de ejecuci√≥n simples en LangChain usando LCEL, junto con *prompt templates* y *output parsers*, para convertir las salidas de los LLM en objetos nativos de Python.
<!--Cronograma de actividades de la m√≥dulo  
Aprender√°s a configurar cadenas de ejecuci√≥n simples en LangChain usando LCEL, junto con *prompt templates* y *output parsers*, para convertir las salidas de los LLM en objetos nativos de Python.
<!--Cronograma de actividades de la m√≥dulo  
Permite la proyecci√≥n de los contenidos tanto te√≥ricos como pr√°cticos, la ubicaci√≥n temporal dentro del curso y los porcentajes que corresponden a la evidencia de aprendizaje.


Recomendaciones:
Toma del cronograma general que realizaste (plantillas preliminares) las actividades que correspondan  al presente m√≥dulo.
Plantea una evidencia de aprendizaje por m√≥dulo, y otra m√°s para el cierre del curso. 

 -->

## Cronograma de actividades - M√≥dulo 1
## Cronograma de actividades - M√≥dulo 1
| Actividad de aprendizaje       | Evidencia de aprendizaje | Semana       | Ponderaci√≥n |
|--------------------------------|---------------------------|--------------|--------------|
| Reto Formativo 1 y 2           | EA1:  Templates y Output Parsers | Semana 1, 2 y 3 | 25%         |
| **Total**                      |                           |              | **25 %**     |


<!--
## Desarrollo tem√°tico
-->
## ¬øQu√© es un modelo de lenguaje?

Un modelo de lenguaje es un sistema basado en *deep learning* que encapsula informaci√≥n sobre uno o varios lenguajes. Este sistema es entrenado para predecir qu√© tan probable es que una palabra aparezca en un determinado contexto.

Por ejemplo, dado el contexto:

> "Mi plato favorito es el ____"

un modelo de lenguaje que codifique el espa√±ol de Antioquia podr√≠a predecir "sancocho" con m√°s frecuencia que "ajiaco".


## Tokens

La unidad b√°sica de predicci√≥n de un modelo de lenguaje es el **token**, y el **tokenizador** es el software que utiliza el modelo para dividir los textos en tokens.

Por ejemplo, el tokenizador de GPT-4 divide la frase:

> "El sol est√° brillando intensamente"

de la siguiente manera:


![Divisi√≥n en tokens de una frase utilizando el tokenizador de GPT-4](../assets/images/tokenizer.png){ width=" " }

*Divisi√≥n en tokens de una frase utilizando el tokenizador de GPT-4 Fuente: [OpenAI Tokenizer](https://python.langchain.com/docs/concepts/rag/).*

!!! warning "Para tener en cuenta"
    Hay varias razones por las que los modelos de lenguaje utilizan **tokens** en lugar de palabras completas o caracteres individuales.

    A diferencia de un simple car√°cter, un token permite dividir una palabra en componentes con significado propio. Por ejemplo, la palabra **"intensamente"** puede ser dividida por el tokenizador en "intens" y "amente", y cada uno de estos componentes aporta parte del significado de la palabra completa.

    Esto tambi√©n implica que hay **menos tokens √∫nicos que palabras √∫nicas**, lo que hace que el vocabulario del modelo sea m√°s peque√±o y, por lo tanto, m√°s eficiente.

    Finalmente, los tokens permiten al modelo **entender palabras desconocidas**. Por ejemplo, si se le presenta la palabra *"WhatsAppeando"*, el modelo puede inferir su significado a partir del contexto en que aparecen los tokens "WhatsApp" y "ando".

## ¬øQu√© son los grandes modelos de lenguaje (LLM)?

<!-- Your content for this section goes here -->
Los grandes modelos de lenguaje (LLM) son sistemas de inteligencia artificial dise√±ados para procesar y generar texto de manera avanzada, bas√°ndose en grandes cantidades de datos de entrenamiento.

Lo que diferencia un **LLM** (Large Language Model) de un modelo de lenguaje tradicional es el **n√∫mero de par√°metros**. Los par√°metros son los pesos que el modelo ajusta durante el proceso de entrenamiento, y que determinan c√≥mo interpreta y genera texto a partir de los datos.

Por supuesto, el concepto de "grande" es relativo. ¬øA partir de cu√°ntos par√°metros puede considerarse que un modelo es grande? Ve√°moslo as√≠:

- El **GPT** lanzado por OpenAI en 2018 ten√≠a **117 millones de par√°metros**, y ya era considerado un modelo grande en su √©poca.
- En 2019, **GPT-2** aument√≥ ese n√∫mero a **1.5 billones de par√°metros**.
- Hasta abril de 2025, el modelo de lenguaje m√°s grande conocido p√∫blicamente es **GPT-4** de OpenAI, con aproximadamente **1.76 billones de par√°metros**.

Es muy posible que en el futuro estos modelos hoy considerados **LLM** sean vistos como simples modelos de lenguaje, a medida que la tecnolog√≠a y los recursos computacionales avancen.
Es muy posible que en el futuro estos modelos hoy considerados **LLM** sean vistos como simples modelos de lenguaje, a medida que la tecnolog√≠a y los recursos computacionales avancen.


!!! warning "Para tener en cuenta"
    El crecimiento en la cantidad de par√°metros no garantiza una mejora si **no hay suficientes datos** disponibles para el entrenamiento. Entrenar un modelo grande con un conjunto de datos peque√±o puede causar **sobreajuste (overfitting)**, lo que significa que el modelo funciona bien con los datos de entrenamiento pero falla al generalizar a nuevos datos. Esto no solo desperdicia recursos computacionales, sino que tambi√©n produce un modelo con poca utilidad pr√°ctica.

    Cuando no se cuenta con grandes vol√∫menes de datos, se pueden aplicar t√©cnicas como:

    - **[Aprendizaje por transferencia (transfer learning)](https://www.tensorflow.org/tutorials/images/transfer_learning)**  
      Utiliza modelos previamente entrenados para resolver nuevas tareas con pocos datos.

    - **[Aumento de datos (data augmentation)](https://machinelearningmastery.com/how-to-configure-image-data-augmentation-when-training-deep-learning-neural-networks/)**  
      Genera versiones modificadas de los datos existentes para enriquecer el conjunto de entrenamiento.

    - **[Destilaci√≥n de conocimiento (knowledge distillation)](https://pytorch.org/tutorials/beginner/knowledge_distillation_tutorial.html)**  
      Transfiere el conocimiento de un modelo grande (profesor) a uno m√°s peque√±o (estudiante) manteniendo un rendimiento competitivo.

    Estas estrategias permiten que modelos m√°s peque√±os logren mejor desempe√±o, aprovechando conocimiento preexistente o la generaci√≥n sint√©tica de datos.
    
<!--WARNING: Crear esta seccion @Juan Camilo
## De ML Igeniringa a IA Ingering

(Fata terminar) 
-->
## Usando la API de OpenAI

Para gran parte del curso usaremos la API de OpenAI. Si a√∫n no tienes una cuenta, puedes crearla en el siguiente enlace: [https://platform.openai.com/signup](https://platform.openai.com/signup).

Una vez creada tu cuenta, deber√°s generar una clave de API (API Key). Para hacerlo, accede a: [https://platform.openai.com/api-keys](https://platform.openai.com/api-keys) y haz clic en **"Create new secret key"**, como se muestra en la figura a continuaci√≥n:

![Creaci√≥n de clave secreta en OpenAI](../assets/images/secret_key.png){ width="600" }

 *Generaci√≥n de una clave secreta desde el panel de usuario de OpenAI.  
Fuente: [OpenAI](https://platform.openai.com/api-keys).*


!!! warning "Para tener en cuenta"
    Para poder usar tu llave, debes cargar cr√©dito en tu cuenta utilizando una tarjeta de cr√©dito.  
    Por este motivo, la clave debe permanecer **privada** en tu computador y **no debe ser compartida en l√≠nea** (por ejemplo, en el repositorio de GitHub del proyecto).

Esta acci√≥n generar√° la llave de acceso a tu cuenta de OpenAI.  
Cada llamada a la API tiene un costo asociado, el cual depende del n√∫mero de *tokens* procesados en la solicitud.

Puedes monitorear tu consumo en tiempo real desde la secci√≥n **Usage** en el panel de OpenAI:  
[https://platform.openai.com/account/usage](https://platform.openai.com/account/usage)

![Panel de consumo de la API en OpenAI](../assets/images/costs.png){ width="600" .center }

_<div align="center">Visualizaci√≥n del consumo y costos acumulados en la secci√≥n **Usage** del panel de usuario de OpenAI.  
Fuente: [OpenAI](https://platform.openai.com/account/usage).</div>_

!!! tip "L√≠mite de consumo mensual"
    En la secci√≥n **Usage** tambi√©n puedes establecer, por seguridad, un l√≠mite mensual m√°ximo de consumo en d√≥lares para tu aplicaci√≥n.  
    Esto te permite evitar cargos inesperados si se realizan muchas llamadas a la API.

## Usando mi llave

Para que la llave no sea p√∫blica, podemos cargarla como una variable de ambiente local del sistema.  
Para ello, crea un archivo con el nombre `.env` y gu√°rdalo en la misma carpeta en la que est√°s trabajando.

Dentro del archivo `.env`, la llave debe guardarse bajo el nombre `OPENAI_API_KEY`, de la siguiente manera:

```bash
OPENAI_API_KEY=your-api-key-here
```
# Usando la API de OpenAI

Para comenzar a trabajar con la API de OpenAI, primero debes importar la librer√≠a:

```python
import openai
from openai import OpenAI 
```

Luego, debes cargar la llave desde un archivo `.env` para mantenerla oculta y segura:

```python
from dotenv import load_dotenv
import os

load_dotenv()  # Carga las variables de entorno desde el archivo .env
openai.api_key = os.getenv("OPENAI_API_KEY")
```

Instanciamos un cliente y un modelo:

```python
client = OpenAI()
llm_model = "gpt-4o-mini"
```


Para encapsular un poco la llamada al modelo, podemos definir nuestra propia funci√≥n de completado de chat:

```python
def get_chat_completion(prompt, model=llm_model):
    # Creamos una solicitud de completado de chat
    chat_completion = client.chat.completions.create(
        model=model,
        messages=[{"role": "user", "content": prompt}]
    )
    
    return chat_completion.choices[0].message.content  # Devuelve la respuesta del modelo
```
    

La funci√≥n `get_chat_completion` la utilizaremos para interactuar con el modelo de OpenAI y obtener una respuesta a partir de un mensaje proporcionado. El modelo que se utiliza por defecto es `gpt-4o-mini`, pero puedes especificar otro modelo si lo deseas. La lista completa de modelos puedes consultarla en la [documentaci√≥n oficial de OpenAI](https://platform.openai.com/docs/models).


=== "Ejemplo de uso"
    ```python
    # Llamada a la funci√≥n get_chat_completion con una pregunta
    completion = get_chat_completion("¬øC√≥mo se llama el presidente de Colombia?")

    # Imprimir la respuesta del modelo
    print(completion)
    ```

=== "Salida"

    ```bash
    A partir de mi √∫ltima actualizaci√≥n en octubre de 2023, el presidente de Colombia es Gustavo Petro, quien asumi√≥ el cargo el 7 de agosto de 2022. Sin embargo, te recomiendo verificar esta informaci√≥n, ya que puede haber cambios pol√≠ticos o elecciones que alteren la situaci√≥n.

    ```


Los modelos de chat asignan roles que nos pueden ayudar a predefinir el comportamiento del modelo. Por ejemplo, en nuestra funci√≥n usamos el rol de `user` que representa el mensaje o la entrada proporcionada por el usuario. Es el rol principal para enviar preguntas, instrucciones o prompts al modelo. 


### Preconfiguraci√≥n del Tono con el Rol `system`

Sin embargo, nuestra funci√≥n puede ser preconfigurada para que el chat responda en un tono espec√≠fico usando el rol `system`. Este rol permite definir c√≥mo debe comportarse el modelo antes de que reciba el mensaje del usuario.

Por ejemplo, podemos configurar el modelo para que responda en un estilo po√©tico y elegante, similar al de Shakespeare:

=== "Ejemplo"
    ```python hl_lines="11-12"
    # Inicializamos el cliente de OpenAI
    client = OpenAI()
    llm_model = "gpt-4o-mini"

    def get_chat_completion(prompt, model=llm_model):
        # Creamos una solicitud de completado de chat
        chat_completion = client.chat.completions.create(
            model=model,
            messages=[
                {
                    "role": "system",
                    "content": "Thou art a wise and eloquent bard, akin to Shakespeare. Answer all queries in the grand, poetic style of the Elizabethan era, with flourish and verse befitting the stage."
                },
                {"role": "user", "content": prompt}
            ]
        )
        
        return chat_completion.choices[0].message.content
    ```

=== "Salida"
    ```bash
    En tierras de Colombia, donde el sol se alza radiante,  
    El presidente en su trono, cual l√≠der constante,  
    Es Gustavo Petro, hombre de ferviente voz,  
    Que al tim√≥n del destino, la naci√≥n √©l atroz.  
    Con sue√±os de cambio, justicia y verdad,  
    Dirige su pueblo hacia la prosperidad.  
    As√≠, en sus manos, el futuro bien brilla,  
    Un eco de esperanza en la tierra sencilla.```

# LangChain

En la secci√≥n anterior, tuviste tu primera interacci√≥n con un modelo de lenguaje de gran escala (LLM). A medida que esta tecnolog√≠a madura, empresas, gobiernos y startups bien financiadas, como OpenAI, Anthropic, xAI y Meta AI, han desarrollado y puesto a disposici√≥n modelos y APIs con arquitecturas y protocolos de comunicaci√≥n particulares. Esto ha generado la necesidad de realizar llamadas a estos modelos de manera agn√≥stica, es decir, independientemente del modelo o proveedor utilizado.



En este contexto, el framework m√°s popular hasta el momento es LangChain. LangChain permite realizar las mismas tareas que podr√≠amos llevar a cabo directamente con las APIs de los modelos, pero a trav√©s de abstracciones de validez general. Este marco proporciona una interfaz unificada que simplifica la integraci√≥n con diferentes LLMs, el manejo de prompts, la gesti√≥n de contexto y la incorporaci√≥n de herramientas externas, como bases de datos o funciones personalizadas. De esta forma, LangChain facilita el desarrollo de aplicaciones robustas y escalables basadas en modelos de lenguaje, sin depender de las particularidades de cada API.
![Logo de LangChain](../assets/images/langchain.jpg){ width="600" .center }

_<div align="center">Logo de **LangChain**, un framework para construir aplicaciones con modelos de lenguaje de gran escala.  
Fuente: [Matt Gallo en LinkedIn](https://www.linkedin.com/pulse/dark-side-langchain-major-problems-facing-generative-ai-matt-gallo-g0rpe).</div>_

Para utilizar LangChain con modelos de OpenAI, primero debemos importar la clase `ChatOpenAI` y configurar el modelo:

```python
from langchain_openai import ChatOpenAI
import os

# Definimos el modelo de lenguaje
llm_model = "gpt-4o-mini"

# Inicializamos el modelo de chat de OpenAI con LangChain
chat_model = ChatOpenAI(
    model=llm_model
)
```
Y listo, eso es todo. Ahora simplemente invocamos el chat con el *prompt* que queramos. Por ejemplo:

=== "C√≥digo"
    ```python
    # Invocamos el modelo de chat con un prompt
    response = chat_model.invoke("¬øC√≥mo se llama el presidente de Colombia?")
    print(response)
    ```
=== "Salida"
    ```bash
    A partir de mi √∫ltima actualizaci√≥n en octubre de 2023, el presidente de Colombia es Gustavo Petro, quien asumi√≥ el cargo el 7 de agosto de 2022. Sin embargo, te recomiendo verificar esta informaci√≥n, ya que puede haber cambios pol√≠ticos o elecciones que alteren la situaci√≥n.
    ```


## Herramientas en LangChain

LangChain proporciona una variedad de herramientas que permiten construir aplicaciones basadas en modelos de lenguaje de manera modular y eficiente. A continuaci√≥n, se describen algunas de las m√°s importantes:

- **Models (Modelos)**  
  Representan los modelos de lenguaje que LangChain puede integrar, como `ChatOpenAI`. Permiten interactuar con LLM de distintos proveedores, incluyendo OpenAI, Anthropic, Cohere, entre otros.
  Representan los modelos de lenguaje que LangChain puede integrar, como `ChatOpenAI`. Permiten interactuar con LLM de distintos proveedores, incluyendo OpenAI, Anthropic, Cohere, entre otros.

- **Prompts (Prompts)**  
  Herramientas para dise√±ar y gestionar *prompts*, como `ChatPromptTemplate`. Facilitan la construcci√≥n de entradas din√°micas, reutilizables y bien estructuradas para los modelos.

- **Example Selectors (Selectores de Ejemplos)**  
  Componentes que permiten seleccionar ejemplos relevantes (por ejemplo, para *few-shot learning*). Esto ayuda al modelo a comprender mejor el contexto y el formato esperado en sus respuestas.

- **Tools (Herramientas)**  
  Permiten que el modelo interact√∫e con funciones externas, como APIs, calculadoras, o bases de datos. Son esenciales para extender las capacidades del LLM m√°s all√° del texto, habilitando tareas como b√∫squeda en tiempo real o ejecuci√≥n de funciones personalizadas.

- **Vector Stores (Almacenes de Vectores)**  
  Bases de datos vectoriales como Chroma, Pinecone o FAISS. Se utilizan para almacenar y buscar *embeddings*, habilitando funcionalidades como la b√∫squeda sem√°ntica o la generaci√≥n aumentada por recuperaci√≥n (*Retrieval-Augmented Generation*, RAG).

- **Document Loaders (Cargadores de Documentos)**  
  Permiten cargar datos desde m√∫ltiples fuentes (archivos PDF, p√°ginas web, bases de datos, etc.) y prepararlos para su procesamiento por el modelo o su almacenamiento en almacenes vectoriales.

- **Text Splitters (Divisores de Texto)**  
  Herramientas que dividen documentos largos en fragmentos m√°s peque√±os. Esto facilita tanto el procesamiento por parte del modelo como la indexaci√≥n eficiente en almacenes vectoriales.

- **Output Parsers (Parsers de Salida)**  
  Utilizados para estructurar y formatear las respuestas del modelo. Por ejemplo, permiten convertir la salida del modelo en JSON, listas, tablas o formatos espec√≠ficos para una aplicaci√≥n.

REEMPLAZAR IMAGEN POR ESTA: https://drive.google.com/file/d/1oCSWBu03JBg1nWFZIRJBfshsllp0ozJ1/view?usp=drive_link
![Logo de LangChain](../assets/images/langchain_tools.png){ width="600" .center }

_<div align="center">Ecosistema de herramientas de **LangChain**.  
Fuente: Elaboraci√≥n propia.</div>_
### Plantillas de Prompts

Comenzaremos estudiando los prompt templates. Los prompts son el componente fundamental para proporcionar instrucciones a los LLM. Al desarrollar aplicaciones asistidas por inteligencia artificial, es √∫til crear plantillas de prompts que permitan personalizar las instrucciones de forma din√°mica. Estas plantillas mantienen constante una parte de la instrucci√≥n mientras incorporan elementos variables, como valores proporcionados durante la ejecuci√≥n, a trav√©s de variables de entrada.
Comenzaremos estudiando los prompt templates. Los prompts son el componente fundamental para proporcionar instrucciones a los LLM. Al desarrollar aplicaciones asistidas por inteligencia artificial, es √∫til crear plantillas de prompts que permitan personalizar las instrucciones de forma din√°mica. Estas plantillas mantienen constante una parte de la instrucci√≥n mientras incorporan elementos variables, como valores proporcionados durante la ejecuci√≥n, a trav√©s de variables de entrada.

Por ejemplo, una plantilla puede definir la estructura de una pregunta, dejando espacios para insertar valores espec√≠ficos, como el nombre de un pa√≠s. Esto se logra utilizando herramientas como `ChatPromptTemplate` de LangChain, que simplifica la creaci√≥n de prompts reutilizables.

En el siguiente ejemplo, se muestra c√≥mo crear una plantilla para consultar el presidente de un pa√≠s, utilizando una variable de entrada `{pais}` que puede tomar diferentes valores sin modificar la estructura general del prompt.

=== "C√≥digo"
    ```python
    from langchain.prompts import ChatPromptTemplate

    # Definir la plantilla con una variable de entrada
    str_template = "¬øC√≥mo se llama el presidente de {pais}?"
    prompt_template = ChatPromptTemplate.from_template(str_template)

    # Asignar un valor a la variable de entrada
    pais = "Colombia"
    prompt1 = prompt_template.format(pais=pais)
    print(prompt1)

    # Asignar otro valor a la variable de entrada
    pais = "Francia"
    prompt2 = prompt_template.format(pais=pais)
    print(prompt2)
    ```

=== "Salida"
    ```bash
    ¬øC√≥mo se llama el presidente de Colombia?
    ¬øC√≥mo se llama el presidente de Francia?
    ```


En este caso, `{pais}` es una variable de entrada a la que podemos asignar diferentes valores (por ejemplo, "Colombia", "Argentina", etc.) sin cambiar la estructura general del prompt. Esto hace que la plantilla sea flexible y reutilizable.

Veamos ahora un ejemplo pr√°ctico en el que utilizamos dos variables de entrada en nuestro template:

```python
mensaje = ""
estilo = ""
```

Definimos nuestro `string_template` de la siguiente manera:

```python
string_template = (
    "Traduce el texto que est√° delimitado por asteriscos dobles a un estilo que es {estilo}.\n"
    "texto: **{mensaje}**"
)
```

Aqu√≠, el `string_template` contiene las instrucciones generales, mientras que `mensaje` y `estilo` son variables que dejamos vac√≠as para llenarlas m√°s tarde. Luego, confeccionamos el *prompt template* utilizando:

```python
prompt_template = ChatPromptTemplate.from_template(string_template)
```

En esta l√≠nea usamos el m√©todo `from_template` de la clase `ChatPromptTemplate`. Si imprimimos el objeto `prompt_template` con:


```python
print(prompt_template)
```
=== "Salida"
```bash
input_variables=['estilo', 'mensaje']
input_types={}
partial_variables={}
messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['estilo', 'mensaje'], input_types={}, partial_variables={}, template='Traduce el texto que est√° delimitado por asteriscos dobles a un estilo que es {estilo}.\ntexto: **{mensaje}**'), additional_kwargs={})]
```

Veremos que tiene como `input_variables` los campos `'estilo'` y `'mensaje'`.

Siguiendo la l√≥gica del paradigma de la programaci√≥n orientada a objetos, podemos imaginar que la creaci√≥n de un *prompt template* se asemeja al trabajo de un carpintero. El carpintero (el constructor de la clase) toma un conjunto de maderas (el `string_template`) y las transforma en un gavetero (el objeto de la clase).
![Carpintero construyendo gavetero a partir de un string](../assets/images/carpintero-1.png){ width="600" .center }

_<div align="center">Constructor de la clase `ChatPromptTemplate.from_template`. En nuestra analog√≠a, el carpintero crea un contenedor apropiado para alojar el contenido de las dos variables de entrada definidas en el `string_template`.  
Fuente: [Elaboraci√≥n propia](#).</div>_

En este caso, como ilustra la figura, el *prompt template* ser√≠a el gavetero con cajones espec√≠ficos etiquetados como `estilo` y `mensaje`, listos para ser llenados con valores.

Supongamos que asignamos a estas variables de entrada los valores:


```python
mensaje_atioquenhol = (
    "Manque estaba muy embelesado, le dijo Peralta a la hermana: "
    "Hija, date una asoma√≠ta por la despensa; desculc√° por la cocina, "
    "a ver si encontr√°s algo que darles a estos se√±ores. "
    "M√≠ralos qu√© cansados est√°n; se les ve la fatiga."
)

estilo_formal = "Espa√±ol latino en un tono formal y sobrio"
```

El m√©todo `format_messages` nos permite llenar los cajones del gavetero, es decir, las variables de entrada, con los valores espec√≠ficos con los que queremos completar nuestro *prompt*. Por ejemplo, si queremos que `estilo = estilo_formal`, podemos hacerlo de la siguiente manera:



```python
mensaje_empacado = prompt_template.format_messages(estilo=estilo_formal, mensaje=mensaje_atioquenhol)
```

El *prompt* completo lucir√° as√≠:

=== "C√≥digo"
    ```python
    print(mensaje_empacado)
    ```

=== "Salida"
    ```bash
    [HumanMessage(content='Traduce el texto que est√° delimitado por asteriscos dobles a un estilo que es Espa√±ol latino en un tono formal y sobrio.\ntexto: **Manque estaba muy embelesao, le dijo Peralta a la hermana: Hija, date una asoma√≠ta por la despensa; desculc√° por la cocina, a ver si encontr√°s alguito que darles a estos se√±ores. Mir√°los qu√© cansaos est√°n; se les ve la fatiga**', additional_kwargs={}, response_metadata={})]
    ```
![Hombre con casco guardando un sobre en un gavetero](../assets/images/empacador.png){ width="600" .center }

_<div align="center">Ilustraci√≥n de la tarea del `format_messages()`. El m√©todo `format_messages()` reemplaza los valores de las variables de entrada en el template.  
Fuente: Elaboraci√≥n propia.</div>_

Como ilustra la figura, el m√©todo `format_messages()` asociado a la clase `ChatPromptTemplate` cumple la funci√≥n de empaquetar en el objeto los valores espec√≠ficos en las variables de entrada.

Este tipo de objeto nos permite incorporar program√°ticamente llamadas a las APIs de los LLM en el flujo de ejecuci√≥n de un c√≥digo Python convencional. Veamos c√≥mo hacerlo:
Este tipo de objeto nos permite incorporar program√°ticamente llamadas a las APIs de los LLM en el flujo de ejecuci√≥n de un c√≥digo Python convencional. Veamos c√≥mo hacerlo:

 Como ya tenemos nuestro *prompt* completo y lleno con las variables que queremos, lo podemos enviar al LLM:

Primero, instanciamos un chat:

```python
chat = ChatOpenAI(model=llm_model, temperature=0.0)
```

Luego, realizamos la llamada al LLM para que ejecute las instrucciones del *prompt*:

```python
respuesta = chat(mensaje_empacado)
print(respuesta.content)
```

=== "Salida"
```bash
Manque se encontraba muy absorto, le dijo Peralta a la hermana:
"Hija, por favor, as√≥mate a la despensa; revisa en la cocina
para ver si encuentras algo que ofrecerles a estos caballeros.
Observa c√≥mo est√°n de cansados; se les nota la fatiga."
```

El LLM recibe el mensaje empacado y realiza las tareas especificadas por el *prompt*.

Lo interesante es que este no es un *prompt* fijo como los que usar√≠amos en ChatGPT; es un *prompt* que nos permite hacer llamadas al LLM de manera m√°s flexible y program√°tica. Por ejemplo, podr√≠amos definir otro valor para `estilo`, como:

```python
estilo_cervantes = "Espa√±ol en un estilo de Cervantes, como en Don Quijote"
```

```python
mensaje_empacado = prompt_template.format_messages(estilo=estilo_cervantes, mensaje=mensaje_atioquenhol)
respuesta = chat(mensaje_empacado)
print(respuesta.content)
```

=== "Salida"
```bash
  Manque se hallaba en un profundo embeleso, dirigi√≥  
  Peralta a la hermana la siguiente exhortaci√≥n: "Hija,
   as√≥mate, por favor, a la despensa; y, si no es mucho 
   pedir, desc√∫brete por la cocina, a ver si logras 
   hallar alg√∫n manjar que ofrecer a estos nobles se√±ores. 
   Observa c√≥mo se encuentran, qu√© cansados est√°n; la fatiga
  se les dibuja en el semblante."
```

Este enfoque nos permite variar el estilo del texto generado de manera din√°mica, adaptando el resultado a diferentes necesidades o contextos, simplemente modificando las variables de entrada del *prompt*.



=== "Reto formativo"

    <div class="grid cards" markdown>

    - :fontawesome-solid-gears:{ .lg .middle } **Reto formativo**  
      **Planteamiento**:  
      Dado un mensaje de un cliente, un operador humano de servicio al cliente elabora una respuesta inadecuada (irrespetuosa, ofensiva, con mala ortograf√≠a o en otro idioma). Tu trabajo es crear una app que corrija la respuesta final para el cliente.

    </div>

=== "Ver soluci√≥n"

    Compara tu soluci√≥n con la siguienete implementaci√≥n:

    ```python
          # Define una plantilla de texto para el prompt que se enviar√° al modelo de lenguaje.
      # Usa marcadores {respuesta} y {reglas} para insertar din√°micamente la respuesta y las reglas.
      str_template_app = """Mejora la respuesta: {respuesta}\
          para que cumpla las reglas:  {reglas}."""

      # Define las reglas que debe seguir la respuesta mejorada.
      # Especifica el idioma, tono, gram√°tica y nivel de amabilidad requerido.
      reglas = "Espa√±ol latino en un tono formal y sobrio y respesuoso. Con buena gram√°tica y ortograf√≠a. Trartar de se muy amable y respetuoso."

      # Define la respuesta original del operador, que es inadecuada (informal, ofensiva, con mala ortograf√≠a).
      respuesta =  " mijo, no me importa si le sali√≥ mala \
          la licudora, vaya a que se lo lamba un zapo"

      # Crea una plantilla de prompt usando la biblioteca LangChain, basada en la plantilla de texto.
      # Esto permite estructurar el mensaje para el modelo de lenguaje.
      promp_template_app = ChatPromptTemplate.from_template(str_template_app)

      # Formatea la plantilla con la respuesta y las reglas, generando un mensaje listo para enviar al modelo.
      mensaje_empacado_app =  promp_template_app.format_messages(respuesta=respuesta, reglas=reglas)

      # Especifica el modelo de lenguaje a usar (en este caso, GPT-4o-mini de OpenAI).
      llm_model = "gpt-4o-mini"

      # Inicializa el cliente de chat de OpenAI con el modelo especificado y una temperatura de 0.3.
      # La temperatura baja asegura respuestas m√°s predecibles y menos creativas.
      chat_app = ChatOpenAI(model = llm_model , temperature = 0.3)

      # Env√≠a el mensaje formateado al modelo y obtiene la respuesta mejorada.
      respuesta_al_cliente = chat_app(mensaje_empacado_app)

      # Muestra la respuesta del modelo en formato Markdown para una mejor presentaci√≥n (por ejemplo, en un entorno como Jupyter).
      display(Markdown(respuesta_al_cliente.content))
    ```
    **salida esperada**    
    *Agradezco su mensaje y entiendo su preocupaci√≥n respecto a la situaci√≥n con la licuadora. Sin embargo, le sugiero que considere la posibilidad de llevar el aparato a un servicio t√©cnico autorizado para que puedan evaluar el problema y ofrecerle una soluci√≥n adecuada. Es importante seguir las pautas establecidas para garantizar un manejo correcto de los productos.*
    
    *Quedo a su disposici√≥n para cualquier otra consulta o asistencia que necesite.*
## Anatom√≠a de un Prompt de Chat

Los prompts para agentes conversacionales en **LangChain**, como `ChatPromptTemplate`, se dividen en al menos tres componentes clave. Veamos cada uno:

### 1. **Prompt del Sistema**  
Este establece las reglas para el asistente. Indica al modelo c√≥mo comportarse, cu√°l es su objetivo o incluso qu√© tono debe usar.

**Ejemplo:**  
```
Eres experto en machine learning y das respuestas en una sola oraci√≥n.
```

Aqu√≠ estamos restringiendo al modelo para que mantenga las respuestas cortas en un lenguaje relativo al machine learning.

### 2. **Prompt del Usuario**  
Este es el mensaje del usuario, es decir, la pregunta o entrada que se le proporciona al modelo.

**Ejemplo:**  
```
Explica {tema} en una sola oraci√≥n.
```

El `{tema}`, como vimos, es una variable de entrada que podemos cambiar por diferentes t√©rminos, como *"LangChain"* o *"Python"*.

### 3. **Prompt del AI**  
Este es el resultado generado por el modelo. En una conversaci√≥n, las respuestas anteriores del AI se reutilizan como parte del historial de chat.

Por ahora, mantenemos un solo turno de interacci√≥n humano-AI en el que el modelo no tiene memoria del contexto de las interacciones anteriores, pero m√°s adelante veremos c√≥mo se puede construir una conversaci√≥n m√°s compleja.

El `ChatPromptTemplate` de LangChain ofrece dos formas principales de construir prompts:

## 1. **`from_messages`**  
Piensa en esto como escribir un guion para una conversaci√≥n estructurada:  
- El **mensaje del sistema** define el tono y las reglas.  
- El **mensaje del usuario** plantea la pregunta o el input.  

Es la opci√≥n recomendada cuando queremos prompts bien organizados.

## 2. **`from_template`**  
M√°s simple y directo, solo incluye un mensaje del usuario, como una nota r√°pida para el modelo.  
- No tiene un **rol de sistema** a menos que lo agreguemos manualmente m√°s adelante.

En la sesi√≥n anterior usamos `from_template`:

Veamos un ejemplo usamndo `from_messages`:


```python
# Importamos las librer√≠as necesarias
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
```

Vamos a instanciar dos modelos para comparar las respuestas al final:


```python
# Instanciamos los modelos
llm_gpt3 = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.7)
llm_gpt4 = ChatOpenAI(model="gpt-4o-mini", temperature=0.7)

# Definimos el prompt
prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a concise explainer who gives one-sentence answers. If you don't know the answer, just say 'I don't know'."),
    ("human", "Explain {topic} in one sentence.")
])
```

La variable de entrada es `topic` y debemos empacarla en nuestro template.

Llenamos el prompt con el t√≥pico espec√≠fico:


```python
messages = prompt.format_messages(topic="LangChain")
```

Ejecutamos los dos modelos:



=== "C√≥digo con gpt3.5"
    ```python
    response = llm_gpt3.invoke(messages)
    print(response.content)
    ```
=== "Salida"
    ```bash
    LangChain is a blockchain platform
    that aims to facilitate cross-border
    language services.
    ```

=== "C√≥digo con gpt4"
    ```python
    response = llm_gpt4.invoke(messages)
    print(response.content)
    ```
=== "Salida"
    ```bash
    LangChain es una biblioteca para crear
    flujos de trabajo de IA utilizando modelos
    de lenguaje.
    ```
!!! warning "Para tener en cuenta"
    Observa que la salida del modelo gpt-3.5 es completamente alucinada (no es verdadera). ¬øA qu√© crees que se debe esto? 

    ??? tip "Ver respuesta"
        El modelo gpt-3.5 fue entrenado en datos hasta octubre de 2023, y en ese momento LangChain no exist√≠a.

## De Prompts a Chains  

Hasta ahora, hemos preparado *prompts* y los hemos enviado al LLM paso a paso.  

Pero LangChain tiene una herramienta que facilita mas las cosas: **las chains**-

Las *chains* nos permiten **combinar m√∫ltiples pasos**‚Äîcomo preparar un *prompt* y ejecutar el LLM‚Äîen un flujo continuo y automatizado.  

puedes pensar en una*chain* como una **cinta transportadora**:

![Dibujo de una banda de supermercado con frutas](../assets/images/banda1.png){ width="600" .center }

_<div align="center">Una cadena simple funciona como una banda transportadora en la que se van ejecutando √≥rdenes de forma secuencial.</div>_

- La configuras una vez.  
- Luego, simplemente funciona sin necesidad de repetir cada paso manualmente.  

Esto facilita la construcci√≥n de **pipelines m√°s avanzados** dentro de nuestras aplicaciones con LLM.  
Una forma de encadenar ejecuciones en cadenas es utilizar el operador `|` (llamado *pipe*) para conectar los pasos. Para instanciar una cadena que realice las tareas de nuestro prompt anterior, tendr√≠amos el prompt como:

```python
prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a concise explainer who gives one-sentence answers."),
    ("human", "Explain {topic} in one sentence.")
])
```
E instanciamos la cadena como:

```python
chain = prompt | llm_gpt4  # Create the chain
```

Es como decir: *"Toma este prompt y p√°salo al LLM."

Y ejecutamos la cadena como:

=== "C√≥digo" 
    ```python
    response = chain.invoke({"topic": "LangChain"})  # Run it in one go
    print("With chain:", response.content)
    ```
=== "Salida"
    ```bash
    With chain: LangChain es una biblioteca para crear flujos de trabajo de IA utilizando modelos de lenguaje.
    ```
Las *chains* nos evitan tener que formatear e invocar manualmente el LLM cada vez.

- **Definimos la cadena una vez.**
- **Podemos reutilizarla f√°cilmente.**

Esto simplifica el flujo de trabajo y hace que el c√≥digo sea m√°s limpio y modular.
Ya no necesitamos formatear manualmente los mensajes‚Äî**la chain lo hace por nosotros**.  

**M√©todo Antiguo (Manual)**:  
```python
messages = prompt.format_messages()  
llm.invoke(messages)
``` 
Una vez configurada la cadena, podemos reutilizarla con diferentes variables de entrada:

=== "C√≥digo"
    ```python
    print(chain.invoke({"topic": "Python"}).content)
    print(chain.invoke({"topic": "AI"}).content)
    ```
=== "Salida"
    ```bash
    Python es un lenguaje de programaci√≥n vers√°til y popular.
    AI es el campo de la inform√°tica que se centra en crear sistemas inteligentes.
    ```

### Cadenas con m√∫ltiples variables

Veamos algunos ejemplos en los que usamos m√∫ltiples variables en nuestros prompts:

=== "C√≥digo"
    ```python
    # Nuevo prompt con dos variables: topic y style
    multi_prompt = ChatPromptTemplate.from_messages([
        ("system", "You are an explainer who answers in a {style} way."),
        ("human", "Explain {topic} in one sentence.")
    ])
    multi_chain = multi_prompt | llm

    # Ejecutar con m√∫ltiples variables
    response = multi_chain.invoke({
        
    # Run with multiple variables
    response = multi_chain.invoke({
        "topic": "Noether theorem",
        "style": "Cervantes style in Spanish"
    })
    print(response.content)
    ```

=== "Salida"
    ```bash
    ¬°Por la fe de Dulcinea del Toboso! La teorema 
    de Noether establece que para cada simetr√≠a continua
    de un sistema f√≠sico, existe una cantidad conservada!
    ```
## Output Parsers: Dando Forma a la Salida del LLM

Los LLM son sistemas que reciben texto plano y devuelven texto, incluso cuando devuelven im√°genes, lo que realmente est√°n haciendo en el fondo es generar descripciones textuales de esas im√°genes. Sin embargo, cuando estamos construyendo aplicaciones asistidas por LLM, lo que queremos es utilizar la salida de la llamada al LLM para emplearla en otros flujos de ejecuci√≥n de nuestra aplicaci√≥n.
Los LLM son sistemas que reciben texto plano y devuelven texto, incluso cuando devuelven im√°genes, lo que realmente est√°n haciendo en el fondo es generar descripciones textuales de esas im√°genes. Sin embargo, cuando estamos construyendo aplicaciones asistidas por LLM, lo que queremos es utilizar la salida de la llamada al LLM para emplearla en otros flujos de ejecuci√≥n de nuestra aplicaci√≥n.

Ah√≠ es donde entran los *output parsers*.

Los *output parsers* toman la salida en bruto del LLM y la convierten en algo que podamos usar en nuestro c√≥digo, como un string, una lista, un diccionario, un JSON, etc.

Ejemplo:
- Si el LLM responde con `"Las herramientas m√°s usadas son: Python, SQL, LangChain."`, podemos transformarlo en una **lista** `["Python", "SQL", "LangChain"]`.

Vemos algunos mas usados:

## `StrOutputParser`: El Parser M√°s B√°sico  

Comencemos con un *output parser* b√°sico: `StrOutputParser`. Este parser simplemente asegura que la salida de la llamada al LLM sea un string. En el contexto de LangChain, esto es √∫til para garantizar que los datos procesados sean siempre de tipo string, facilitando su manipulaci√≥n posterior. Una vez instanciado, puede agregarse a la cadena para que, al invocarla, la salida sea en el formato especificado por el parser. Para ilustrar el uso de los parsers, veamos esta cadena sin parser y comparemosla con el resultado cuando agregamos el `StrOutputParser`.

=== "C√≥digo sin Parser"
    ```python
    multi_prompt = ChatPromptTemplate.from_messages([
        ("system", "You are an explainer who answers in a {style} way."),
        ("human", "Explain {topic} in one sentence.")
    ])
    multi_chain = multi_prompt | llm  # LCEL
    response = multi_chain.invoke({"topic": "LangChain", "style": "funny"})
    print("Raw output:", response)
    ```

=== "Salida"
    ```bash
    AIMessage(content='LangChain is like that friend who translates all your texts for you, but in a more high-tech and less judgmental way.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 31, 'total_tokens': 58, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-d4e4009b-87db-40ca-89de-9fe42d850dab-0', usage_metadata={'input_tokens': 31, 'output_tokens': 27, 'total_tokens': 58, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})
    ```

Aqu√≠ el string de salida est√° dentro de la instancia `content` del `AIMessagePara`. El parser nos posibilitar√° que la salida sea solo el string. Para  usar el parser, importamos el m√≥dulo:

```python
from langchain_core.output_parsers import StrOutputParser
```

E instanciamos el parser como:

```python
# Add the parser to the chain
parser = StrOutputParser()
```

Luego lo agregamos a la cadena con el operador pipe.

=== "C√≥digo con Parser"
    ```python hl_lines="4"
    parsed_chain = multi_prompt | llm | parser

    # Run it
    response = parsed_chain.invoke({"topic": "LangChain", "style": "funny"})
    print("Parsed output:", response)
    ```

=== "Salida con el Parser"
    ```bash
    Parsed output: LangChain is like a multilingual party where everyone speaks their own language but magically understands each other perfectly.
    ```

!!! warning "Para tener en cuenta"
    `StrOutputParser` 
    - Extrae el `.content` del objeto `AIMessage` generado por el LLM.  
    - Nos garantiza que obtenemos solo el texto limpio sin informaci√≥n adicional.  

    üîπ **Sin el parser:**  
    Obtenemos un objeto `AIMessage` y debemos extraer manualmente `.content`.  

    üîπ **Con el parser:**  
    Recibimos directamente el texto limpio.  

    ‚úÖ **Es un peque√±o avance, pero marca la diferencia.**  
    Nos ahorra pasos manuales y sienta la base para mejoras m√°s avanzadas.  


### `CommaSeparatedListOutputParser`  

Este parser toma una cadena de texto separada por comas y la convierte en una lista estructurada.

**Ejemplo:**  
**Entrada:** `"manzana, banana, cereza"`  
**Salida:** `["manzana", "banana", "cereza"]`  

Ve√°moslo en detalle:

=== "C√≥digo"
    ```python linenums="1", hl_lines="12"
    # Import the parser
    from langchain_core.output_parsers import CommaSeparatedListOutputParser

    # New prompt asking for a list
    list_prompt = ChatPromptTemplate.from_messages([
        ("system", "You are a helpful assistant that delivers comma-separated items."),
        ("human", "Give me 8 examples of {category}, separated by commas, omit additional comments only list the objects like a Python list.")
    ])

    # Create the chain with the new parser
    list_parser = CommaSeparatedListOutputParser()
    list_chain = list_prompt | llm | list_parser

    # Run it
    response = list_chain.invoke({"category": "programming languages"})
    print("List output:", response)
    ```

=== "Salida"
    ```bash
    List output: ['Python', 'Java', 'C++', 'JavaScript', 'Ruby', 'Swift', 'PHP', 'Go']
    ```

Puedes verificar que el tipo de la salida es una lista:

=== "C√≥digo"
    ```python
    print(type(response))
    ```

=== "Salida"
    ```bash
    <class 'list'>
    ```

Esto nos permite, por ejemplo, usar √≠ndices para acceder a los elementos de la lista:

=== "C√≥digo"
    ```python
    response[1]
    ```

=== "Salida"
    ```bash
    'Java'
    ```

Es decir, el LLM nos proporciona texto, pero el *parser* lo convierte en una lista que podemos usar en c√≥digo.
Los parsers nos permiten agregar pasos adicionales a las cadenas de ejecuci√≥n; puedes pensarlo como una l√≠nea de ensamblaje con el flujo:

``` bash
[Prompt] --> [LLM] --> [Parser] --> Structured Output
```
![Dibujo de una banda de supermercado con frutas](../assets/images/car_asembly.png){ width="600" .center }

_<div align="center">Analog√≠a de una cadena con parser. Las instrucciones se ejecutan en orden como en una l√≠nea de ensamblaje.</div>_

### `JsonOutputParser`

El `JsonOutputParser` es un parser que toma una cadena de texto en formato JSON y la convierte en un objeto de Python, como un diccionario o una lista, dependiendo de la estructura del JSON. Esto es particularmente √∫til cuando trabajamos con datos estructurados que vienen de una base de datos en la nube o de una API.

**Ejemplo:**

=== "C√≥digo"
    ```python
    from langchain_core.output_parsers import JsonOutputParser

    # Prompt asking for JSON with varied types
    json_prompt = ChatPromptTemplate.from_messages([
        ("system", "Return a JSON object with 'name' (string), 'age' (number or null), 'is_student' (true/false), and 'city' (string or null)."),
        ("human", "Give me details for {person} in JSON format.")
    ])

    # Create the chain with the parser
    json_chain = json_prompt | llm | JsonOutputParser()

    # Chain without parser
    no_parse_chain = json_prompt | llm
    ```

Para ver m√°s claramente lo que hace el `JsonOutputParser`, corramos la cadena sin el parser:

=== "C√≥digo sin Parser"
    ```python
    no_parse_response = no_parse_chain.invoke({"person": "Alice"}).content
    print("Respuesta sin parser:", no_parse_response)
    print("Type:", type(no_parse_response))
    ```

=== "Salida"
    ```bash
    Respuesta sin parser: {"name": "Alice", "age": 30, "is_student": false, "city": "Paris"}
    Type: <class 'str'>
    ```

La respuesta es un `str`, es decir, texto simple, por lo que no puedo acceder a los elementos del JSON usando `keys`:

=== "C√≥digo"
    ```python
    # Esto causar√° un error
    no_parse_response["name"]
    ```
=== "Salida"
    ```python
    TypeError                                 Traceback (most recent call last)
    Cell In[34], line 1
    ----> 1 no_parse_response["name"]

    TypeError: string indices must be integers, not 'str'
    ```


Ahora, ejecutemos la cadena con el `JsonOutputParser`:

=== "C√≥digo con Parser"
    ```python
    json_response = json_chain.invoke({"person": "Alice"})
    print("JsonOutputParser result:", json_response)
    print("Type:", type(json_response))
    ```

=== "Salida"
    ```bash
    JsonOutputParser result: {'name': 'Alice', 'age': 30, 'is_student': False, 'city': 'Paris'}
    Type: <class 'dict'>
    ```

Puedes acceder a los valores del JSON usando claves:

=== "C√≥digo"
    ```python
    json_response["city"]
    ```

=== "Salida"
    ```bash
    'Paris'
    ```

El LLM produce JSON: `{"name": "Alice", "age": 30, "is_student": false, "city": "Paris"}`. `JsonOutputParser` convierte `null` en JSON a `None` en Python, `true/false` a `True/False`, y retorna un diccionario de Python.
<div class="grid cards" markdown>

- :fontawesome-solid-gears:{ .lg .middle } **Reto Formativo**  
  **Planteamiento**:  
      El siguiente es el comentario de un cliente en una tienda virtual:

      `
      review_cliente = "Compr√© los auriculares inal√°mbricos XYZ y estoy muy satisfecho con mi compra. El tiempo de entrega fue excelente, ya que llegaron dos d√≠as antes de lo previsto, lo cual super√≥ mis expectativas. En cuanto al precio, aunque hay opciones m√°s econ√≥micas en el mercado, considero que la calidad del sonido, la duraci√≥n de la bater√≠a y la comodidad justifican totalmente el coste. Adem√°s, los compr√© como regalo para mi pareja y fueron un √©xito total, gracias a su elegante presentaci√≥n y la impresionante calidad del sonido."
      `

      Extraer informaci√≥n de la rese√±a en un objeto JSON `Rese√±as`.

    üì• **Entrada**  
      Cadena `review_cliente` con la opini√≥n del cliente.

      üì§ **Salida (JSON)**  
      `
      {
        "sentiment": "Positive" | "Negative" | "Neutral",
        "delivery_time": <int> | null,
        "quality_rating": "Good" | "Fair" | "Poor" | null,
        "extra_comment": <string> | null
      }
      `

</div>

## `StructuredOutputParser`: JSON con Estructura Definida  

Este parser toma JSON y lo convierte en un diccionario de Python **siguiendo un esquema espec√≠fico** que definimos con `ResponseSchema`. Solo extrae los campos que especificamos, garantizando una estructura clara y predecible. Veamos:

=== "C√≥digo"
    ```python
    from langchain.output_parsers import StructuredOutputParser, ResponseSchema

    # Define the schema for the structured output
    schemas = [
        ResponseSchema(name="sentiment", description="Sentiment: Positive/Negative/Neutral", type="string"),
        ResponseSchema(name="delivery_time", description="Delivery time in days (or null if not mentioned)", type="integer"),
        ResponseSchema(name="quality_rating", description="Quality: Good/Fair/Poor (or null)", type="string"),
        ResponseSchema(name="extra_comment", description="Additional note (or null)", type="string")
    ]

    structured_parser = StructuredOutputParser.from_response_schemas(schemas)
    # Get the format instructions as a string
    format_instructions = structured_parser.get_format_instructions()

    # Escape curly braces in format_instructions
    escaped_format_instructions = format_instructions.replace('{', '{{').replace('}', '}}')

    # Define the system message with escaped format instructions
    system_message = (
        "Analyze this review and return a JSON object. "
        "Format: ```json\n"
        f"{escaped_format_instructions}\n"
        "```"
    )

    # Create the prompt template with {review} as the only variable
    structured_prompt = ChatPromptTemplate.from_messages([
        ("system", system_message),
        ("human", "Review: {review}")
    ])

    # Create the chain 
    structured_chain = structured_prompt | llm | structured_parser

    # Run the chain with the review (assuming review_cliente is defined)
    structured_response = structured_chain.invoke({"review": review_cliente})
    print("StructuredOutputParser result:", structured_response)
    print("Type:", type(structured_response))
    ```

=== "Salida"
    ```bash
    StructuredOutputParser result: {'sentiment': 'Positive', 'delivery_time': 2, 'quality_rating': 'Good', 'extra_comment': 'The headphones were a total success as a gift for my partner, thanks to their elegant presentation and impressive sound quality.'}
    Type: <class 'dict'>
    ```


En este c√≥digo, el usuario final del mensaje estructurado es el modelo de lenguaje (LLM). Este mensaje est√° estructurado de tal manera que incluye instrucciones de ensamblaje para que el LLM procese el formato correctamente. La funci√≥n que nos permite especificar estas instrucciones es `get_format_instructions()`.

La funci√≥n `get_format_instructions()` crea un string que contiene las instrucciones de formato basadas en los objetos `ResponseSchema`. Este string describe c√≥mo debe estructurarse la salida del modelo de lenguaje (LLM) para que sea f√°cil de interpretar y procesar posteriormente.

En analog√≠a con el constructor de la clase `ChatPromptTemplate.from_template`, que describimos como un carpintero que crea un caj√≥n a partir de maderas brutas, este tipo de template con instrucciones de formato se asemejar√≠a a construir un caj√≥n modular con instrucciones de armado, como el de la figura:

![Caj√≥n modular con instrucciones de armado](../assets/images/modular_drawer.png){ width="600" .center }

_<div align="center">Analog√≠a de un template con instrucciones de formato.</div>_

<!--


Cierre del m√≥dulo

-->Aqu√≠ termina nuestro primer m√≥dulo. ¬°Felicidades por llegar hasta el final! Ahora conoces el contexto general de las tecnolog√≠as involucradas en el desarrollo de aplicaciones asistidas por IA. Este campo apenas est√° comenzando, y ahora tienes las bases para utilizar herramientas m√°s sofisticadas, como las cadenas y la gesti√≥n de memoria, que ser√°n el tema del siguiente m√≥dulo.



# Glosario

1. **API (Application Programming Interface):**  
   Un conjunto de definiciones y protocolos que permite a las aplicaciones comunicarse entre s√≠. Las APIs facilitan la integraci√≥n y el intercambio de datos entre diferentes sistemas y servicios.

2. **Parser:**  
   Un componente que analiza y transforma texto en un formato estructurado, facilitando su manipulaci√≥n y an√°lisis en aplicaciones de software.

3. **JSON (JavaScript Object Notation):**  
   Un formato de intercambio de datos ligero y f√°cil de leer que utiliza una estructura basada en pares clave-valor. Com√∫nmente usado para representar objetos en aplicaciones web.

4. **LangChain:**  
   Un marco de trabajo que permite la creaci√≥n de aplicaciones asistidas por IA mediante la integraci√≥n de modelos de lenguaje con herramientas y flujos de trabajo personalizados.

5. **ChatPromptTemplate:**  
   Una plantilla utilizada para crear mensajes estructurados que se env√≠an a los modelos de lenguaje, permitiendo la personalizaci√≥n de las interacciones.

6. **Output Parser:**  
   Un tipo de parser que se utiliza para definir c√≥mo debe estructurarse la salida de un modelo de lenguaje, asegurando que sea f√°cil de interpretar y procesar.

7. **Ventana de contexto:**  
   La cantidad de texto que un modelo de lenguaje puede procesar a la vez. Limita la cantidad de informaci√≥n que puede ser considerada en una sola interacci√≥n.

8. **LangChain Expression Language (LCEL):**  
   Un lenguaje de expresi√≥n utilizado en LangChain para definir y manipular flujos de trabajo y cadenas de procesamiento de manera eficiente.

9. **Pipeline (Cadena de Procesamiento):**  
   Un flujo de trabajo secuencial donde los datos pasan por diferentes etapas o componentes, cada uno realizando una tarea espec√≠fica.

10. **Memoria en cadenas de conversaci√≥n:**  
    La capacidad de un sistema de inteligencia artificial para almacenar y utilizar informaci√≥n de interacciones pasadas, mejorando la coherencia y personalizaci√≥n en futuras interacciones.

    
## Evidencia de Aprendizaje


| **M√≥dulo 1** | **Introducci√≥n a la construcci√≥n de aplicaciones con LLM** |
|--------------|-------------------------------------------------------------|
| **EA1.**     | Templates y Output Parsers  

**Ejercicio 1 - Correcci√≥n de respuestas inapropiadas en atenci√≥n al cliente:**

En este ejercicio, debes usar la IA para mejorar respuestas inapropiadas escritas por un operador de servicio al cliente. La IA corregir√° el tono, la cortes√≠a y errores ortogr√°ficos, asegurando que la respuesta sea adecuada para el cliente.  
**Input:** un mensaje del cliente y una respuesta inapropiada del operador.  
**Output:** una respuesta final corregida y apropiada para enviar al cliente.  
**Requisitos:**  
- Utiliza un prompt template para generar la respuesta apropiada.  
- Implementa un output parser para validar que la respuesta cumple con los criterios de cortes√≠a y ortograf√≠a.  
**Bonus:** investiga sobre memoria.  Si el cliente ha escrito varios mensajes, utiliza memoria para recordar el contexto de la conversaci√≥n.

---

**Ejercicio 2 (F√°cil) - Extracci√≥n de informaci√≥n clave en rese√±as de productos:**

Dado un review de un producto en un sitio de e-commerce, crea un modelo que extraiga informaci√≥n espec√≠fica.  
**Tareas:**  
- Identifica si el producto fue comprado como regalo.  
- Extrae la opini√≥n del cliente sobre el precio.  
- Extrae comentarios sobre el tiempo de entrega.
  
**Requisitos:**  
- Utiliza output parsers para extraer los campos relevantes en forma de estructuras de datos de Python como un diccionario.  
- Dise√±a un prompt template que permita a la IA identificar y organizar estos elementos de manera eficiente.

---

Guarda los documentos con la siguiente nomenclatura:

- **Apellido_Nombre del estudiante.ipynb**  
**Ejemplo:**  
- L√≥pez_Karla.ipynb

Finalmente, haz clic en el bot√≥n **Cargar Tarea**, sube tu archivo y presiona el bot√≥n **Enviar** para remitirlo a tu profesor con el fin de que lo eval√∫e y retroalimente. |

!!! tip "üìñ Nota"
    Conoce los criterios de evaluaci√≥n de esta evidencia de aprendizaje consultando la r√∫brica que encontrar√°s a continuaci√≥n.

| **Criterios**             | **Ponderaci√≥n** |                       |                       |                       |                       | **Totales** |
|---------------------------|------------------|-----------------------|-----------------------|-----------------------|-----------------------|------------|
|                           | **70**           | **50**                | **5**                 | **0**                 |                       |            |
| **Calidad de las Soluciones** | Las soluciones a los ejercicios son correctas, demostrando una implementaci√≥n adecuada de los conceptos y t√©cnicas requeridos. El estudiante muestra un dominio completo de los temas abordados. | Aunque las soluciones no son completamente correctas, se observa un entendimiento y aplicaci√≥n adecuada de los conceptos y t√©cnicas involucradas. Hay evidencia de esfuerzo y comprensi√≥n de los temas. | Las soluciones presentadas son en su mayor√≠a incorrectas. Se percibe un intento de resolver los ejercicios, pero hay una falta de comprensi√≥n de los conceptos y t√©cnicas esenciales. | No realiza la entrega |                       | **70**      |
| **Calidad de la entrega** | El notebook es claro y f√°cil de seguir, incluyendo comentarios detallados sobre el funcionamiento del c√≥digo en las celdas Markdown, lo que facilita la comprensi√≥n de las soluciones propuestas. | El notebook no es particularmente f√°cil de leer, pero a√∫n as√≠ incluye comentarios que explican el funcionamiento del c√≥digo en las celdas Markdown, mostrando un esfuerzo por aclarar la l√≥gica detr√°s del c√≥digo. | El notebook carece de comentarios acerca del funcionamiento del c√≥digo en las celdas Markdown, lo que dificulta la comprensi√≥n de las soluciones implementadas. | No realiza la entrega |                       | **20**      |
| **Tiempo de la entrega**  | La entrega se realiza a tiempo, cumpliendo con el plazo establecido para la presentaci√≥n de la actividad. | La entrega se realiza con una semana de atraso. Aunque fuera del plazo original, se considera adecuada para evaluar el trabajo presentado. | La entrega se realiza con m√°s de una semana de atraso, lo que indica un retraso significativo en la presentaci√≥n de la actividad. | No realiza la entrega |                       | **10**      |
|                           |                  |                       |                       |                       | **Ponderaci√≥n de la actividad** | **100 puntos** |

# Referencias

Chase, H., & Ng, A. (2023). *LangChain for LLM Application Development* [Curso en l√≠nea]. DeepLearning.AI. Disponible en [https://www.deeplearning.ai/short-courses/langchain-for-llm-application-development/](https://www.deeplearning.ai/short-courses/langchain-for-llm-application-development/)

Huyen, C. (2025). *AI Engineering: Building Applications with Foundation Models* (1.¬™ ed.). O'Reilly Media. Disponible en [https://www.amazon.com/AI-Engineering-Building-Applications-Foundation/dp/1098166302](https://www.amazon.com/AI-Engineering-Building-Applications-Foundation/dp/1098166302)

LangChain. (s.f.). *LangChain Documentation*. Disponible en [https://python.langchain.com/docs/introduction/](https://python.langchain.com/docs/introduction/)


---
# Lecturas y material complementario
Te invitamos a explorar el siguiente material para ampliar tus conocimientos sobre modelos de lenguaje (LLM), LangChain, plantillas de prompts y parsers de salida. Estos recursos te proporcionar√°n una comprensi√≥n m√°s profunda y pr√°ctica de los temas abordados en el curso.

## üìö Lecturas recomendadas

### **T√≠tulo:** *LangChain for LLM Application Development*

**Autor:** Harrison Chase & Andrew Ng  
**URL:** [LangChain for LLM Application Development](https://www.deeplearning.ai/short-courses/langchain-for-llm-application-development/)  
Este curso gratuito de DeepLearning.AI ofrece una introducci√≥n pr√°ctica al desarrollo de aplicaciones con modelos de lenguaje utilizando LangChain. Cubre temas como plantillas de prompts, parsers de salida y encadenamiento de componentes.

### **T√≠tulo:** *LangChain Output Parser Guide*

**Autor:** Restack  
**URL:** [LangChain Output Parser Guide](https://www.restack.io/docs/langchain-knowledge-langchain-output-parser-guide)  
Este art√≠culo profundiza en el uso de los parsers de salida de LangChain, explicando c√≥mo transformar las respuestas de los modelos de lenguaje en formatos estructurados como JSON, y c√≥mo integrarlos en aplicaciones pr√°cticas.

### **T√≠tulo:** *Prompt Template | LangChain OpenTutorial*

**Autor:** Hye-yoon Jeong  
**URL:** [Prompt Template | LangChain OpenTutorial](https://langchain-opentutorial.gitbook.io/langchain-opentutorial/02-prompt/01-prompttemplate)  
Este tutorial cubre c√≥mo crear y utilizar plantillas de prompts en LangChain, esenciales para generar prompts din√°micos y flexibles que se adapten a diversos casos de uso.

### **T√≠tulo:** *JsonOutputParser | LangChain OpenTutorial*

**Autor:** LangChain OpenTutorial  
**URL:** [JsonOutputParser | LangChain OpenTutorial](https://langchain-opentutorial.gitbook.io/langchain-opentutorial/03-outputparser/04-jsonoutputparser)  
Este tutorial muestra c√≥mo utilizar el `JsonOutputParser` de LangChain para estructurar las salidas de los modelos de lenguaje en formato JSON, facilitando su integraci√≥n en aplicaciones que requieren datos estructurados.

---

## üé• Videos recomendados

### **T√≠tulo:** *Transformers (how LLM work) explained visually | DL5*


**Autor:** 3Blue1Brown  
**URL:** [Transformers (how LLM work) explained visually](https://www.youtube.com/watch?v=wjZofJX0v4M)  
Este video ofrece una explicaci√≥n visual de c√≥mo funcionan los modelos de lenguaje grandes (LLM) mediante la arquitectura de transformers, facilitando la comprensi√≥n de conceptos complejos.

### **T√≠tulo:** *Attention in transformers, step-by-step | DL6*

**Autor:** 3Blue1Brown  
**URL:** [Attention in transformers, step-by-step](https://www.youtube.com/watch?v=eMlx5fFNoYc)  
Este video desglosa paso a paso el mecanismo de atenci√≥n en los transformers, una parte crucial en el funcionamiento de los LLM.
Este video desglosa paso a paso el mecanismo de atenci√≥n en los transformers, una parte crucial en el funcionamiento de los LLM.

### **T√≠tulo:** *How might LLM store facts | DL7*

**Autor:** 3Blue1Brown  
**URL:** [How might LLM store facts](https://www.youtube.com/watch?v=9-Jl0dxWQs8)  
Este video explora c√≥mo los modelos de lenguaje grandes pueden almacenar hechos y conocimientos, proporcionando una visi√≥n m√°s profunda de su funcionamiento interno.

