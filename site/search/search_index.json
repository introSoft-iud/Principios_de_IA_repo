{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Presentaci\u00f3n del curso","text":""},{"location":"#diplomado-en-construccion-de-aplicaciones-asistidas-por-ia","title":"Diplomado en construcci\u00f3n de Aplicaciones Asistidas por IA","text":"<p>Bienvenidos al diplomado en construcci\u00f3n de aplicaciones asistidas por modelos de lenguaje de la Instituci\u00f3n Universitaria Digital de Antioquia.</p> <p>Aunque la inteligencia artificial ha existido como un campo exitoso y prometedor entre los expertos durante varias d\u00e9cadas, la llegada de capacidades computacionales m\u00e1s avanzadas \u2014ofrecidas por las GPU modernas\u2014 y las habilidades demostradas con el lanzamiento de ChatGPT fueron una gran sorpresa para muchos.</p> <p>No est\u00e1 del todo claro c\u00f3mo este \"peque\u00f1o\" avance en la escala de los modelos pudo desencadenar la gran cantidad de aplicaciones asistidas por IA que estamos viendo explotar cada semana. Lo que s\u00ed es claro es que el campo de la ingenier\u00eda de software est\u00e1 siendo revolucionado, y que el nuevo paradigma de construcci\u00f3n de software ya no consiste en los tradicionales flujos de ejecuci\u00f3n, sino que la nueva ingenier\u00eda de sistemas debe integrar a los LLM en dichos flujos.</p> <p>Hemos dise\u00f1ado este diplomado para introducirte a este nuevo paradigma. Este curso est\u00e1 basado en LangChain, el framework m\u00e1s popular de la actualidad para interactuar con las APIs de los principales modelos de lenguaje.</p> <p>En el m\u00f3dulo 1 comenzar\u00e1s por aprender a crear instrucciones reutilizables para los LLM, los llamados <code>prompt templates</code>, luego ver\u00e1s c\u00f3mo encadenar estas instrucciones a trav\u00e9s de las cadenas usando  el Lenguaje de Expresi\u00f3n de LangChain (LCEL). Aprender\u00e1s a especificar el formato de salida de tus cadenas de ejecuci\u00f3n mediante los <code>output parsers</code>. A continuaci\u00f3n, en el m\u00f3dulo 2, crear\u00e1s tus primeros chatbots y aprender\u00e1s a gestionar sus memorias. Como ejercicio pr\u00e1ctico, implementar\u00e1s un chatbot asistente de un m\u00e9dico que carga en su memoria la historia cl\u00ednica de los pacientes. Finalmente, en el m\u00f3dulo, estar\u00e1s preparado para incorporar los componentes de los dos m\u00f3dulos anteriores junto con loaders de documentos, retrievers y bases de datos vectoriales, para crear y desplegar un sistema RAG sobre documentos en PDF, de manera que podr\u00e1s conversar con tus archivos PDF.</p>"},{"location":"#resultados-de-aprendizaje","title":"Resultados de Aprendizaje","text":"<ol> <li> <p>Dise\u00f1ar y aplicar prompt templates y cadenas de ejecuci\u00f3n en LangChain: los estudiantes ser\u00e1n capaces de crear instrucciones reutilizables (prompt templates) y encadenarlas utilizando el Lenguaje de Expresi\u00f3n de LangChain (LCEL), especificando formatos de salida con output parsers para interactuar eficazmente con modelos de lenguaje.</p> </li> <li> <p>Desarrollar chatbots con gesti\u00f3n de memoria contextual: los estudiantes podr\u00e1n construir chatbots funcionales utilizando LangChain, integrando memorias para almacenar y gestionar informaci\u00f3n contextual.</p> </li> <li> <p>Construir y desplegar sistemas RAG para interacci\u00f3n con documentos: los estudiantes estar\u00e1n capacitados para integrar componentes como loaders de documentos, retrievers y bases de datos vectoriales en LangChain, creando sistemas de Retrieval-Augmented Generation (RAG) que permitan conversar con archivos PDF.</p> </li> </ol>"},{"location":"#pregunta-orientadora","title":"Pregunta Orientadora","text":"<p>La siguiente imagen es la respuesta de ChatGPT al prompt:</p> <p>\u201cBased on what you know about me, draw a picture of what you think my current life looks like.\u201d</p> <p></p> <p>Figura 1: Representaci\u00f3n visual generada por ChatGPT sobre la vida del autor del diplomado.</p> <p>\u00bfQu\u00e9 tan parecida es esta imagen a tu vida en tu caso? \u00bfTe sorprende que ChatGPT tenga tanta informaci\u00f3n sobre tu vida y tus gustos o todo lo contrario? \u00bfQu\u00e9 opinas que va a pasar con los datos en un futuro donde todo es asistido por IA? \u00bfLe daremos a las IA la informaci\u00f3n sensible de las personas y las empresas?</p> <p>\u00bfC\u00f3mo podemos asegurarnos de que nuestros datos permanezcan privados, y al mismo tiempo, aprovechar todo el poder de los modelos de lenguaje en los datos privados?</p>"},{"location":"#mapa-del-curso","title":"Mapa del curso","text":"<p>Estos son los m\u00f3dulos que conforman nuestro diploma:</p> <p></p>"},{"location":"#modulo-1-introduccion-a-la-construccion-de-aplicaciones-con-llm","title":"M\u00f3dulo 1: introducci\u00f3n a la Construcci\u00f3n de Aplicaciones con LLM","text":"<ul> <li>Fundamentos de modelos de lenguaje grandes (LLM) y su integraci\u00f3n en flujos de software</li> <li>Creaci\u00f3n y uso de prompt templates para instrucciones reutilizables</li> <li>Encadenamiento de instrucciones con el Lenguaje de Expresi\u00f3n de LangChain (LCEL)</li> <li>Especificaci\u00f3n de formatos de salida mediante output parsers</li> </ul>"},{"location":"#modulo-2-cadenas-y-memoria","title":"M\u00f3dulo 2: Cadenas y Memoria","text":"<ul> <li>Desarrollo de chatbots funcionales con LangChain</li> <li>Gesti\u00f3n de memoria contextual para mantener el historial de interacciones</li> <li>Implementaci\u00f3n pr\u00e1ctica de un chatbot funcional</li> </ul>"},{"location":"#modulo-3-proyecto-integrador-construccion-y-despliegue-de-un-sistema-rag","title":"M\u00f3dulo 3: Proyecto integrador: construcci\u00f3n y Despliegue de un Sistema RAG","text":"<ul> <li>Integraci\u00f3n de document loaders para procesar archivos PDF</li> <li>Uso de retrievers y bases de datos vectoriales para recuperaci\u00f3n de informaci\u00f3n</li> <li>Construcci\u00f3n de sistemas Retrieval-Augmented Generation (RAG)</li> <li>Despliegue de una aplicaci\u00f3n RAG para interacci\u00f3n conversacional con documentos PDF</li> </ul>"},{"location":"#cronograma-de-actividades","title":"Cronograma de actividades","text":"Actividad de aprendizaje Evidencia de aprendizaje Semana Ponderaci\u00f3n Actividad de conocimientos previos Actividad de conocimientos previos Semana 1 0% Reto formativo 1 y 2 EA1: Templates y Output Parsers Semanas 2 y 3 35% Cadenas EA2: Generaci\u00f3n de Informes de Salud Utilizando Archivos CSV Semanas 4 y 5 35% Reto formativo: \"Carga y divisi\u00f3n de archivos PDF\" EA3: Chat con tus datos. Proyecto integrador Semanas 6, 7 y 8 30% Total 100%"},{"location":"#actividad-de-conocimientos-previos-foro-transformers-y-mecanismo-de-atencion","title":"Actividad de conocimientos previos. Foro: Transformers y mecanismo de atenci\u00f3n","text":"<p>Como actividad de conocimientos previos, te invitamos a participar en el foro de discusi\u00f3n sobre el funcionamiento de los LLM.</p>"},{"location":"#configuracion-del-sistema-antes-de-comenzar","title":"Configuraci\u00f3n del sistema antes de comenzar","text":"<p>Antes de empezar a trabajar con los m\u00f3dulos del curso, debes configurar tu sistema para poder ejecutar los ejemplos correctamente.</p> <p>La forma m\u00e1s sencilla de hacerlo es descargando el archivo de configuraci\u00f3n \ud83d\udcc4environment.yml , el cual crear\u00e1 autom\u00e1ticamente un entorno de Conda llamado <code>DiplomadoIA_env</code> con todas las dependencias necesarias para el curso.</p>"},{"location":"#requisitos-previos","title":"Requisitos previos","text":"<ul> <li>Tener Anaconda instalado en tu computador.</li> <li>Usar una terminal Bash (en Windows puedes usar Anaconda Prompt, git bash, WSL o similares).</li> </ul>"},{"location":"#instalacion","title":"Instalaci\u00f3n","text":"<p>Una vez descargado el archivo de configuraci\u00f3n, ejecuta el siguiente comando en tu terminal:</p> <pre><code>conda env create -f environment.yml\n</code></pre>"},{"location":"#activacion-del-entorno","title":"Activaci\u00f3n del entorno","text":"<p>Para activar el entorno en tu terminal, ejecuta:</p> <p><pre><code>conda activate diplomado_IA\n</code></pre> A partir de aqu\u00ed, cualquier comando que ejecutes usar\u00e1 las dependencias definidas para el curso.</p>"},{"location":"#uso-del-entorno-en-visual-studio-code","title":"Uso del entorno en Visual Studio Code","text":"<p>Para tener en cuenta</p> <p>Para ejecutar notebooks <code>.ipynb</code> en Visual Studio Code usando este entorno:</p> <ol> <li>Abre VS Code.</li> <li>Abre la carpeta del proyecto o el notebook deseado.</li> <li>En la parte superior derecha del notebook, haz clic en la selecci\u00f3n de kernel.</li> <li>Elige el kernel correspondiente al entorno <code>diplomado_IA</code>. Si no aparece, reinicia VS Code o aseg\u00farate de haber activado el entorno desde la terminal integrada.</li> <li>Comienza a ejecutar celdas normalmente.</li> </ol> <p>Tip</p> <p>Puedes asegurarte de que el entorno se registre correctamente como kernel ejecutando en la terminal: <pre><code>python -m ipykernel install --user --name diplomado_IA --display-name \"Python (diplomado_IA)\"\n</code></pre></p>"},{"location":"Unidad%201/modulo1/","title":"M\u00f3dulo 1","text":""},{"location":"Unidad%201/modulo1/#modulo-1-introduccion-a-la-construccion-de-aplicaciones-con-llm","title":"M\u00f3dulo 1. Introducci\u00f3n a la construcci\u00f3n de aplicaciones con LLM","text":""},{"location":"Unidad%201/modulo1/#introduccion-a-la-modulo","title":"Introducci\u00f3n a la m\u00f3dulo","text":"<p>Bienvenidos al primer m\u00f3dulo. Aqu\u00ed aprender\u00e1s de manera general c\u00f3mo funciona un modelo de lenguaje. Comenzaremos utilizando la API de OpenAI y exploraremos c\u00f3mo conectar sus modelos en aplicaciones. Luego, aprender\u00e1s a utilizar esta misma API a trav\u00e9s del framework LangChain. Introduciremos los aspectos fundamentales de la interacci\u00f3n con los LLM usando LangChain: prompts, templates y output parsers.</p> <p>Como actividad pr\u00e1ctica, elaborar\u00e1s un sistema asistido por IA para extraer datos de comentarios de usuarios en un e-commerce.</p> <p>\u00a1Comencemos!</p>"},{"location":"Unidad%201/modulo1/#resultados-de-aprendizaje","title":"Resultados de aprendizaje","text":"<p>Al finalizar esta m\u00f3dulo, estar\u00e1s en capacidad de realizar llamadas a los modelos de lenguaje de OpenAI a trav\u00e9s de la API para crear c\u00f3digo Python, cuya ejecuci\u00f3n es asistida por LLM.</p> <p>Aprender\u00e1s a configurar cadenas de ejecuci\u00f3n simples en LangChain usando LCEL, junto con prompt templates y output parsers, para convertir las salidas de los LLM en objetos nativos de Python.</p>"},{"location":"Unidad%201/modulo1/#cronograma-de-actividades-modulo-1","title":"Cronograma de actividades - M\u00f3dulo 1","text":""},{"location":"Unidad%201/modulo1/#cronograma-de-actividades-modulo-1_1","title":"Cronograma de actividades - M\u00f3dulo 1","text":"Actividad de aprendizaje Evidencia de aprendizaje Semana Ponderaci\u00f3n Reto Formativo 1 y 2 EA1:  Templates y Output Parsers Semana 1, 2 y 3 25% Total 25 %"},{"location":"Unidad%201/modulo1/#que-es-un-modelo-de-lenguaje","title":"\u00bfQu\u00e9 es un modelo de lenguaje?","text":"<p>Un modelo de lenguaje es un sistema basado en deep learning que encapsula informaci\u00f3n sobre uno o varios lenguajes. Este sistema es entrenado para predecir qu\u00e9 tan probable es que una palabra aparezca en un determinado contexto.</p> <p>Por ejemplo, dado el contexto:</p> <p>\"Mi plato favorito es el ____\"</p> <p>un modelo de lenguaje que codifique el espa\u00f1ol de Antioquia podr\u00eda predecir \"sancocho\" con m\u00e1s frecuencia que \"ajiaco\".</p>"},{"location":"Unidad%201/modulo1/#tokens","title":"Tokens","text":"<p>La unidad b\u00e1sica de predicci\u00f3n de un modelo de lenguaje es el token, y el tokenizador es el software que utiliza el modelo para dividir los textos en tokens.</p> <p>Por ejemplo, el tokenizador de GPT-4 divide la frase:</p> <p>\"El sol est\u00e1 brillando intensamente\"</p> <p>de la siguiente manera:</p> <p></p> <p>Divisi\u00f3n en tokens de una frase utilizando el tokenizador de GPT-4 Fuente: OpenAI Tokenizer.</p> <p>Para tener en cuenta</p> <p>Hay varias razones por las que los modelos de lenguaje utilizan tokens en lugar de palabras completas o caracteres individuales.</p> <p>A diferencia de un simple car\u00e1cter, un token permite dividir una palabra en componentes con significado propio. Por ejemplo, la palabra \"intensamente\" puede ser dividida por el tokenizador en \"intens\" y \"amente\", y cada uno de estos componentes aporta parte del significado de la palabra completa.</p> <p>Esto tambi\u00e9n implica que hay menos tokens \u00fanicos que palabras \u00fanicas, lo que hace que el vocabulario del modelo sea m\u00e1s peque\u00f1o y, por lo tanto, m\u00e1s eficiente.</p> <p>Finalmente, los tokens permiten al modelo entender palabras desconocidas. Por ejemplo, si se le presenta la palabra \"WhatsAppeando\", el modelo puede inferir su significado a partir del contexto en que aparecen los tokens \"WhatsApp\" y \"ando\".</p>"},{"location":"Unidad%201/modulo1/#que-son-los-grandes-modelos-de-lenguaje-llm","title":"\u00bfQu\u00e9 son los grandes modelos de lenguaje (LLM)?","text":"<p>Los grandes modelos de lenguaje (LLM) son sistemas de inteligencia artificial dise\u00f1ados para procesar y generar texto de manera avanzada, bas\u00e1ndose en grandes cantidades de datos de entrenamiento.</p> <p>Lo que diferencia un LLM (Large Language Model) de un modelo de lenguaje tradicional es el n\u00famero de par\u00e1metros. Los par\u00e1metros son los pesos que el modelo ajusta durante el proceso de entrenamiento, y que determinan c\u00f3mo interpreta y genera texto a partir de los datos.</p> <p>Por supuesto, el concepto de \"grande\" es relativo. \u00bfA partir de cu\u00e1ntos par\u00e1metros puede considerarse que un modelo es grande? Ve\u00e1moslo as\u00ed:</p> <ul> <li>El GPT lanzado por OpenAI en 2018 ten\u00eda 117 millones de par\u00e1metros, y ya era considerado un modelo grande en su \u00e9poca.</li> <li>En 2019, GPT-2 aument\u00f3 ese n\u00famero a 1.5 billones de par\u00e1metros.</li> <li>Hasta abril de 2025, el modelo de lenguaje m\u00e1s grande conocido p\u00fablicamente es GPT-4 de OpenAI, con aproximadamente 1.76 billones de par\u00e1metros.</li> </ul> <p>Es muy posible que en el futuro estos modelos hoy considerados LLM sean vistos como simples modelos de lenguaje, a medida que la tecnolog\u00eda y los recursos computacionales avancen. Es muy posible que en el futuro estos modelos hoy considerados LLM sean vistos como simples modelos de lenguaje, a medida que la tecnolog\u00eda y los recursos computacionales avancen.</p> <p>Para tener en cuenta</p> <p>El crecimiento en la cantidad de par\u00e1metros no garantiza una mejora si no hay suficientes datos disponibles para el entrenamiento. Entrenar un modelo grande con un conjunto de datos peque\u00f1o puede causar sobreajuste (overfitting), lo que significa que el modelo funciona bien con los datos de entrenamiento pero falla al generalizar a nuevos datos. Esto no solo desperdicia recursos computacionales, sino que tambi\u00e9n produce un modelo con poca utilidad pr\u00e1ctica.</p> <p>Cuando no se cuenta con grandes vol\u00famenes de datos, se pueden aplicar t\u00e9cnicas como:</p> <ul> <li> <p>Aprendizaje por transferencia (transfer learning)   Utiliza modelos previamente entrenados para resolver nuevas tareas con pocos datos.</p> </li> <li> <p>Aumento de datos (data augmentation)   Genera versiones modificadas de los datos existentes para enriquecer el conjunto de entrenamiento.</p> </li> <li> <p>Destilaci\u00f3n de conocimiento (knowledge distillation)   Transfiere el conocimiento de un modelo grande (profesor) a uno m\u00e1s peque\u00f1o (estudiante) manteniendo un rendimiento competitivo.</p> </li> </ul> <p>Estas estrategias permiten que modelos m\u00e1s peque\u00f1os logren mejor desempe\u00f1o, aprovechando conocimiento preexistente o la generaci\u00f3n sint\u00e9tica de datos.</p>"},{"location":"Unidad%201/modulo1/#usando-la-api-de-openai","title":"Usando la API de OpenAI","text":"<p>Para gran parte del curso usaremos la API de OpenAI. Si a\u00fan no tienes una cuenta, puedes crearla en el siguiente enlace: https://platform.openai.com/signup.</p> <p>Una vez creada tu cuenta, deber\u00e1s generar una clave de API (API Key). Para hacerlo, accede a: https://platform.openai.com/api-keys y haz clic en \"Create new secret key\", como se muestra en la figura a continuaci\u00f3n:</p> <p></p> <p>Generaci\u00f3n de una clave secreta desde el panel de usuario de OpenAI. Fuente: OpenAI.</p> <p>Para tener en cuenta</p> <p>Para poder usar tu llave, debes cargar cr\u00e9dito en tu cuenta utilizando una tarjeta de cr\u00e9dito. Por este motivo, la clave debe permanecer privada en tu computador y no debe ser compartida en l\u00ednea (por ejemplo, en el repositorio de GitHub del proyecto).</p> <p>Esta acci\u00f3n generar\u00e1 la llave de acceso a tu cuenta de OpenAI. Cada llamada a la API tiene un costo asociado, el cual depende del n\u00famero de tokens procesados en la solicitud.</p> <p>Puedes monitorear tu consumo en tiempo real desde la secci\u00f3n Usage en el panel de OpenAI: https://platform.openai.com/account/usage</p> <p></p> <p>Visualizaci\u00f3n del consumo y costos acumulados en la secci\u00f3n Usage del panel de usuario de OpenAI. Fuente: OpenAI.</p> <p>L\u00edmite de consumo mensual</p> <p>En la secci\u00f3n Usage tambi\u00e9n puedes establecer, por seguridad, un l\u00edmite mensual m\u00e1ximo de consumo en d\u00f3lares para tu aplicaci\u00f3n. Esto te permite evitar cargos inesperados si se realizan muchas llamadas a la API.</p>"},{"location":"Unidad%201/modulo1/#usando-mi-llave","title":"Usando mi llave","text":"<p>Para que la llave no sea p\u00fablica, podemos cargarla como una variable de ambiente local del sistema. Para ello, crea un archivo con el nombre <code>.env</code> y gu\u00e1rdalo en la misma carpeta en la que est\u00e1s trabajando.</p> <p>Dentro del archivo <code>.env</code>, la llave debe guardarse bajo el nombre <code>OPENAI_API_KEY</code>, de la siguiente manera:</p> <pre><code>OPENAI_API_KEY=your-api-key-here\n</code></pre>"},{"location":"Unidad%201/modulo1/#usando-la-api-de-openai_1","title":"Usando la API de OpenAI","text":"<p>Para comenzar a trabajar con la API de OpenAI, primero debes importar la librer\u00eda:</p> <pre><code>import openai\nfrom openai import OpenAI \n</code></pre> <p>Luego, debes cargar la llave desde un archivo <code>.env</code> para mantenerla oculta y segura:</p> <pre><code>from dotenv import load_dotenv\nimport os\n\nload_dotenv()  # Carga las variables de entorno desde el archivo .env\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n</code></pre> <p>Instanciamos un cliente y un modelo:</p> <pre><code>client = OpenAI()\nllm_model = \"gpt-4o-mini\"\n</code></pre> <p>Para encapsular un poco la llamada al modelo, podemos definir nuestra propia funci\u00f3n de completado de chat:</p> <pre><code>def get_chat_completion(prompt, model=llm_model):\n    # Creamos una solicitud de completado de chat\n    chat_completion = client.chat.completions.create(\n        model=model,\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n\n    return chat_completion.choices[0].message.content  # Devuelve la respuesta del modelo\n</code></pre> <p>La funci\u00f3n <code>get_chat_completion</code> la utilizaremos para interactuar con el modelo de OpenAI y obtener una respuesta a partir de un mensaje proporcionado. El modelo que se utiliza por defecto es <code>gpt-4o-mini</code>, pero puedes especificar otro modelo si lo deseas. La lista completa de modelos puedes consultarla en la documentaci\u00f3n oficial de OpenAI.</p> Ejemplo de usoSalida <pre><code># Llamada a la funci\u00f3n get_chat_completion con una pregunta\ncompletion = get_chat_completion(\"\u00bfC\u00f3mo se llama el presidente de Colombia?\")\n\n# Imprimir la respuesta del modelo\nprint(completion)\n</code></pre> <pre><code>A partir de mi \u00faltima actualizaci\u00f3n en octubre de 2023, el presidente de Colombia es Gustavo Petro, quien asumi\u00f3 el cargo el 7 de agosto de 2022. Sin embargo, te recomiendo verificar esta informaci\u00f3n, ya que puede haber cambios pol\u00edticos o elecciones que alteren la situaci\u00f3n.\n</code></pre> <p>Los modelos de chat asignan roles que nos pueden ayudar a predefinir el comportamiento del modelo. Por ejemplo, en nuestra funci\u00f3n usamos el rol de <code>user</code> que representa el mensaje o la entrada proporcionada por el usuario. Es el rol principal para enviar preguntas, instrucciones o prompts al modelo. </p>"},{"location":"Unidad%201/modulo1/#preconfiguracion-del-tono-con-el-rol-system","title":"Preconfiguraci\u00f3n del Tono con el Rol <code>system</code>","text":"<p>Sin embargo, nuestra funci\u00f3n puede ser preconfigurada para que el chat responda en un tono espec\u00edfico usando el rol <code>system</code>. Este rol permite definir c\u00f3mo debe comportarse el modelo antes de que reciba el mensaje del usuario.</p> <p>Por ejemplo, podemos configurar el modelo para que responda en un estilo po\u00e9tico y elegante, similar al de Shakespeare:</p> EjemploSalida <pre><code># Inicializamos el cliente de OpenAI\nclient = OpenAI()\nllm_model = \"gpt-4o-mini\"\n\ndef get_chat_completion(prompt, model=llm_model):\n    # Creamos una solicitud de completado de chat\n    chat_completion = client.chat.completions.create(\n        model=model,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"Thou art a wise and eloquent bard, akin to Shakespeare. Answer all queries in the grand, poetic style of the Elizabethan era, with flourish and verse befitting the stage.\"\n            },\n            {\"role\": \"user\", \"content\": prompt}\n        ]\n    )\n\n    return chat_completion.choices[0].message.content\n</code></pre> <p><code>bash En tierras de Colombia, donde el sol se alza radiante,   El presidente en su trono, cual l\u00edder constante,   Es Gustavo Petro, hombre de ferviente voz,   Que al tim\u00f3n del destino, la naci\u00f3n \u00e9l atroz.   Con sue\u00f1os de cambio, justicia y verdad,   Dirige su pueblo hacia la prosperidad.   As\u00ed, en sus manos, el futuro bien brilla,   Un eco de esperanza en la tierra sencilla.</code></p>"},{"location":"Unidad%201/modulo1/#langchain","title":"LangChain","text":"<p>En la secci\u00f3n anterior, tuviste tu primera interacci\u00f3n con un modelo de lenguaje de gran escala (LLM). A medida que esta tecnolog\u00eda madura, empresas, gobiernos y startups bien financiadas, como OpenAI, Anthropic, xAI y Meta AI, han desarrollado y puesto a disposici\u00f3n modelos y APIs con arquitecturas y protocolos de comunicaci\u00f3n particulares. Esto ha generado la necesidad de realizar llamadas a estos modelos de manera agn\u00f3stica, es decir, independientemente del modelo o proveedor utilizado.</p> <p>En este contexto, el framework m\u00e1s popular hasta el momento es LangChain. LangChain permite realizar las mismas tareas que podr\u00edamos llevar a cabo directamente con las APIs de los modelos, pero a trav\u00e9s de abstracciones de validez general. Este marco proporciona una interfaz unificada que simplifica la integraci\u00f3n con diferentes LLMs, el manejo de prompts, la gesti\u00f3n de contexto y la incorporaci\u00f3n de herramientas externas, como bases de datos o funciones personalizadas. De esta forma, LangChain facilita el desarrollo de aplicaciones robustas y escalables basadas en modelos de lenguaje, sin depender de las particularidades de cada API. </p> <p>Logo de LangChain, un framework para construir aplicaciones con modelos de lenguaje de gran escala. Fuente: Matt Gallo en LinkedIn.</p> <p>Para utilizar LangChain con modelos de OpenAI, primero debemos importar la clase <code>ChatOpenAI</code> y configurar el modelo:</p> <p><pre><code>from langchain_openai import ChatOpenAI\nimport os\n\n# Definimos el modelo de lenguaje\nllm_model = \"gpt-4o-mini\"\n\n# Inicializamos el modelo de chat de OpenAI con LangChain\nchat_model = ChatOpenAI(\n    model=llm_model\n)\n</code></pre> Y listo, eso es todo. Ahora simplemente invocamos el chat con el prompt que queramos. Por ejemplo:</p> C\u00f3digoSalida <pre><code># Invocamos el modelo de chat con un prompt\nresponse = chat_model.invoke(\"\u00bfC\u00f3mo se llama el presidente de Colombia?\")\nprint(response)\n</code></pre> <pre><code>A partir de mi \u00faltima actualizaci\u00f3n en octubre de 2023, el presidente de Colombia es Gustavo Petro, quien asumi\u00f3 el cargo el 7 de agosto de 2022. Sin embargo, te recomiendo verificar esta informaci\u00f3n, ya que puede haber cambios pol\u00edticos o elecciones que alteren la situaci\u00f3n.\n</code></pre>"},{"location":"Unidad%201/modulo1/#herramientas-en-langchain","title":"Herramientas en LangChain","text":"<p>LangChain proporciona una variedad de herramientas que permiten construir aplicaciones basadas en modelos de lenguaje de manera modular y eficiente. A continuaci\u00f3n, se describen algunas de las m\u00e1s importantes:</p> <ul> <li> <p>Models (Modelos)   Representan los modelos de lenguaje que LangChain puede integrar, como <code>ChatOpenAI</code>. Permiten interactuar con LLM de distintos proveedores, incluyendo OpenAI, Anthropic, Cohere, entre otros.   Representan los modelos de lenguaje que LangChain puede integrar, como <code>ChatOpenAI</code>. Permiten interactuar con LLM de distintos proveedores, incluyendo OpenAI, Anthropic, Cohere, entre otros.</p> </li> <li> <p>Prompts (Prompts)   Herramientas para dise\u00f1ar y gestionar prompts, como <code>ChatPromptTemplate</code>. Facilitan la construcci\u00f3n de entradas din\u00e1micas, reutilizables y bien estructuradas para los modelos.</p> </li> <li> <p>Example Selectors (Selectores de Ejemplos)   Componentes que permiten seleccionar ejemplos relevantes (por ejemplo, para few-shot learning). Esto ayuda al modelo a comprender mejor el contexto y el formato esperado en sus respuestas.</p> </li> <li> <p>Tools (Herramientas)   Permiten que el modelo interact\u00fae con funciones externas, como APIs, calculadoras, o bases de datos. Son esenciales para extender las capacidades del LLM m\u00e1s all\u00e1 del texto, habilitando tareas como b\u00fasqueda en tiempo real o ejecuci\u00f3n de funciones personalizadas.</p> </li> <li> <p>Vector Stores (Almacenes de Vectores)   Bases de datos vectoriales como Chroma, Pinecone o FAISS. Se utilizan para almacenar y buscar embeddings, habilitando funcionalidades como la b\u00fasqueda sem\u00e1ntica o la generaci\u00f3n aumentada por recuperaci\u00f3n (Retrieval-Augmented Generation, RAG).</p> </li> <li> <p>Document Loaders (Cargadores de Documentos)   Permiten cargar datos desde m\u00faltiples fuentes (archivos PDF, p\u00e1ginas web, bases de datos, etc.) y prepararlos para su procesamiento por el modelo o su almacenamiento en almacenes vectoriales.</p> </li> <li> <p>Text Splitters (Divisores de Texto)   Herramientas que dividen documentos largos en fragmentos m\u00e1s peque\u00f1os. Esto facilita tanto el procesamiento por parte del modelo como la indexaci\u00f3n eficiente en almacenes vectoriales.</p> </li> <li> <p>Output Parsers (Parsers de Salida)   Utilizados para estructurar y formatear las respuestas del modelo. Por ejemplo, permiten convertir la salida del modelo en JSON, listas, tablas o formatos espec\u00edficos para una aplicaci\u00f3n.</p> </li> </ul> <p>REEMPLAZAR IMAGEN POR ESTA: https://drive.google.com/file/d/1oCSWBu03JBg1nWFZIRJBfshsllp0ozJ1/view?usp=drive_link </p> <p>Ecosistema de herramientas de LangChain. Fuente: Elaboraci\u00f3n propia.</p>"},{"location":"Unidad%201/modulo1/#plantillas-de-prompts","title":"Plantillas de Prompts","text":"<p>Comenzaremos estudiando los prompt templates. Los prompts son el componente fundamental para proporcionar instrucciones a los LLM. Al desarrollar aplicaciones asistidas por inteligencia artificial, es \u00fatil crear plantillas de prompts que permitan personalizar las instrucciones de forma din\u00e1mica. Estas plantillas mantienen constante una parte de la instrucci\u00f3n mientras incorporan elementos variables, como valores proporcionados durante la ejecuci\u00f3n, a trav\u00e9s de variables de entrada. Comenzaremos estudiando los prompt templates. Los prompts son el componente fundamental para proporcionar instrucciones a los LLM. Al desarrollar aplicaciones asistidas por inteligencia artificial, es \u00fatil crear plantillas de prompts que permitan personalizar las instrucciones de forma din\u00e1mica. Estas plantillas mantienen constante una parte de la instrucci\u00f3n mientras incorporan elementos variables, como valores proporcionados durante la ejecuci\u00f3n, a trav\u00e9s de variables de entrada.</p> <p>Por ejemplo, una plantilla puede definir la estructura de una pregunta, dejando espacios para insertar valores espec\u00edficos, como el nombre de un pa\u00eds. Esto se logra utilizando herramientas como <code>ChatPromptTemplate</code> de LangChain, que simplifica la creaci\u00f3n de prompts reutilizables.</p> <p>En el siguiente ejemplo, se muestra c\u00f3mo crear una plantilla para consultar el presidente de un pa\u00eds, utilizando una variable de entrada <code>{pais}</code> que puede tomar diferentes valores sin modificar la estructura general del prompt.</p> C\u00f3digoSalida <pre><code>from langchain.prompts import ChatPromptTemplate\n\n# Definir la plantilla con una variable de entrada\nstr_template = \"\u00bfC\u00f3mo se llama el presidente de {pais}?\"\nprompt_template = ChatPromptTemplate.from_template(str_template)\n\n# Asignar un valor a la variable de entrada\npais = \"Colombia\"\nprompt1 = prompt_template.format(pais=pais)\nprint(prompt1)\n\n# Asignar otro valor a la variable de entrada\npais = \"Francia\"\nprompt2 = prompt_template.format(pais=pais)\nprint(prompt2)\n</code></pre> <pre><code>\u00bfC\u00f3mo se llama el presidente de Colombia?\n\u00bfC\u00f3mo se llama el presidente de Francia?\n</code></pre> <p>En este caso, <code>{pais}</code> es una variable de entrada a la que podemos asignar diferentes valores (por ejemplo, \"Colombia\", \"Argentina\", etc.) sin cambiar la estructura general del prompt. Esto hace que la plantilla sea flexible y reutilizable.</p> <p>Veamos ahora un ejemplo pr\u00e1ctico en el que utilizamos dos variables de entrada en nuestro template:</p> <pre><code>mensaje = \"\"\nestilo = \"\"\n</code></pre> <p>Definimos nuestro <code>string_template</code> de la siguiente manera:</p> <pre><code>string_template = (\n    \"Traduce el texto que est\u00e1 delimitado por asteriscos dobles a un estilo que es {estilo}.\\n\"\n    \"texto: **{mensaje}**\"\n)\n</code></pre> <p>Aqu\u00ed, el <code>string_template</code> contiene las instrucciones generales, mientras que <code>mensaje</code> y <code>estilo</code> son variables que dejamos vac\u00edas para llenarlas m\u00e1s tarde. Luego, confeccionamos el prompt template utilizando:</p> <pre><code>prompt_template = ChatPromptTemplate.from_template(string_template)\n</code></pre> <p>En esta l\u00ednea usamos el m\u00e9todo <code>from_template</code> de la clase <code>ChatPromptTemplate</code>. Si imprimimos el objeto <code>prompt_template</code> con:</p> <pre><code>print(prompt_template)\n</code></pre> Salida <pre><code>input_variables=['estilo', 'mensaje']\ninput_types={}\npartial_variables={}\nmessages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['estilo', 'mensaje'], input_types={}, partial_variables={}, template='Traduce el texto que est\u00e1 delimitado por asteriscos dobles a un estilo que es {estilo}.\\ntexto: **{mensaje}**'), additional_kwargs={})]\n</code></pre> <p>Veremos que tiene como <code>input_variables</code> los campos <code>'estilo'</code> y <code>'mensaje'</code>.</p> <p>Siguiendo la l\u00f3gica del paradigma de la programaci\u00f3n orientada a objetos, podemos imaginar que la creaci\u00f3n de un prompt template se asemeja al trabajo de un carpintero. El carpintero (el constructor de la clase) toma un conjunto de maderas (el <code>string_template</code>) y las transforma en un gavetero (el objeto de la clase). </p> <p>Constructor de la clase <code>ChatPromptTemplate.from_template</code>. En nuestra analog\u00eda, el carpintero crea un contenedor apropiado para alojar el contenido de las dos variables de entrada definidas en el <code>string_template</code>. Fuente: Elaboraci\u00f3n propia.</p> <p>En este caso, como ilustra la figura, el prompt template ser\u00eda el gavetero con cajones espec\u00edficos etiquetados como <code>estilo</code> y <code>mensaje</code>, listos para ser llenados con valores.</p> <p>Supongamos que asignamos a estas variables de entrada los valores:</p> <pre><code>mensaje_atioquenhol = (\n    \"Manque estaba muy embelesado, le dijo Peralta a la hermana: \"\n    \"Hija, date una asoma\u00edta por la despensa; desculc\u00e1 por la cocina, \"\n    \"a ver si encontr\u00e1s algo que darles a estos se\u00f1ores. \"\n    \"M\u00edralos qu\u00e9 cansados est\u00e1n; se les ve la fatiga.\"\n)\n\nestilo_formal = \"Espa\u00f1ol latino en un tono formal y sobrio\"\n</code></pre> <p>El m\u00e9todo <code>format_messages</code> nos permite llenar los cajones del gavetero, es decir, las variables de entrada, con los valores espec\u00edficos con los que queremos completar nuestro prompt. Por ejemplo, si queremos que <code>estilo = estilo_formal</code>, podemos hacerlo de la siguiente manera:</p> <pre><code>mensaje_empacado = prompt_template.format_messages(estilo=estilo_formal, mensaje=mensaje_atioquenhol)\n</code></pre> <p>El prompt completo lucir\u00e1 as\u00ed:</p> C\u00f3digoSalida <pre><code>print(mensaje_empacado)\n</code></pre> <pre><code>[HumanMessage(content='Traduce el texto que est\u00e1 delimitado por asteriscos dobles a un estilo que es Espa\u00f1ol latino en un tono formal y sobrio.\\ntexto: **Manque estaba muy embelesao, le dijo Peralta a la hermana: Hija, date una asoma\u00edta por la despensa; desculc\u00e1 por la cocina, a ver si encontr\u00e1s alguito que darles a estos se\u00f1ores. Mir\u00e1los qu\u00e9 cansaos est\u00e1n; se les ve la fatiga**', additional_kwargs={}, response_metadata={})]\n</code></pre> <p></p> <p>Ilustraci\u00f3n de la tarea del <code>format_messages()</code>. El m\u00e9todo <code>format_messages()</code> reemplaza los valores de las variables de entrada en el template. Fuente: Elaboraci\u00f3n propia.</p> <p>Como ilustra la figura, el m\u00e9todo <code>format_messages()</code> asociado a la clase <code>ChatPromptTemplate</code> cumple la funci\u00f3n de empaquetar en el objeto los valores espec\u00edficos en las variables de entrada.</p> <p>Este tipo de objeto nos permite incorporar program\u00e1ticamente llamadas a las APIs de los LLM en el flujo de ejecuci\u00f3n de un c\u00f3digo Python convencional. Veamos c\u00f3mo hacerlo: Este tipo de objeto nos permite incorporar program\u00e1ticamente llamadas a las APIs de los LLM en el flujo de ejecuci\u00f3n de un c\u00f3digo Python convencional. Veamos c\u00f3mo hacerlo:</p> <p>Como ya tenemos nuestro prompt completo y lleno con las variables que queremos, lo podemos enviar al LLM:</p> <p>Primero, instanciamos un chat:</p> <pre><code>chat = ChatOpenAI(model=llm_model, temperature=0.0)\n</code></pre> <p>Luego, realizamos la llamada al LLM para que ejecute las instrucciones del prompt:</p> <pre><code>respuesta = chat(mensaje_empacado)\nprint(respuesta.content)\n</code></pre> Salida <pre><code>Manque se encontraba muy absorto, le dijo Peralta a la hermana:\n\"Hija, por favor, as\u00f3mate a la despensa; revisa en la cocina\npara ver si encuentras algo que ofrecerles a estos caballeros.\nObserva c\u00f3mo est\u00e1n de cansados; se les nota la fatiga.\"\n</code></pre> <p>El LLM recibe el mensaje empacado y realiza las tareas especificadas por el prompt.</p> <p>Lo interesante es que este no es un prompt fijo como los que usar\u00edamos en ChatGPT; es un prompt que nos permite hacer llamadas al LLM de manera m\u00e1s flexible y program\u00e1tica. Por ejemplo, podr\u00edamos definir otro valor para <code>estilo</code>, como:</p> <pre><code>estilo_cervantes = \"Espa\u00f1ol en un estilo de Cervantes, como en Don Quijote\"\n</code></pre> <pre><code>mensaje_empacado = prompt_template.format_messages(estilo=estilo_cervantes, mensaje=mensaje_atioquenhol)\nrespuesta = chat(mensaje_empacado)\nprint(respuesta.content)\n</code></pre> Salida <pre><code>  Manque se hallaba en un profundo embeleso, dirigi\u00f3  \n  Peralta a la hermana la siguiente exhortaci\u00f3n: \"Hija,\n   as\u00f3mate, por favor, a la despensa; y, si no es mucho \n   pedir, desc\u00fabrete por la cocina, a ver si logras \n   hallar alg\u00fan manjar que ofrecer a estos nobles se\u00f1ores. \n   Observa c\u00f3mo se encuentran, qu\u00e9 cansados est\u00e1n; la fatiga\n  se les dibuja en el semblante.\"\n</code></pre> <p>Este enfoque nos permite variar el estilo del texto generado de manera din\u00e1mica, adaptando el resultado a diferentes necesidades o contextos, simplemente modificando las variables de entrada del prompt.</p> Reto formativoVer soluci\u00f3n <ul> <li> Reto formativo Planteamiento:   Dado un mensaje de un cliente, un operador humano de servicio al cliente elabora una respuesta inadecuada (irrespetuosa, ofensiva, con mala ortograf\u00eda o en otro idioma). Tu trabajo es crear una app que corrija la respuesta final para el cliente.</li> </ul> <p>Compara tu soluci\u00f3n con la siguienete implementaci\u00f3n:</p> <p><pre><code>      # Define una plantilla de texto para el prompt que se enviar\u00e1 al modelo de lenguaje.\n  # Usa marcadores {respuesta} y {reglas} para insertar din\u00e1micamente la respuesta y las reglas.\n  str_template_app = \"\"\"Mejora la respuesta: {respuesta}\\\n      para que cumpla las reglas:  {reglas}.\"\"\"\n\n  # Define las reglas que debe seguir la respuesta mejorada.\n  # Especifica el idioma, tono, gram\u00e1tica y nivel de amabilidad requerido.\n  reglas = \"Espa\u00f1ol latino en un tono formal y sobrio y respesuoso. Con buena gram\u00e1tica y ortograf\u00eda. Trartar de se muy amable y respetuoso.\"\n\n  # Define la respuesta original del operador, que es inadecuada (informal, ofensiva, con mala ortograf\u00eda).\n  respuesta =  \" mijo, no me importa si le sali\u00f3 mala \\\n      la licudora, vaya a que se lo lamba un zapo\"\n\n  # Crea una plantilla de prompt usando la biblioteca LangChain, basada en la plantilla de texto.\n  # Esto permite estructurar el mensaje para el modelo de lenguaje.\n  promp_template_app = ChatPromptTemplate.from_template(str_template_app)\n\n  # Formatea la plantilla con la respuesta y las reglas, generando un mensaje listo para enviar al modelo.\n  mensaje_empacado_app =  promp_template_app.format_messages(respuesta=respuesta, reglas=reglas)\n\n  # Especifica el modelo de lenguaje a usar (en este caso, GPT-4o-mini de OpenAI).\n  llm_model = \"gpt-4o-mini\"\n\n  # Inicializa el cliente de chat de OpenAI con el modelo especificado y una temperatura de 0.3.\n  # La temperatura baja asegura respuestas m\u00e1s predecibles y menos creativas.\n  chat_app = ChatOpenAI(model = llm_model , temperature = 0.3)\n\n  # Env\u00eda el mensaje formateado al modelo y obtiene la respuesta mejorada.\n  respuesta_al_cliente = chat_app(mensaje_empacado_app)\n\n  # Muestra la respuesta del modelo en formato Markdown para una mejor presentaci\u00f3n (por ejemplo, en un entorno como Jupyter).\n  display(Markdown(respuesta_al_cliente.content))\n</code></pre> salida esperada Agradezco su mensaje y entiendo su preocupaci\u00f3n respecto a la situaci\u00f3n con la licuadora. Sin embargo, le sugiero que considere la posibilidad de llevar el aparato a un servicio t\u00e9cnico autorizado para que puedan evaluar el problema y ofrecerle una soluci\u00f3n adecuada. Es importante seguir las pautas establecidas para garantizar un manejo correcto de los productos.</p> <p>Quedo a su disposici\u00f3n para cualquier otra consulta o asistencia que necesite.</p>"},{"location":"Unidad%201/modulo1/#anatomia-de-un-prompt-de-chat","title":"Anatom\u00eda de un Prompt de Chat","text":"<p>Los prompts para agentes conversacionales en LangChain, como <code>ChatPromptTemplate</code>, se dividen en al menos tres componentes clave. Veamos cada uno:</p>"},{"location":"Unidad%201/modulo1/#1-prompt-del-sistema","title":"1. Prompt del Sistema","text":"<p>Este establece las reglas para el asistente. Indica al modelo c\u00f3mo comportarse, cu\u00e1l es su objetivo o incluso qu\u00e9 tono debe usar.</p> <p>Ejemplo: <pre><code>Eres experto en machine learning y das respuestas en una sola oraci\u00f3n.\n</code></pre></p> <p>Aqu\u00ed estamos restringiendo al modelo para que mantenga las respuestas cortas en un lenguaje relativo al machine learning.</p>"},{"location":"Unidad%201/modulo1/#2-prompt-del-usuario","title":"2. Prompt del Usuario","text":"<p>Este es el mensaje del usuario, es decir, la pregunta o entrada que se le proporciona al modelo.</p> <p>Ejemplo: <pre><code>Explica {tema} en una sola oraci\u00f3n.\n</code></pre></p> <p>El <code>{tema}</code>, como vimos, es una variable de entrada que podemos cambiar por diferentes t\u00e9rminos, como \"LangChain\" o \"Python\".</p>"},{"location":"Unidad%201/modulo1/#3-prompt-del-ai","title":"3. Prompt del AI","text":"<p>Este es el resultado generado por el modelo. En una conversaci\u00f3n, las respuestas anteriores del AI se reutilizan como parte del historial de chat.</p> <p>Por ahora, mantenemos un solo turno de interacci\u00f3n humano-AI en el que el modelo no tiene memoria del contexto de las interacciones anteriores, pero m\u00e1s adelante veremos c\u00f3mo se puede construir una conversaci\u00f3n m\u00e1s compleja.</p> <p>El <code>ChatPromptTemplate</code> de LangChain ofrece dos formas principales de construir prompts:</p>"},{"location":"Unidad%201/modulo1/#1-from_messages","title":"1. <code>from_messages</code>","text":"<p>Piensa en esto como escribir un guion para una conversaci\u00f3n estructurada: - El mensaje del sistema define el tono y las reglas. - El mensaje del usuario plantea la pregunta o el input.  </p> <p>Es la opci\u00f3n recomendada cuando queremos prompts bien organizados.</p>"},{"location":"Unidad%201/modulo1/#2-from_template","title":"2. <code>from_template</code>","text":"<p>M\u00e1s simple y directo, solo incluye un mensaje del usuario, como una nota r\u00e1pida para el modelo. - No tiene un rol de sistema a menos que lo agreguemos manualmente m\u00e1s adelante.</p> <p>En la sesi\u00f3n anterior usamos <code>from_template</code>:</p> <p>Veamos un ejemplo usamndo <code>from_messages</code>:</p> <pre><code># Importamos las librer\u00edas necesarias\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.prompts import ChatPromptTemplate\n</code></pre> <p>Vamos a instanciar dos modelos para comparar las respuestas al final:</p> <pre><code># Instanciamos los modelos\nllm_gpt3 = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.7)\nllm_gpt4 = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\n\n# Definimos el prompt\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a concise explainer who gives one-sentence answers. If you don't know the answer, just say 'I don't know'.\"),\n    (\"human\", \"Explain {topic} in one sentence.\")\n])\n</code></pre> <p>La variable de entrada es <code>topic</code> y debemos empacarla en nuestro template.</p> <p>Llenamos el prompt con el t\u00f3pico espec\u00edfico:</p> <pre><code>messages = prompt.format_messages(topic=\"LangChain\")\n</code></pre> <p>Ejecutamos los dos modelos:</p> C\u00f3digo con gpt3.5SalidaC\u00f3digo con gpt4Salida <pre><code>response = llm_gpt3.invoke(messages)\nprint(response.content)\n</code></pre> <pre><code>LangChain is a blockchain platform\nthat aims to facilitate cross-border\nlanguage services.\n</code></pre> <pre><code>response = llm_gpt4.invoke(messages)\nprint(response.content)\n</code></pre> <pre><code>LangChain es una biblioteca para crear\nflujos de trabajo de IA utilizando modelos\nde lenguaje.\n</code></pre> <p>Para tener en cuenta</p> <p>Observa que la salida del modelo gpt-3.5 es completamente alucinada (no es verdadera). \u00bfA qu\u00e9 crees que se debe esto? </p> Ver respuesta <p>El modelo gpt-3.5 fue entrenado en datos hasta octubre de 2023, y en ese momento LangChain no exist\u00eda.</p>"},{"location":"Unidad%201/modulo1/#de-prompts-a-chains","title":"De Prompts a Chains","text":"<p>Hasta ahora, hemos preparado prompts y los hemos enviado al LLM paso a paso.  </p> <p>Pero LangChain tiene una herramienta que facilita mas las cosas: las chains-</p> <p>Las chains nos permiten combinar m\u00faltiples pasos\u2014como preparar un prompt y ejecutar el LLM\u2014en un flujo continuo y automatizado.  </p> <p>puedes pensar en unachain como una cinta transportadora:</p> <p></p> <p>Una cadena simple funciona como una banda transportadora en la que se van ejecutando \u00f3rdenes de forma secuencial.</p> <ul> <li>La configuras una vez.  </li> <li>Luego, simplemente funciona sin necesidad de repetir cada paso manualmente.  </li> </ul> <p>Esto facilita la construcci\u00f3n de pipelines m\u00e1s avanzados dentro de nuestras aplicaciones con LLM. Una forma de encadenar ejecuciones en cadenas es utilizar el operador <code>|</code> (llamado pipe) para conectar los pasos. Para instanciar una cadena que realice las tareas de nuestro prompt anterior, tendr\u00edamos el prompt como:</p> <p><pre><code>prompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a concise explainer who gives one-sentence answers.\"),\n    (\"human\", \"Explain {topic} in one sentence.\")\n])\n</code></pre> E instanciamos la cadena como:</p> <pre><code>chain = prompt | llm_gpt4  # Create the chain\n</code></pre> <p>Es como decir: *\"Toma este prompt y p\u00e1salo al LLM.\"</p> <p>Y ejecutamos la cadena como:</p> C\u00f3digoSalida <pre><code>response = chain.invoke({\"topic\": \"LangChain\"})  # Run it in one go\nprint(\"With chain:\", response.content)\n</code></pre> <pre><code>With chain: LangChain es una biblioteca para crear flujos de trabajo de IA utilizando modelos de lenguaje.\n</code></pre> <p>Las chains nos evitan tener que formatear e invocar manualmente el LLM cada vez.</p> <ul> <li>Definimos la cadena una vez.</li> <li>Podemos reutilizarla f\u00e1cilmente.</li> </ul> <p>Esto simplifica el flujo de trabajo y hace que el c\u00f3digo sea m\u00e1s limpio y modular. Ya no necesitamos formatear manualmente los mensajes\u2014la chain lo hace por nosotros.  </p> <p>M\u00e9todo Antiguo (Manual): <pre><code>messages = prompt.format_messages()  \nllm.invoke(messages)\n</code></pre> Una vez configurada la cadena, podemos reutilizarla con diferentes variables de entrada:</p> C\u00f3digoSalida <pre><code>print(chain.invoke({\"topic\": \"Python\"}).content)\nprint(chain.invoke({\"topic\": \"AI\"}).content)\n</code></pre> <pre><code>Python es un lenguaje de programaci\u00f3n vers\u00e1til y popular.\nAI es el campo de la inform\u00e1tica que se centra en crear sistemas inteligentes.\n</code></pre>"},{"location":"Unidad%201/modulo1/#cadenas-con-multiples-variables","title":"Cadenas con m\u00faltiples variables","text":"<p>Veamos algunos ejemplos en los que usamos m\u00faltiples variables en nuestros prompts:</p> C\u00f3digoSalida <pre><code># Nuevo prompt con dos variables: topic y style\nmulti_prompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are an explainer who answers in a {style} way.\"),\n    (\"human\", \"Explain {topic} in one sentence.\")\n])\nmulti_chain = multi_prompt | llm\n\n# Ejecutar con m\u00faltiples variables\nresponse = multi_chain.invoke({\n\n# Run with multiple variables\nresponse = multi_chain.invoke({\n    \"topic\": \"Noether theorem\",\n    \"style\": \"Cervantes style in Spanish\"\n})\nprint(response.content)\n</code></pre> <pre><code>\u00a1Por la fe de Dulcinea del Toboso! La teorema \nde Noether establece que para cada simetr\u00eda continua\nde un sistema f\u00edsico, existe una cantidad conservada!\n</code></pre>"},{"location":"Unidad%201/modulo1/#output-parsers-dando-forma-a-la-salida-del-llm","title":"Output Parsers: Dando Forma a la Salida del LLM","text":"<p>Los LLM son sistemas que reciben texto plano y devuelven texto, incluso cuando devuelven im\u00e1genes, lo que realmente est\u00e1n haciendo en el fondo es generar descripciones textuales de esas im\u00e1genes. Sin embargo, cuando estamos construyendo aplicaciones asistidas por LLM, lo que queremos es utilizar la salida de la llamada al LLM para emplearla en otros flujos de ejecuci\u00f3n de nuestra aplicaci\u00f3n. Los LLM son sistemas que reciben texto plano y devuelven texto, incluso cuando devuelven im\u00e1genes, lo que realmente est\u00e1n haciendo en el fondo es generar descripciones textuales de esas im\u00e1genes. Sin embargo, cuando estamos construyendo aplicaciones asistidas por LLM, lo que queremos es utilizar la salida de la llamada al LLM para emplearla en otros flujos de ejecuci\u00f3n de nuestra aplicaci\u00f3n.</p> <p>Ah\u00ed es donde entran los output parsers.</p> <p>Los output parsers toman la salida en bruto del LLM y la convierten en algo que podamos usar en nuestro c\u00f3digo, como un string, una lista, un diccionario, un JSON, etc.</p> <p>Ejemplo: - Si el LLM responde con <code>\"Las herramientas m\u00e1s usadas son: Python, SQL, LangChain.\"</code>, podemos transformarlo en una lista <code>[\"Python\", \"SQL\", \"LangChain\"]</code>.</p> <p>Vemos algunos mas usados:</p>"},{"location":"Unidad%201/modulo1/#stroutputparser-el-parser-mas-basico","title":"<code>StrOutputParser</code>: El Parser M\u00e1s B\u00e1sico","text":"<p>Comencemos con un output parser b\u00e1sico: <code>StrOutputParser</code>. Este parser simplemente asegura que la salida de la llamada al LLM sea un string. En el contexto de LangChain, esto es \u00fatil para garantizar que los datos procesados sean siempre de tipo string, facilitando su manipulaci\u00f3n posterior. Una vez instanciado, puede agregarse a la cadena para que, al invocarla, la salida sea en el formato especificado por el parser. Para ilustrar el uso de los parsers, veamos esta cadena sin parser y comparemosla con el resultado cuando agregamos el <code>StrOutputParser</code>.</p> C\u00f3digo sin ParserSalida <pre><code>multi_prompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are an explainer who answers in a {style} way.\"),\n    (\"human\", \"Explain {topic} in one sentence.\")\n])\nmulti_chain = multi_prompt | llm  # LCEL\nresponse = multi_chain.invoke({\"topic\": \"LangChain\", \"style\": \"funny\"})\nprint(\"Raw output:\", response)\n</code></pre> <pre><code>AIMessage(content='LangChain is like that friend who translates all your texts for you, but in a more high-tech and less judgmental way.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 31, 'total_tokens': 58, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-d4e4009b-87db-40ca-89de-9fe42d850dab-0', usage_metadata={'input_tokens': 31, 'output_tokens': 27, 'total_tokens': 58, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})\n</code></pre> <p>Aqu\u00ed el string de salida est\u00e1 dentro de la instancia <code>content</code> del <code>AIMessagePara</code>. El parser nos posibilitar\u00e1 que la salida sea solo el string. Para  usar el parser, importamos el m\u00f3dulo:</p> <pre><code>from langchain_core.output_parsers import StrOutputParser\n</code></pre> <p>E instanciamos el parser como:</p> <pre><code># Add the parser to the chain\nparser = StrOutputParser()\n</code></pre> <p>Luego lo agregamos a la cadena con el operador pipe.</p> C\u00f3digo con ParserSalida con el Parser <pre><code>parsed_chain = multi_prompt | llm | parser\n\n# Run it\nresponse = parsed_chain.invoke({\"topic\": \"LangChain\", \"style\": \"funny\"})\nprint(\"Parsed output:\", response)\n</code></pre> <pre><code>Parsed output: LangChain is like a multilingual party where everyone speaks their own language but magically understands each other perfectly.\n</code></pre> <p>Para tener en cuenta</p> <p><code>StrOutputParser</code>  - Extrae el <code>.content</code> del objeto <code>AIMessage</code> generado por el LLM. - Nos garantiza que obtenemos solo el texto limpio sin informaci\u00f3n adicional.  </p> <p>\ud83d\udd39 Sin el parser: Obtenemos un objeto <code>AIMessage</code> y debemos extraer manualmente <code>.content</code>.  </p> <p>\ud83d\udd39 Con el parser: Recibimos directamente el texto limpio.  </p> <p>\u2705 Es un peque\u00f1o avance, pero marca la diferencia. Nos ahorra pasos manuales y sienta la base para mejoras m\u00e1s avanzadas.  </p>"},{"location":"Unidad%201/modulo1/#commaseparatedlistoutputparser","title":"<code>CommaSeparatedListOutputParser</code>","text":"<p>Este parser toma una cadena de texto separada por comas y la convierte en una lista estructurada.</p> <p>Ejemplo: Entrada: <code>\"manzana, banana, cereza\"</code> Salida: <code>[\"manzana\", \"banana\", \"cereza\"]</code> </p> <p>Ve\u00e1moslo en detalle:</p> C\u00f3digoSalida <pre><code># Import the parser\nfrom langchain_core.output_parsers import CommaSeparatedListOutputParser\n\n# New prompt asking for a list\nlist_prompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant that delivers comma-separated items.\"),\n    (\"human\", \"Give me 8 examples of {category}, separated by commas, omit additional comments only list the objects like a Python list.\")\n])\n\n# Create the chain with the new parser\nlist_parser = CommaSeparatedListOutputParser()\nlist_chain = list_prompt | llm | list_parser\n\n# Run it\nresponse = list_chain.invoke({\"category\": \"programming languages\"})\nprint(\"List output:\", response)\n</code></pre> <pre><code>List output: ['Python', 'Java', 'C++', 'JavaScript', 'Ruby', 'Swift', 'PHP', 'Go']\n</code></pre> <p>Puedes verificar que el tipo de la salida es una lista:</p> C\u00f3digoSalida <pre><code>print(type(response))\n</code></pre> <pre><code>&lt;class 'list'&gt;\n</code></pre> <p>Esto nos permite, por ejemplo, usar \u00edndices para acceder a los elementos de la lista:</p> C\u00f3digoSalida <pre><code>response[1]\n</code></pre> <pre><code>'Java'\n</code></pre> <p>Es decir, el LLM nos proporciona texto, pero el parser lo convierte en una lista que podemos usar en c\u00f3digo. Los parsers nos permiten agregar pasos adicionales a las cadenas de ejecuci\u00f3n; puedes pensarlo como una l\u00ednea de ensamblaje con el flujo:</p> <p><pre><code>[Prompt] --&gt; [LLM] --&gt; [Parser] --&gt; Structured Output\n</code></pre> </p> <p>Analog\u00eda de una cadena con parser. Las instrucciones se ejecutan en orden como en una l\u00ednea de ensamblaje.</p>"},{"location":"Unidad%201/modulo1/#jsonoutputparser","title":"<code>JsonOutputParser</code>","text":"<p>El <code>JsonOutputParser</code> es un parser que toma una cadena de texto en formato JSON y la convierte en un objeto de Python, como un diccionario o una lista, dependiendo de la estructura del JSON. Esto es particularmente \u00fatil cuando trabajamos con datos estructurados que vienen de una base de datos en la nube o de una API.</p> <p>Ejemplo:</p> C\u00f3digo <pre><code>from langchain_core.output_parsers import JsonOutputParser\n\n# Prompt asking for JSON with varied types\njson_prompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"Return a JSON object with 'name' (string), 'age' (number or null), 'is_student' (true/false), and 'city' (string or null).\"),\n    (\"human\", \"Give me details for {person} in JSON format.\")\n])\n\n# Create the chain with the parser\njson_chain = json_prompt | llm | JsonOutputParser()\n\n# Chain without parser\nno_parse_chain = json_prompt | llm\n</code></pre> <p>Para ver m\u00e1s claramente lo que hace el <code>JsonOutputParser</code>, corramos la cadena sin el parser:</p> C\u00f3digo sin ParserSalida <pre><code>no_parse_response = no_parse_chain.invoke({\"person\": \"Alice\"}).content\nprint(\"Respuesta sin parser:\", no_parse_response)\nprint(\"Type:\", type(no_parse_response))\n</code></pre> <pre><code>Respuesta sin parser: {\"name\": \"Alice\", \"age\": 30, \"is_student\": false, \"city\": \"Paris\"}\nType: &lt;class 'str'&gt;\n</code></pre> <p>La respuesta es un <code>str</code>, es decir, texto simple, por lo que no puedo acceder a los elementos del JSON usando <code>keys</code>:</p> C\u00f3digoSalida <pre><code># Esto causar\u00e1 un error\nno_parse_response[\"name\"]\n</code></pre> <pre><code>TypeError                                 Traceback (most recent call last)\nCell In[34], line 1\n----&gt; 1 no_parse_response[\"name\"]\n\nTypeError: string indices must be integers, not 'str'\n</code></pre> <p>Ahora, ejecutemos la cadena con el <code>JsonOutputParser</code>:</p> C\u00f3digo con ParserSalida <pre><code>json_response = json_chain.invoke({\"person\": \"Alice\"})\nprint(\"JsonOutputParser result:\", json_response)\nprint(\"Type:\", type(json_response))\n</code></pre> <pre><code>JsonOutputParser result: {'name': 'Alice', 'age': 30, 'is_student': False, 'city': 'Paris'}\nType: &lt;class 'dict'&gt;\n</code></pre> <p>Puedes acceder a los valores del JSON usando claves:</p> C\u00f3digoSalida <pre><code>json_response[\"city\"]\n</code></pre> <pre><code>'Paris'\n</code></pre> <p>El LLM produce JSON: <code>{\"name\": \"Alice\", \"age\": 30, \"is_student\": false, \"city\": \"Paris\"}</code>. <code>JsonOutputParser</code> convierte <code>null</code> en JSON a <code>None</code> en Python, <code>true/false</code> a <code>True/False</code>, y retorna un diccionario de Python.</p> <ul> <li> <p> Reto Formativo Planteamiento:       El siguiente es el comentario de un cliente en una tienda virtual:</p> <p><code>review_cliente = \"Compr\u00e9 los auriculares inal\u00e1mbricos XYZ y estoy muy satisfecho con mi compra. El tiempo de entrega fue excelente, ya que llegaron dos d\u00edas antes de lo previsto, lo cual super\u00f3 mis expectativas. En cuanto al precio, aunque hay opciones m\u00e1s econ\u00f3micas en el mercado, considero que la calidad del sonido, la duraci\u00f3n de la bater\u00eda y la comodidad justifican totalmente el coste. Adem\u00e1s, los compr\u00e9 como regalo para mi pareja y fueron un \u00e9xito total, gracias a su elegante presentaci\u00f3n y la impresionante calidad del sonido.\"</code></p> <p>Extraer informaci\u00f3n de la rese\u00f1a en un objeto JSON <code>Rese\u00f1as</code>.</p> <p>\ud83d\udce5 Entrada   Cadena <code>review_cliente</code> con la opini\u00f3n del cliente.</p> <p>\ud83d\udce4 Salida (JSON) <code>{     \"sentiment\": \"Positive\" | \"Negative\" | \"Neutral\",     \"delivery_time\": &lt;int&gt; | null,     \"quality_rating\": \"Good\" | \"Fair\" | \"Poor\" | null,     \"extra_comment\": &lt;string&gt; | null   }</code></p> </li> </ul>"},{"location":"Unidad%201/modulo1/#structuredoutputparser-json-con-estructura-definida","title":"<code>StructuredOutputParser</code>: JSON con Estructura Definida","text":"<p>Este parser toma JSON y lo convierte en un diccionario de Python siguiendo un esquema espec\u00edfico que definimos con <code>ResponseSchema</code>. Solo extrae los campos que especificamos, garantizando una estructura clara y predecible. Veamos:</p> C\u00f3digoSalida <pre><code>from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n\n# Define the schema for the structured output\nschemas = [\n    ResponseSchema(name=\"sentiment\", description=\"Sentiment: Positive/Negative/Neutral\", type=\"string\"),\n    ResponseSchema(name=\"delivery_time\", description=\"Delivery time in days (or null if not mentioned)\", type=\"integer\"),\n    ResponseSchema(name=\"quality_rating\", description=\"Quality: Good/Fair/Poor (or null)\", type=\"string\"),\n    ResponseSchema(name=\"extra_comment\", description=\"Additional note (or null)\", type=\"string\")\n]\n\nstructured_parser = StructuredOutputParser.from_response_schemas(schemas)\n# Get the format instructions as a string\nformat_instructions = structured_parser.get_format_instructions()\n\n# Escape curly braces in format_instructions\nescaped_format_instructions = format_instructions.replace('{', '{{').replace('}', '}}')\n\n# Define the system message with escaped format instructions\nsystem_message = (\n    \"Analyze this review and return a JSON object. \"\n    \"Format: ```json\\n\"\n    f\"{escaped_format_instructions}\\n\"\n    \"```\"\n)\n\n# Create the prompt template with {review} as the only variable\nstructured_prompt = ChatPromptTemplate.from_messages([\n    (\"system\", system_message),\n    (\"human\", \"Review: {review}\")\n])\n\n# Create the chain \nstructured_chain = structured_prompt | llm | structured_parser\n\n# Run the chain with the review (assuming review_cliente is defined)\nstructured_response = structured_chain.invoke({\"review\": review_cliente})\nprint(\"StructuredOutputParser result:\", structured_response)\nprint(\"Type:\", type(structured_response))\n</code></pre> <pre><code>StructuredOutputParser result: {'sentiment': 'Positive', 'delivery_time': 2, 'quality_rating': 'Good', 'extra_comment': 'The headphones were a total success as a gift for my partner, thanks to their elegant presentation and impressive sound quality.'}\nType: &lt;class 'dict'&gt;\n</code></pre> <p>En este c\u00f3digo, el usuario final del mensaje estructurado es el modelo de lenguaje (LLM). Este mensaje est\u00e1 estructurado de tal manera que incluye instrucciones de ensamblaje para que el LLM procese el formato correctamente. La funci\u00f3n que nos permite especificar estas instrucciones es <code>get_format_instructions()</code>.</p> <p>La funci\u00f3n <code>get_format_instructions()</code> crea un string que contiene las instrucciones de formato basadas en los objetos <code>ResponseSchema</code>. Este string describe c\u00f3mo debe estructurarse la salida del modelo de lenguaje (LLM) para que sea f\u00e1cil de interpretar y procesar posteriormente.</p> <p>En analog\u00eda con el constructor de la clase <code>ChatPromptTemplate.from_template</code>, que describimos como un carpintero que crea un caj\u00f3n a partir de maderas brutas, este tipo de template con instrucciones de formato se asemejar\u00eda a construir un caj\u00f3n modular con instrucciones de armado, como el de la figura:</p> <p></p> <p>Analog\u00eda de un template con instrucciones de formato.</p> <p>Aqu\u00ed termina nuestro primer m\u00f3dulo. \u00a1Felicidades por llegar hasta el final! Ahora conoces el contexto general de las tecnolog\u00edas involucradas en el desarrollo de aplicaciones asistidas por IA. Este campo apenas est\u00e1 comenzando, y ahora tienes las bases para utilizar herramientas m\u00e1s sofisticadas, como las cadenas y la gesti\u00f3n de memoria, que ser\u00e1n el tema del siguiente m\u00f3dulo.</p>"},{"location":"Unidad%201/modulo1/#glosario","title":"Glosario","text":"<ol> <li> <p>API (Application Programming Interface):    Un conjunto de definiciones y protocolos que permite a las aplicaciones comunicarse entre s\u00ed. Las APIs facilitan la integraci\u00f3n y el intercambio de datos entre diferentes sistemas y servicios.</p> </li> <li> <p>Parser:    Un componente que analiza y transforma texto en un formato estructurado, facilitando su manipulaci\u00f3n y an\u00e1lisis en aplicaciones de software.</p> </li> <li> <p>JSON (JavaScript Object Notation):    Un formato de intercambio de datos ligero y f\u00e1cil de leer que utiliza una estructura basada en pares clave-valor. Com\u00fanmente usado para representar objetos en aplicaciones web.</p> </li> <li> <p>LangChain:    Un marco de trabajo que permite la creaci\u00f3n de aplicaciones asistidas por IA mediante la integraci\u00f3n de modelos de lenguaje con herramientas y flujos de trabajo personalizados.</p> </li> <li> <p>ChatPromptTemplate:    Una plantilla utilizada para crear mensajes estructurados que se env\u00edan a los modelos de lenguaje, permitiendo la personalizaci\u00f3n de las interacciones.</p> </li> <li> <p>Output Parser:    Un tipo de parser que se utiliza para definir c\u00f3mo debe estructurarse la salida de un modelo de lenguaje, asegurando que sea f\u00e1cil de interpretar y procesar.</p> </li> <li> <p>Ventana de contexto:    La cantidad de texto que un modelo de lenguaje puede procesar a la vez. Limita la cantidad de informaci\u00f3n que puede ser considerada en una sola interacci\u00f3n.</p> </li> <li> <p>LangChain Expression Language (LCEL):    Un lenguaje de expresi\u00f3n utilizado en LangChain para definir y manipular flujos de trabajo y cadenas de procesamiento de manera eficiente.</p> </li> <li> <p>Pipeline (Cadena de Procesamiento):    Un flujo de trabajo secuencial donde los datos pasan por diferentes etapas o componentes, cada uno realizando una tarea espec\u00edfica.</p> </li> <li> <p>Memoria en cadenas de conversaci\u00f3n:     La capacidad de un sistema de inteligencia artificial para almacenar y utilizar informaci\u00f3n de interacciones pasadas, mejorando la coherencia y personalizaci\u00f3n en futuras interacciones.</p> </li> </ol>"},{"location":"Unidad%201/modulo1/#evidencia-de-aprendizaje","title":"Evidencia de Aprendizaje","text":"M\u00f3dulo 1 Introducci\u00f3n a la construcci\u00f3n de aplicaciones con LLM EA1. Templates y Output Parsers <p>Ejercicio 1 - Correcci\u00f3n de respuestas inapropiadas en atenci\u00f3n al cliente:</p> <p>En este ejercicio, debes usar la IA para mejorar respuestas inapropiadas escritas por un operador de servicio al cliente. La IA corregir\u00e1 el tono, la cortes\u00eda y errores ortogr\u00e1ficos, asegurando que la respuesta sea adecuada para el cliente. Input: un mensaje del cliente y una respuesta inapropiada del operador. Output: una respuesta final corregida y apropiada para enviar al cliente. Requisitos: - Utiliza un prompt template para generar la respuesta apropiada. - Implementa un output parser para validar que la respuesta cumple con los criterios de cortes\u00eda y ortograf\u00eda. Bonus: investiga sobre memoria.  Si el cliente ha escrito varios mensajes, utiliza memoria para recordar el contexto de la conversaci\u00f3n.</p> <p>Ejercicio 2 (F\u00e1cil) - Extracci\u00f3n de informaci\u00f3n clave en rese\u00f1as de productos:</p> <p>Dado un review de un producto en un sitio de e-commerce, crea un modelo que extraiga informaci\u00f3n espec\u00edfica. Tareas: - Identifica si el producto fue comprado como regalo. - Extrae la opini\u00f3n del cliente sobre el precio. - Extrae comentarios sobre el tiempo de entrega.</p> <p>Requisitos: - Utiliza output parsers para extraer los campos relevantes en forma de estructuras de datos de Python como un diccionario. - Dise\u00f1a un prompt template que permita a la IA identificar y organizar estos elementos de manera eficiente.</p> <p>Guarda los documentos con la siguiente nomenclatura:</p> <ul> <li>Apellido_Nombre del estudiante.ipynb Ejemplo: </li> <li>L\u00f3pez_Karla.ipynb</li> </ul> <p>Finalmente, haz clic en el bot\u00f3n Cargar Tarea, sube tu archivo y presiona el bot\u00f3n Enviar para remitirlo a tu profesor con el fin de que lo eval\u00fae y retroalimente. |</p> <p>\ud83d\udcd6 Nota</p> <p>Conoce los criterios de evaluaci\u00f3n de esta evidencia de aprendizaje consultando la r\u00fabrica que encontrar\u00e1s a continuaci\u00f3n.</p> Criterios Ponderaci\u00f3n Totales 70 50 5 0 Calidad de las Soluciones Las soluciones a los ejercicios son correctas, demostrando una implementaci\u00f3n adecuada de los conceptos y t\u00e9cnicas requeridos. El estudiante muestra un dominio completo de los temas abordados. Aunque las soluciones no son completamente correctas, se observa un entendimiento y aplicaci\u00f3n adecuada de los conceptos y t\u00e9cnicas involucradas. Hay evidencia de esfuerzo y comprensi\u00f3n de los temas. Las soluciones presentadas son en su mayor\u00eda incorrectas. Se percibe un intento de resolver los ejercicios, pero hay una falta de comprensi\u00f3n de los conceptos y t\u00e9cnicas esenciales. No realiza la entrega 70 Calidad de la entrega El notebook es claro y f\u00e1cil de seguir, incluyendo comentarios detallados sobre el funcionamiento del c\u00f3digo en las celdas Markdown, lo que facilita la comprensi\u00f3n de las soluciones propuestas. El notebook no es particularmente f\u00e1cil de leer, pero a\u00fan as\u00ed incluye comentarios que explican el funcionamiento del c\u00f3digo en las celdas Markdown, mostrando un esfuerzo por aclarar la l\u00f3gica detr\u00e1s del c\u00f3digo. El notebook carece de comentarios acerca del funcionamiento del c\u00f3digo en las celdas Markdown, lo que dificulta la comprensi\u00f3n de las soluciones implementadas. No realiza la entrega 20 Tiempo de la entrega La entrega se realiza a tiempo, cumpliendo con el plazo establecido para la presentaci\u00f3n de la actividad. La entrega se realiza con una semana de atraso. Aunque fuera del plazo original, se considera adecuada para evaluar el trabajo presentado. La entrega se realiza con m\u00e1s de una semana de atraso, lo que indica un retraso significativo en la presentaci\u00f3n de la actividad. No realiza la entrega 10 Ponderaci\u00f3n de la actividad 100 puntos"},{"location":"Unidad%201/modulo1/#referencias","title":"Referencias","text":"<p>Chase, H., &amp; Ng, A. (2023). LangChain for LLM Application Development [Curso en l\u00ednea]. DeepLearning.AI. Disponible en https://www.deeplearning.ai/short-courses/langchain-for-llm-application-development/</p> <p>Huyen, C. (2025). AI Engineering: Building Applications with Foundation Models (1.\u00aa ed.). O'Reilly Media. Disponible en https://www.amazon.com/AI-Engineering-Building-Applications-Foundation/dp/1098166302</p> <p>LangChain. (s.f.). LangChain Documentation. Disponible en https://python.langchain.com/docs/introduction/</p>"},{"location":"Unidad%201/modulo1/#lecturas-y-material-complementario","title":"Lecturas y material complementario","text":"<p>Te invitamos a explorar el siguiente material para ampliar tus conocimientos sobre modelos de lenguaje (LLM), LangChain, plantillas de prompts y parsers de salida. Estos recursos te proporcionar\u00e1n una comprensi\u00f3n m\u00e1s profunda y pr\u00e1ctica de los temas abordados en el curso.</p>"},{"location":"Unidad%201/modulo1/#lecturas-recomendadas","title":"\ud83d\udcda Lecturas recomendadas","text":""},{"location":"Unidad%201/modulo1/#titulo-langchain-for-llm-application-development","title":"T\u00edtulo: LangChain for LLM Application Development","text":"<p>Autor: Harrison Chase &amp; Andrew Ng URL: LangChain for LLM Application Development Este curso gratuito de DeepLearning.AI ofrece una introducci\u00f3n pr\u00e1ctica al desarrollo de aplicaciones con modelos de lenguaje utilizando LangChain. Cubre temas como plantillas de prompts, parsers de salida y encadenamiento de componentes.</p>"},{"location":"Unidad%201/modulo1/#titulo-langchain-output-parser-guide","title":"T\u00edtulo: LangChain Output Parser Guide","text":"<p>Autor: Restack URL: LangChain Output Parser Guide Este art\u00edculo profundiza en el uso de los parsers de salida de LangChain, explicando c\u00f3mo transformar las respuestas de los modelos de lenguaje en formatos estructurados como JSON, y c\u00f3mo integrarlos en aplicaciones pr\u00e1cticas.</p>"},{"location":"Unidad%201/modulo1/#titulo-prompt-template-langchain-opentutorial","title":"T\u00edtulo: Prompt Template | LangChain OpenTutorial","text":"<p>Autor: Hye-yoon Jeong URL: Prompt Template | LangChain OpenTutorial Este tutorial cubre c\u00f3mo crear y utilizar plantillas de prompts en LangChain, esenciales para generar prompts din\u00e1micos y flexibles que se adapten a diversos casos de uso.</p>"},{"location":"Unidad%201/modulo1/#titulo-jsonoutputparser-langchain-opentutorial","title":"T\u00edtulo: JsonOutputParser | LangChain OpenTutorial","text":"<p>Autor: LangChain OpenTutorial URL: JsonOutputParser | LangChain OpenTutorial Este tutorial muestra c\u00f3mo utilizar el <code>JsonOutputParser</code> de LangChain para estructurar las salidas de los modelos de lenguaje en formato JSON, facilitando su integraci\u00f3n en aplicaciones que requieren datos estructurados.</p>"},{"location":"Unidad%201/modulo1/#videos-recomendados","title":"\ud83c\udfa5 Videos recomendados","text":""},{"location":"Unidad%201/modulo1/#titulo-transformers-how-llm-work-explained-visually-dl5","title":"T\u00edtulo: Transformers (how LLM work) explained visually | DL5","text":"<p>Autor: 3Blue1Brown URL: Transformers (how LLM work) explained visually Este video ofrece una explicaci\u00f3n visual de c\u00f3mo funcionan los modelos de lenguaje grandes (LLM) mediante la arquitectura de transformers, facilitando la comprensi\u00f3n de conceptos complejos.</p>"},{"location":"Unidad%201/modulo1/#titulo-attention-in-transformers-step-by-step-dl6","title":"T\u00edtulo: Attention in transformers, step-by-step | DL6","text":"<p>Autor: 3Blue1Brown URL: Attention in transformers, step-by-step Este video desglosa paso a paso el mecanismo de atenci\u00f3n en los transformers, una parte crucial en el funcionamiento de los LLM. Este video desglosa paso a paso el mecanismo de atenci\u00f3n en los transformers, una parte crucial en el funcionamiento de los LLM.</p>"},{"location":"Unidad%201/modulo1/#titulo-how-might-llm-store-facts-dl7","title":"T\u00edtulo: How might LLM store facts | DL7","text":"<p>Autor: 3Blue1Brown URL: How might LLM store facts Este video explora c\u00f3mo los modelos de lenguaje grandes pueden almacenar hechos y conocimientos, proporcionando una visi\u00f3n m\u00e1s profunda de su funcionamiento interno.</p>"},{"location":"Unidad2/modulo2/","title":"M\u00f3dulo 2. Cadenas y Memoria","text":""},{"location":"Unidad2/modulo2/#introduccion-al-modulo","title":"Introducci\u00f3n al m\u00f3dulo","text":"<p>Bienvenidos al segundo m\u00f3dulo de nuestro diplomado sobre construcci\u00f3n de aplicaciones asistidas por LLM. En el primer m\u00f3dulo aprendiste a confeccionar instrucciones reutilizables para LLM, los prompt templates, y exploraste c\u00f3mo acoplar estas instrucciones en cadenas con especificadores de formato llamados output parsers. Sin embargo, estas cadenas de ejecuci\u00f3n eran cadenas de un solo turno de interacci\u00f3n entre la IA y el usuario humano. En este m\u00f3dulo aprender\u00e1s a darle memoria y contexto a tus cadenas de ejecuci\u00f3n. Profundizaremos a\u00fan m\u00e1s en el funcionamiento de las cadenas y como limitar su memoria en el contexto del LCEL. Finalmente, pondr\u00e1s a prueba tu conocimiento creando un chatbot que asiste las labores de un m\u00e9dico realizando tareas secuenciales y que tiene como contexto en su memoria los datos espec\u00edficos de un paciente.</p> <p>\u00a1Comencemos!</p>"},{"location":"Unidad2/modulo2/#resultados-de-aprendizaje","title":"Resultados de aprendizaje","text":"<p>Al finalizar esta m\u00f3dulo, estar\u00e1s en capacidad de:</p> <ul> <li>Usar cadenas dotadas de memoria usando el LECL.</li> <li>Limitar el tama\u00f1o del contexto cargado en la memoria de tus cadenas.</li> </ul>"},{"location":"Unidad2/modulo2/#cronograma-de-actividades-modulo-2","title":"Cronograma de actividades - M\u00f3dulo 2","text":"Actividad de aprendizaje Evidencia de aprendizaje Semana Ponderaci\u00f3n EA1:  Cadenas y memoria EA1: Cadenas y memoria Semana 4 y 5 25% Total 25 %"},{"location":"Unidad2/modulo2/#cadenas","title":"Cadenas","text":"<p>Hay varias maneras de instanciar cadenas de ejecuci\u00f3n en LangChain, algunas de las cuales fueron exploradas en el m\u00f3dulo 1. Recientemente, LangChain introdujo LangChain Expression Language (LCEL) como el est\u00e1ndar para construir cadenas. Revisemos m\u00e1s detalladamente de qu\u00e9 se trata:</p> <p>Para tener en cuenta</p> <p>Recuerda cargar tu llave en las variables de sistema si no lo has hecho: <pre><code>import os\nimport openai\nfrom dotenv import load_dotenv, find_dotenv\n\n# Cargar el archivo .env local\n_ = load_dotenv(find_dotenv()) \nopenai.api_key = os.environ['OPENAI_API_KEY']\n</code></pre></p>"},{"location":"Unidad2/modulo2/#langchain-expression-language-lcel","title":"LangChain Expression Language (LCEL)","text":"<p>LangChain Expression Language (LCEL) es una sintaxis para definir cadenas. Permite componer objetos ejecutables (runnables)\u2014objetos que pueden ser ejecutados o encadenados\u2014usando el operador pipe (|). Un runnable es cualquier componente que implementa la interfaz Runnable, lo que significa que puede procesar entradas y producir salidas. Ejemplos incluyen:</p> <ul> <li>Plantillas de prompt (ChatPromptTemplate), como lo hicimos en el m\u00f3dulo 1.</li> </ul> <p>El operador de tuber\u00eda (|) conecta estos componentes, pasando la salida de un Runnable como la entrada al siguiente. Por ejemplo:</p> <pre><code>chain = prompt | llm | output_parser\n</code></pre> <p>Veamos un ejemplo en detalle:</p> C\u00f3digoSalida <pre><code>from langchain_core.prompts import ChatPromptTemplate\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.output_parsers import StrOutputParser\n\n# Define the prompt template\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a world-class technical documentation writer.\"),\n    (\"user\", \"{input}\")\n])\n\n# Initialize the LLM\nllm = ChatOpenAI(model=\"gpt-4\", temperature=0.7)\n\n# Create the output parser\noutput_parser = StrOutputParser()\n\n# Compose the chain\nchain = prompt | llm | output_parser\n\n# Invoke the chain\nresponse = chain.invoke({\"input\": \"How can LangSmith help with testing?\"})\nprint(response)\n</code></pre> <pre><code>LangSmith provides a suite of tools designed to assist with software testing, particularly in the realm of language translation and localization. It can be invaluable for teams developing software that needs to function in multiple languages and regions.\n\n1. **Automated Testing:** LangSmith can automatically test your software\u2019s language functionality, ensuring translations are accurate, context-appropriate, and that all elements of the user interface are correctly localized.\n\n2. **Quality Assurance:** By checking translations against a comprehensive database, LangSmith aids in maintaining high translation quality, reducing the risk of miscommunication or confusion for users.\n\n3. **Regression Testing:** When updates or changes are made to the software, LangSmith can help ensure that these changes have not negatively affected the language functionality.\n\n4. **Cultural Accuracy:** Beyond simple language translation, LangSmith can also verify that cultural nuances and local customs are appropriately considered, which can greatly improve user experience.\n\n5. **Reporting and Analytics:** LangSmith provides detailed reports on testing outcomes, highlighting any issues or potential areas for improvement. This can provide valuable insights for your development team.\n\nBy integrating LangSmith into your testing process, you can more effectively ensure the quality and accuracy of your software's multilingual and multicultural features, improving overall user experience and satisfaction.\n</code></pre>"},{"location":"Unidad2/modulo2/#funciones-envueltas-con-runnablelambda","title":"Funciones (envueltas con RunnableLambda)","text":"<p>Podemos usar cadenas con funciones al comienzo de la l\u00ednea de ejecuci\u00f3n, un cierto tipo de funci\u00f3n muy vers\u00e1til son las funciones de la clase RunnableLambda. Estas est\u00e1n dise\u00f1adas para integrar funciones personalizadas de Python en cadenas de LangChain. Permiten a los desarrolladores envolver funciones de Python arbitrarias (o funciones lambda) en un objeto ejecutable, haci\u00e9ndolas compatibles con la sintaxis del LCEL. Es decir, convierte una funci\u00f3n de Python en un componente encadenable.</p> <p>La funci\u00f3n debe aceptar una entrada compatible con la salida del paso anterior y producir una salida compatible con el siguiente paso.</p> <p>Por ejemplo:</p> <p><pre><code>from langchain_core.runnables import RunnableLambda\n\n# Wrap a function\nrunnable_sumaUno = RunnableLambda(lambda x: x + 1)  # suma 1 a la entrada\nrunnable_cuadrado = RunnableLambda(lambda x: x**2)  # eleva al cuadrado la entrada\n\n# La cadena de ejecuci\u00f3n ser\u00eda\nchain = runnable_sumaUno | runnable_cuadrado  # encadena las funciones\nresult = chain.invoke(5)  # (5 + 1) ** 2 = 36\nprint(result)  # Output: 36\n</code></pre> Vemos otro ejemplo:</p> <pre><code>from langchain_core.runnables import RunnableLambda\n\n# Define funciones lambda individuales\nadd_prefix = RunnableLambda(lambda x: f\"Hello, {x}!\")\nto_upper = RunnableLambda(lambda x: x.upper())\n\n# Construir la cadena\nchain = add_prefix | to_upper\n\n# Invocar la cadena\nresult = chain.invoke(\"Alice\")\nprint(result)  # Salida: HELLO, ALICE!\n</code></pre> <p>Veamos c\u00f3mo construir una cadena que toma una consulta de usuario, la formatea en un prompt, la procesa con un LLM y extrae la primera palabra de la respuesta.</p> <pre><code>from langchain_core.runnables import RunnableLambda\nfrom langchain_core.prompts import PromptTemplate\nfrom langchain_openai import ChatOpenAI  # o usar otro LLM\n\n# Definir componentes\n\n# Crear un prompt template y llenarlo\nformat_prompt = RunnableLambda(\n    lambda x: PromptTemplate.from_template(\"Answer briefly: {query}\").format(query=x)\n)\n\n# Extraer la primera palabra de la respuesta\nextract_first_word = RunnableLambda(\n    lambda x: x.content.split()[0] if hasattr(x, 'content') else x.split()[0]\n)\n\nllm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n\n# Construir la cadena\nchain = format_prompt | llm | extract_first_word\n\n# Invocar la cadena\nresult = chain.invoke(\"\u00bfC\u00f3mo se llama el presidente de Colombia?\")\nprint(result)  # Salida: Iv\u00e1n (recuerda que gpt-3.5 fue entrenado en datos hasta octubre de 2023)\n</code></pre> <p>La cadena procesa la consulta de entrada paso a paso: formatea la consulta, la procesa con el LLM y extrae la primera palabra de la respuesta. Puedes reemplazar <code>ChatOpenAI</code> con otro modelo (por ejemplo, <code>HuggingFaceHub</code>) si es necesario. Language models (ChatOpenAI).</p>"},{"location":"Unidad2/modulo2/#memoria","title":"Memoria","text":"<p>La memoria es el mecanismo por el cual le damos al LLM contexto de nuestras interacciones previas.</p> <p>Para explorar el uso de la memoria, instanciaremos una cadena de conversaci\u00f3n a partir de la clase preconstruida <code>ConversationChain</code>. Importamos los m\u00f3dulos necesarios:</p> <pre><code>from langchain.chains.conversation.base import ConversationChain\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain_openai import ChatOpenAI\n</code></pre> <p>Usaremos GPT-4 y el modelo de chat de OpenAI:</p> <pre><code>chat_model = ChatOpenAI(model_name=\"gpt-4\", temperature=0, streaming=True)\n</code></pre> <p>Para instanciar una cadena de conversaci\u00f3n de la clase <code>ConversationChain</code>, necesitamos instanciar primero el objeto de clase <code>Memory</code> que nos servir\u00e1 para administrar la memoria de las interacciones en la conversaci\u00f3n. Lo hacemos de la siguiente manera:</p> <pre><code>memory = ConversationBufferMemory() # Es una instancia de la clase que contiene los controles de la memoria\n</code></pre> <p>As\u00ed, la cadena de conversaci\u00f3n la instanciaremos como:</p> <p><pre><code>chain = ConversationChain(llm=chat_model, memory=memory, verbose=True)\n</code></pre> Ahora est\u00e1 todo listo para que comencemos nuestra conversaci\u00f3n:</p> C\u00f3digoSalida <pre><code>chain.invoke(\"Hola, mi nombre es Juan, \u00bfc\u00f3mo est\u00e1s?\")\n</code></pre> <pre><code>&gt; Entering new ConversationChain chain...\nPrompt after formatting:\nThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n\nCurrent conversation:\n\nHuman: Hola, mi nombre es Juan, \u00bfc\u00f3mo est\u00e1s?\nAI:\n\n&gt; Finished chain.\n\n    {'input': 'Hola, mi nombre es Juan, \u00bfc\u00f3mo est\u00e1s?',\n'history': '',\n'response': '\u00a1Hola, Juan! Estoy muy bien, gracias por preguntar. Soy una inteligencia artificial, as\u00ed que no tengo emociones como los humanos, pero estoy aqu\u00ed para ayudarte y conversar contigo. \u00bfEn qu\u00e9 puedo asistirte hoy?'}\n</code></pre> <p>Si pregunto aluna cosa adicional, por ejeplo cuando es 2 + 2. No olvidar\u00e1 mi nombre:</p> C\u00f3digoSalida <pre><code>chain.invoke(\"cuanto es 2 + 2?\")\nchain.invoke(\"cual es mi nombre?\")\n</code></pre> <pre><code>&gt; Entering new ConversationChain chain...\nPrompt after formatting:\nThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n\nCurrent conversation:\nHuman: Hola, mi nombre es Juan, \u00bfc\u00f3mo est\u00e1s?\nAI: \u00a1Hola, Juan! Estoy muy bien, gracias por preguntar. Soy una inteligencia artificial, as\u00ed que no tengo emociones como los humanos, pero estoy aqu\u00ed para ayudarte y conversar contigo. \u00bfEn qu\u00e9 puedo asistirte hoy?\nHuman: cuanto es 2 + 2?\nAI: 2 + 2 es igual a 4. Es una de las operaciones matem\u00e1ticas m\u00e1s b\u00e1sicas y es un buen ejemplo de c\u00f3mo funcionan las sumas. Si tienes m\u00e1s preguntas de matem\u00e1ticas o cualquier otro tema, estar\u00e9 encantado de ayudarte.\nHuman: \u00bfCu\u00e1l es mi nombre?\nAI:\n\n{'input': '\u00bfCu\u00e1l es mi nombre?',\n'history': 'Human: Hola, mi nombre es Juan, \u00bfc\u00f3mo est\u00e1s?\\nAI: \u00a1Hola, Juan! Estoy muy bien, gracias por preguntar. Soy una inteligencia artificial, as\u00ed que no tengo emociones como los humanos, pero estoy aqu\u00ed para ayudarte y conversar contigo. \u00bfEn qu\u00e9 puedo asistirte hoy?\\nHuman: cuanto es 2 + 2?\\nAI: 2 + 2 es igual a 4. Es una de las operaciones matem\u00e1ticas m\u00e1s b\u00e1sicas y es un buen ejemplo de c\u00f3mo funcionan las sumas. Si tienes m\u00e1s preguntas de matem\u00e1ticas o cualquier otro tema, estar\u00e9 encantado de ayudarte.',\n'response': 'Tu nombre es Juan. Me lo dijiste al comienzo de nuestra conversaci\u00f3n. Si tienes m\u00e1s preguntas o necesitas ayuda con algo m\u00e1s, no dudes en dec\u00edrmelo.'}\n</code></pre> <p>La IA responde que Tu nombre es Juan. Me lo dijiste al comienzo de nuestra conversaci\u00f3n. Lo cual no era posible en el m\u00f3dulo 1 cuando invoc\u00e1bamos el modelo sin memoria.</p>"},{"location":"Unidad2/modulo2/#la-ventana-de-contexto","title":"La ventana de contexto","text":"<p>La memoria es un recurso costoso, pues los modelos tienen una capacidad limitada para guardar el contexto de las conversaciones. Esta caracter\u00edstica es llamada la ventana de contexto. A medida que los modelos se han vuelto m\u00e1s avanzados, las ventanas de contexto ofrecidas son cada vez m\u00e1s grandes. </p> <ul> <li>GPT-3: 4096 tokens</li> <li>GPT-3.5-turbo: 4096 tokens</li> <li>GPT-4: 8192 tokens (con una versi\u00f3n extendida de 32768 tokens)</li> </ul> <p>Sin embargo, debemos tener en cuenta que el modelo siempre tratar\u00e1 de relacionar sus respuestas con el contexto de la conversaci\u00f3n. Esto puede ser beneficioso, pero al mismo tiempo, puede introducir ruido a la conversaci\u00f3n, y el modelo puede generar respuestas menos precisas si est\u00e1 distra\u00eddo en un contexto muy amplio. Por este motivo, resulta conveniente tener la posibilidad de limitar la memoria que queremos dar a las aplicaciones asistidas por IA. Veremos ahora algunos tipos de memoria y sus usos.</p>"},{"location":"Unidad2/modulo2/#tipos-de-memoria-en-langchain","title":"Tipos de Memoria en LangChain","text":"<p>LangChain ofrece varios tipos de memoria que se pueden utilizar para gestionar el contexto en las aplicaciones de inteligencia artificial</p> <p>crearemos una pequena encapsulaci\u00f3n para ilistrar el uso de las dos clases principales:</p> <pre><code>class ChatBot:\n    def __init__(self, memory):\n        self.chat_model  =  ChatOpenAI(model_name= \"gpt-4o\", temperature= 0, streaming= True)\n        self.memory = memory\n        self.chain = ConversationChain(llm=self.chat_model, memory=self.memory, verbose=True)\n\n# metodo para responder\n    def pregunta(self, pregunta: str):        \n        response = self.chain.predict(input = pregunta)\n        print(response)\n        return 0\n</code></pre>"},{"location":"Unidad2/modulo2/#tipos-de-memoria-en-langchain_1","title":"Tipos de Memoria en LangChain","text":"<p>ConversationBufferMemory:     - Uso: Esta memoria almacena todo el historial de la conversaci\u00f3n sin ning\u00fan l\u00edmite. Es \u00fatil cuando se desea mantener un registro completo de todas las interacciones previas, como lo hicimos en el ejemplo anterior.</p> <p>Por ejemplo, un chat con memoria ilimitada se crear\u00eda de la siguiente manera:</p> <p><pre><code>from langchain.memory import ConversationBufferMemory\n\nmucha_memoria = ConversationBufferMemory()\ngenioBot = ChatBot(mucha_memoria)\n</code></pre> ConversationBufferWindowMemory:    - Uso: Similar a <code>ConversationBufferMemory</code>, pero con un par\u00e1metro llamado <code>window_size</code> que permite recordar solo un n\u00famero fijo de interacciones recientes.</p> <p>Creamos un bot muy inteligente pero con memoria limitada usando <code>ConversationBufferWindowMemory</code>:</p> C\u00f3digoSalida <pre><code>from langchain.memory import ConversationBufferWindowMemory\n\n# Memoria con ventana de 1\npoca_memoria = ConversationBufferWindowMemory(k=1)\n\n# Instanciamos un bot con poca memoria\nolvidoBot = ChatBot(poca_memoria)\nolvidoBot.pregunta(\"Hola, mi nombre es Juan, \u00bfc\u00f3mo est\u00e1s?\")\nolvidoBot.pregunta(\"\u00bfCu\u00e1nto es 2 + 5?\")\nolvidoBot.pregunta(\"\u00bfCu\u00e1l es mi nombre?\") # No sabe el nombre\n</code></pre> <pre><code>&gt; Entering new ChatBot session...\nHuman: Hola, mi nombre es Juan, \u00bfc\u00f3mo est\u00e1s?\nAI: \u00a1Hola, Juan! Estoy muy bien, gracias por preguntar. Soy una inteligencia artificial dise\u00f1ada para ayudarte con informaci\u00f3n y responder a tus preguntas. \u00bfEn qu\u00e9 puedo asistirte hoy?\n\nHuman: \u00bfCu\u00e1nto es 2 + 5?\nAI: 2 + 5 es igual a 7. Si tienes m\u00e1s preguntas de matem\u00e1ticas o cualquier otra cosa en mente, \u00a1estar\u00e9 encantado de ayudarte!\n\nHuman: \u00bfCu\u00e1l es mi nombre?\nAI: Lo siento, no tengo la capacidad de saber tu nombre a menos que me lo hayas dicho antes en esta conversaci\u00f3n. Si quieres, puedes dec\u00edrmelo ahora y lo recordar\u00e9 para el resto de nuestra charla.\n</code></pre> <p><code>olvidoBot</code> solo recordar\u00e1 una interacci\u00f3n a la vez debido a <code>window_size=1</code>. Cuando preguntamos \"\u00bfCu\u00e1l es mi nombre?\", ya no recuerda la interacci\u00f3n donde le dijimos nuestro nombre, por lo que no puede recordarlo.</p> <p><code>ConversationSummaryMemory</code> almacena un resumen de la conversaci\u00f3n, ideal para interacciones muy largas donde no es necesario retener todos los detalles. </p> <p>Para tener en cuenta</p> <p>Las gu\u00edas m\u00e1s recientes recomiendan utilizar la funci\u00f3n <code>trim_messages</code>, que proporciona una forma flexible de gestionar el historial de la conversaci\u00f3n. Las funcionalidades aqu\u00ed expuestas est\u00e1n siendo migradas a la plataforma de LangGraph, por lo que est\u00e1n fuera del alcance de este curso. Ver\u00e1s el mensaje <code>LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/</code>.</p>"},{"location":"Unidad2/modulo2/#entendiendo-la-funcion-trim_messages","title":"Entendiendo la Funci\u00f3n <code>trim_messages</code>","text":"<p>La funci\u00f3n <code>trim_messages</code> es una utilidad en LangChain dise\u00f1ada para reducir el tama\u00f1o de un historial de chat a un n\u00famero espec\u00edfico de tokens o mensajes, asegurando que el historial recortado siga siendo v\u00e1lido para los modelos de chat. Un historial de chat v\u00e1lido t\u00edpicamente:</p> <ul> <li>Comienza con:</li> <li>Un <code>HumanMessage</code>, o</li> <li> <p>Un <code>SystemMessage</code> seguido de un <code>HumanMessage</code>.</p> </li> <li> <p>Termina con:</p> </li> <li>Un <code>HumanMessage</code>, o</li> <li>Un <code>ToolMessage</code> (com\u00fan en conversaciones basadas en agentes).</li> </ul> <p>Al recortar mensajes m\u00e1s antiguos o menos relevantes, <code>trim_messages</code> ayuda a enfocar el modelo en el contexto reciente y pertinente, evitanto informaci\u00f3n que que pueda distraer al modelo (no simepre un contexto grande es mejor).</p> <p>La funci\u00f3n <code>trim_messages</code> opera tomando varios par\u00e1metros para controlar c\u00f3mo se realiza el recorte:</p> Par\u00e1metro Descripci\u00f3n <code>messages</code> La secuencia de mensajes (por ejemplo, <code>HumanMessage</code>, <code>AIMessage</code>, <code>SystemMessage</code>) a recortar. <code>max_tokens</code> El n\u00famero m\u00e1ximo de tokens que deben tener los mensajes recortados. <code>strategy</code> La estrategia de recorte: \"first\" (mantiene los primeros mensajes) o \"last\" (mantiene los m\u00e1s recientes, a menudo preferido para conversaciones). <code>token_counter</code> Una funci\u00f3n o LLM utilizado para contar tokens en los mensajes, como el m\u00e9todo de conteo de tokens incorporado de un LLM. <code>include_system</code> Un booleano (por defecto: <code>False</code>) que especifica si se debe mantener el <code>SystemMessage</code> al principio si est\u00e1 presente. <code>allow_partial</code> Un booleano (por defecto: <code>False</code>) que permite dividir un mensaje si solo parte de \u00e9l puede incluirse para cumplir con el l\u00edmite de tokens. <p>La funci\u00f3n devuelve una lista de mensajes recortados que se ajustan a los l\u00edmites especificados mientras mantienen la coherencia del contexto.</p> <p>Consideremos un ejemplo pr\u00e1ctico donde tenemos un chatbot que utiliza una cadena para procesar consultas de usuario. El chatbot ha estado funcionando durante varias interacciones, y el historial de chat se ha alargado. Necesitamos recortar el historial para ajustarlo a un l\u00edmite de 100 tokens, manteniendo los mensajes m\u00e1s recientes y el <code>SystemMessage</code>.</p> <p>Primero, definamos un historial de chat de ejemplo:</p> <pre><code>from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n\nchat_history = [\n    SystemMessage(content=\"You are a helpful assistant.\"),\n    HumanMessage(content=\"Hello, my name is Juan how are you?\"),\n    AIMessage(content=\"I'm doing well, thank you. How can I help you today?\"),\n    HumanMessage(content=\"Can you tell me about the weather today?\"),\n    AIMessage(content=\"Sure, let me check that for you. [checks weather] It's sunny with a high of 75 degrees.\"),\n    HumanMessage(content=\"What's the capital of France?\"),\n    AIMessage(content=\"The capital of France is Paris.\"),\n]\n</code></pre> <p>Ahora, usaremos <code>trim_messages</code> para recortar este historial a 100 tokens, manteniendo los mensajes m\u00e1s recientes e incluyendo el <code>SystemMessage</code>:</p> <pre><code>from langchain_core.messages.utils import trim_messages\nfrom langchain_openai import OpenAI\n\n# Inicializar un LLM para el conteo de tokens\nllm = OpenAI(model=\"gpt-3.5-turbo-1106\")\n\n# Recortar los mensajes\ntrimmed_history = trim_messages(\n    messages=chat_history,\n    max_tokens=100,\n    strategy=\"last\",\n    token_counter=llm.get_num_tokens_from_messages,\n    include_system=True, # para mantener el SystemMessage en la memoria\n)\n</code></pre> <p>Si el conteo total de tokens del historial original excede los 100, el historial recortado podr\u00eda verse as\u00ed:</p> <pre><code>[\n    SystemMessage(content=\"You are a helpful assistant.\"),\n    HumanMessage(content=\"What's the capital of France?\"),\n    AIMessage(content=\"The capital of France is Paris.\"),\n]\n</code></pre> <p>Esto mantiene la conversaci\u00f3n enfocada y dentro de la ventana de contexto especificada, pero no recordar\u00e1 nuestro nombre. A continuaci\u00f3n, se presenta el c\u00f3digo completo:</p> C\u00f3digoSalida <pre><code>from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\nfrom langchain_core.messages.utils import trim_messages\nfrom langchain_core.runnables import RunnableLambda\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_openai import ChatOpenAI\n\n# Inicializar el LLM\n# Reemplazar 'your-openai-api-key' con tu clave API real de OpenAI\nllm = ChatOpenAI(model=\"gpt-3.5-turbo-1106\")\n\n# Definir la plantilla de prompt\n# La plantilla incluye un mensaje de sistema, un marcador de posici\u00f3n para el historial de chat y la entrada del usuario\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant.\"),\n    (\"placeholder\", \"{chat_history}\"),  # Importante: este es el marcador de posici\u00f3n para el historial de chat\n    (\"human\", \"{input}\")\n])\n\n# Definir una funci\u00f3n para recortar mensajes\n# Esta funci\u00f3n recorta el historial de chat para ajustarse a un l\u00edmite de tokens especificado\ndef trim_chat_history(messages):\n    return trim_messages(\n        messages=messages,\n        max_tokens=100,  # Limitar a 100 tokens para ajustarse al contexto del modelo\n        strategy=\"last\",  # Conservar los mensajes m\u00e1s recientes\n        token_counter=llm.get_num_tokens_from_messages,  # Usar el contador de tokens del LLM\n        include_system=True  # Preservar el mensaje del sistema\n    )\n</code></pre> <pre><code>LLM Response: You haven't provided your name. Can you please share it with me?\n</code></pre> <p>\ud83d\udcd6 Para aprender m\u00e1s</p> <p>El <code>(\"placeholder\", \"{chat_history}\")</code> en el <code>ChatPromptTemplate</code> es un componente clave del sistema de plantillas de prompt de LangChain, utilizado para definir un espacio en el prompt donde se insertar\u00e1 una secuencia de mensajes (por ejemplo, el historial de la conversaci\u00f3n).</p> Detalles <p>message type and its content or template.</p> <p>Tipos comunes de mensajes incluyen: - <code>(\"system\", \"...\")</code>: Un mensaje del sistema que define el rol o las instrucciones del asistente. - <code>(\"human\", \"...\")</code>: Un mensaje de entrada del usuario. - <code>(\"ai\", \"...\")</code>: Un mensaje de respuesta del asistente. - <code>(\"placeholder\", \"{variable_name}\")</code>: Un marcador de posici\u00f3n para una secuencia de mensajes o datos que se proporcionar\u00e1n m\u00e1s tarde.</p> <p>La tupla <code>(\"placeholder\", \"{chat_history}\")</code> indica que la variable llamada <code>chat_history</code> contendr\u00e1 una lista de mensajes (por ejemplo, <code>SystemMessage</code>, <code>HumanMessage</code>, <code>AIMessage</code>) que se insertar\u00e1n en esa posici\u00f3n en el prompt.</p> <p>Esta cadena procesa una lista \u00fanica de mensajes como contexto, pero no es propiamente una memoria de la conversaci\u00f3n, ya que es fija. Idealmente, queremos que la memoria sea persistente, es decir, que recuerde un cierto n\u00famero de interacciones. Para este prop\u00f3sito, podemos integrar <code>RunnableWithMessageHistory</code> a nuestra cadena para almacenar el historial de la conversaci\u00f3n.</p>"},{"location":"Unidad2/modulo2/#anadiendo-memoria-persistente-para-chatbots","title":"A\u00f1adiendo Memoria Persistente para Chatbots","text":"<p>En el ejemplo anterior, la cadena procesa una lista plana de mensajes sin mantener el estado entre invocaciones:</p> <pre><code>chain = (\n    RunnableLambda(trim_chat_history)  # Recorta los mensajes directamente\n    | prompt  # Formatea los mensajes recortados en el prompt\n    | llm  # Genera una respuesta usando el LLM\n)\n</code></pre> <p>Para a\u00f1adir memoria persistente, envolveremos esta cadena con <code>RunnableWithMessageHistory</code>. Al implementar <code>RunnableWithMessageHistory</code> en LCEL, necesitamos definir una pipeline de base, y esta a su vez se define a partir de una plantilla de prompt y un LLM. Comencemos definiendo un nuevo prompt template; en este caso, usaremos <code>from_message</code>, para lo cual importaremos los m\u00f3dulos:</p> <pre><code>from langchain.prompts import (\n    SystemMessagePromptTemplate, \n    HumanMessagePromptTemplate,\n    MessagesPlaceholder,\n    ChatPromptTemplate\n)\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.messages import AIMessage, HumanMessage\nfrom langchain_core.chat_history import InMemoryChatMessageHistory\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\n</code></pre> <p>Dividiremos el proceso en los siguientes pasos:</p> <ol> <li> <p>Crear la pipeline base. Como lo hemos hecho antes, definimos el prompt y un LLM. Por ejemplo:</p> <pre><code># Definir el prompt del sistema al estilo de Don Quijote\nsystem_prompt = \"Eres un asistente \u00fatil que responde en una sola oraci\u00f3n en espa\u00f1ol, al estilo de Don Quijote de la Mancha.\"\n\n# Crear la plantilla de prompt para el chat\nprompt_template = ChatPromptTemplate.from_messages([\n    SystemMessagePromptTemplate.from_template(system_prompt),\n    MessagesPlaceholder(variable_name=\"history\"),\n    HumanMessagePromptTemplate.from_template(\"{query}\"),\n])\n\n# Inicializar el LLM\nllm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n\n# Crear la pipeline base\npipeline = prompt_template | llm\n</code></pre> <p>Esta es nuestra pipeline base sobre la cual a\u00f1adiremos memoria usando <code>RunnableWithMessageHistory</code> y de esta forma retener el historial de conversaciones. Necesitaremos configurar un almac\u00e9n de historial de chat, puede ser un simple diccionario:</p> </li> <li> <p>Configurar la Gesti\u00f3n del Historial de Chat. Para a\u00f1adir memoria, necesitamos un mecanismo para almacenar y recuperar el historial de conversaciones para diferentes sesiones de usuario. Nuestro <code>RunnableWithMessageHistory</code> requiere que nuestra pipeline est\u00e9 envuelta en un objeto <code>RunnableWithMessageHistory</code>. Este objeto necesita algunos par\u00e1metros de entrada. Uno de ellos es <code>get_session_history</code>, que requiere una funci\u00f3n que devuelva un objeto <code>ChatMessageHistory</code> basado en un ID de sesi\u00f3n. Definimos esta funci\u00f3n nosotros mismos:</p> <pre><code># Definir el historial de chat\nchat_history = [\n    HumanMessage(content=\"Saludos, mi nombre es Juan, \u00bfc\u00f3mo te hallas?\"),\n    AIMessage(content=\"En verdad, me hallo en buen estado, y estoy presto a servirte, \u00bfcu\u00e1l es tu deseo?\"),\n    HumanMessage(content=\"\u00bfPuedes hablarme del tiempo que hoy nos acompa\u00f1a?\"),\n    AIMessage(content=\"Ciertamente, el d\u00eda se muestra soleado con un calor apacible que alcanza los 24 grados.\"),\n    HumanMessage(content=\"\u00bfCu\u00e1l es la capital de Francia?\"),\n    AIMessage(content=\"La capital de Francia es Par\u00eds, noble villa de gran renombre.\"),\n]\n\n# Configurar la gesti\u00f3n del historial de chat con un historial predefinido\nchat_dict = {}\nsession_id = \"id_123\"\nchat_dict[session_id] = InMemoryChatMessageHistory()\nchat_dict[session_id].add_messages(chat_history)\n\ndef get_chat_history(session_id: str) -&gt; InMemoryChatMessageHistory:\n    if session_id not in chat_dict:\n        chat_dict[session_id] = InMemoryChatMessageHistory()\n    return chat_dict[session_id]\n</code></pre> <p><code>chat_dict</code> es un diccionario que asigna IDs de sesi\u00f3n a objetos <code>InMemoryChatMessageHistory</code>, los cuales almacenan secuencias de mensajes (por ejemplo, mensajes del usuario y respuestas del asistente).</p> <p><code>get_chat_history</code> es una funci\u00f3n que: - Toma un <code>session_id</code> (por ejemplo, \"id_123\") como entrada. - Verifica si existe un historial de chat para ese <code>session_id</code> en <code>chat_dict</code>. - Si no, crea un nuevo <code>InMemoryChatMessageHistory</code> y lo almacena en <code>chat_dict</code>. - Luego, devuelve el objeto de historial de chat correspondiente.</p> <p>Para aprender m\u00e1s</p> <p>Esta configuraci\u00f3n asegura que cada sesi\u00f3n de usuario tenga su propio historial de chat aislado, permitiendo que m\u00faltiples usuarios interact\u00faen simult\u00e1neamente sin mezclar conversaciones. Sin embargo, para entornos de producci\u00f3n, el almacenamiento en memoria no es ideal debido a su volatilidad (los datos se pierden al reiniciar la aplicaci\u00f3n). Las alternativas incluyen: - <code>RedisChatMessageHistory</code> para almacenamiento escalable y persistente con Redis. - <code>PostgresChatMessageHistory</code> para almacenamiento respaldado por bases de datos. - <code>FileChatMessageHistory</code> para persistencia basada en archivos.</p> <p>Por ejemplo, para usar Redis, debes usar el paquete Redis (<code>%pip install --upgrade --quiet redis</code>), iniciar un servidor Redis (por ejemplo, a trav\u00e9s de Docker) y definir un <code>get_message_history</code> con <code>RedisChatMessageHistory</code>, como se muestra en la documentaci\u00f3n de LangChain sobre Integraciones de Memoria.</p> </li> <li> <p>Envolver la Pipeline Base con <code>RunnableWithMessageHistory</code>. Para a\u00f1adir memoria, envolvemos la pipeline base con <code>RunnableWithMessageHistory</code>, que gestiona el historial de chat para el <code>Runnable</code>:</p> <pre><code># Envolver la pipeline con RunnableWithMessageHistory\npipeline_with_history = RunnableWithMessageHistory(\n    pipeline,\n    get_session_history=get_chat_history,\n    input_messages_key=\"query\",\n    history_messages_key=\"history\"\n)\n</code></pre> <p>Aqu\u00ed, <code>pipeline</code> es el <code>Runnable</code> base que vamos a envolver (en este caso, <code>prompt_template | llm</code>). <code>get_session_history</code> es la funci\u00f3n (<code>get_chat_history</code>) que devuelve el historial de chat para un ID de sesi\u00f3n dado. Esta funci\u00f3n debe tomar un <code>session_id</code> y devolver una instancia de <code>BaseChatMessageHistory</code>. <code>input_messages_key</code> especifica la clave en el diccionario de entrada que contiene el mensaje actual del usuario. En el c\u00f3digo, es \"query\", lo que significa que la entrada espera algo como <code>{\"query\": \"What is my name again?\"}</code>. <code>history_messages_key</code> especifica la clave donde el historial de la conversaci\u00f3n debe ser inyectado en la entrada. En el c\u00f3digo, es \"history\", lo que significa que el historial (una lista de objetos <code>BaseMessage</code>) se a\u00f1ade bajo esta variable de entrada.</p> <p>Al invocar, <code>RunnableWithMessageHistory</code> recupera el historial de chat para el ID de sesi\u00f3n usando <code>get_chat_history</code>. Aumenta el diccionario de entrada a\u00f1adiendo el historial bajo la clave <code>history_messages_key</code> (por ejemplo, <code>{\"query\": \"What is my name again?\", \"history\": [...]}</code>). La entrada aumentada se pasa a la pipeline base, que incluye el historial en el prompt a trav\u00e9s de <code>MessagesPlaceholder</code>. Despu\u00e9s de que la pipeline genera una respuesta, el historial de chat se actualiza con el nuevo mensaje del usuario y la respuesta del asistente.</p> </li> </ol> <p>Y listo, nuestro historial de chat ahora se memorizar\u00e1 y recuperar\u00e1 cada vez que invoquemos nuestro runnable con el mismo ID de sesi\u00f3n.</p> C\u00f3digoSalida <pre><code># Invocar la pipeline para demostrar la memoria\nresult1 = pipeline_with_history.invoke(\n    {\"query\": \"\u00bfCu\u00e1l es mi nombre?\"},\n    config={\"session_id\": \"id_123\"}\n)\nprint(result1.content)  # Esperado: \"Vuestro nombre, seg\u00fan me hab\u00e9is dicho, es Juan.\"\n\nresult2 = pipeline_with_history.invoke(\n    {\"query\": \"\u00bfQu\u00e9 m\u00e1s sabes de Francia?\"},\n    config={\"session_id\": \"id_123\"}\n)\nprint(result2.content)\n</code></pre> <pre><code>Vuestro nombre es Juan, valeroso caballero que busca conocimiento y respuestas.\nFrancia es tierra de exquisita gastronom\u00eda, arte refinado y hermosos paisajes, dignos de ser explorados y admirados.\n</code></pre> <p>Nuestro chat ahora tiene la capacidad de recordar todas las interacciones. Vemos:</p> <pre><code>result2 = pipeline_with_history.invoke(\n    {\"query\": \"El nombre de mi madre es Maria\"},\n    config={\"session_id\": \"id_123\"}\n)\nprint(result2.content) \n\nresult2 = pipeline_with_history.invoke(\n    {\"query\": \"\u00bfQui\u00e9n es Maria en mi vida?\"},\n    config={\"session_id\": \"id_123\"}\n)\n\nchat = get_chat_history('id_123')\n\nfor msg in chat.messages:\n    print(msg.content)\n</code></pre> Salida <pre><code>Saludos, mi nombre es Juan, \u00bfc\u00f3mo te hallas?\nEn verdad, me hallo en buen estado, y estoy presto a servirte, \u00bfcu\u00e1l es tu deseo?\n\u00bfPuedes hablarme del tiempo que hoy nos acompa\u00f1a?\nCiertamente, el d\u00eda se muestra soleado con un calor apacible que alcanza los 24 grados.\n\u00bfCu\u00e1l es la capital de Francia?\nLa capital de Francia es Par\u00eds, noble villa de gran renombre.\n\u00bfCu\u00e1l es mi nombre?\nVuestro nombre es Juan, valeroso caballero que busca conocimiento y respuestas.\n\u00bfQu\u00e9 m\u00e1s sabes de Francia?\nFrancia es tierra de exquisita gastronom\u00eda, arte refinado y hermosos paisajes, dignos de ser explorados y admirados.\nEl nombre de mi madre es Maria\nVuestra madre, Mar\u00eda, posee un nombre tan puro y bello como el de la Virgen Santa.\n\u00bfQui\u00e9n es Maria?\nMar\u00eda es un nombre com\u00fan entre las mujeres, pero tambi\u00e9n es el nombre de la madre de Jes\u00fas, la Virgen Mar\u00eda, figura importante en la religi\u00f3n cat\u00f3lica.\n\u00bfQui\u00e9n es Maria en mi vida?\nMar\u00eda, en vuestra vida, es la mujer que os dio la vida, os cuid\u00f3 con amor y os gui\u00f3 en vuestro camino, como una luz en la oscuridad.\n</code></pre> <p>\ud83d\udcd6 Para aprender m\u00e1s</p> <p>Hasta aqu\u00ed hemos reproducido el comportamiento de la clase <code>ConversationBufferMemory</code> que describimos al principio. Sin embargo, esta clase ser\u00e1 deprecada en las versiones futuras de LangChain. Para ver c\u00f3mo crear wrappers de los dem\u00e1s tipos de memoria, puedes consultar el siguiente material: Art\u00edculo: Introducci\u00f3n a los Tipos de Memoria en LangChain URL: https://www.aurelio.ai/learn/langchain-memory-types</p> <p>\u00a1Felicidades por llegar al final del m\u00f3dulo 2! Has aprendido c\u00f3mo crear cadenas con memoria utilizando el LCEL. Ahora est\u00e1s en capacidad de crear chatbots funcionales. Te invito a realizar la actividad de aprendizaje para que pongas en pr\u00e1ctica lo aprendido.</p> <p>Glosario</p> <ul> <li> <p>Configurable runnables: En LangChain, son funciones ejecutables que pueden personalizarse din\u00e1micamente en tiempo de ejecuci\u00f3n utilizando un objeto <code>RunnableConfig</code>. Esto permite pasar par\u00e1metros como el nombre de la ejecuci\u00f3n, etiquetas o metadatos para controlar el comportamiento, como los l\u00edmites de concurrencia o recursi\u00f3n.</p> </li> <li> <p>Context window: La cantidad m\u00e1xima de tokens de entrada (texto, datos, etc.) que un modelo de chat puede procesar en una sola interacci\u00f3n, determinada por la arquitectura del modelo.</p> </li> <li> <p>langchain: Un paquete de Python que proporciona componentes de alto nivel para construir aplicaciones con modelos de lenguaje, como cadenas preconstruidas y herramientas para tareas comunes.</p> </li> <li> <p>langchain-community: Una colecci\u00f3n de componentes e integraciones contribuidas por la comunidad para LangChain, que extiende su funcionalidad con herramientas de terceros.</p> </li> <li> <p>langchain-core: El paquete fundamental de LangChain, que contiene interfaces centrales, abstracciones base e implementaciones en memoria para construir cadenas y funciones ejecutables.</p> </li> <li> <p>langgraph: Una extensi\u00f3n de LangChain para orquestar flujos de trabajo y pipelines complejos, permitiendo una gesti\u00f3n avanzada de estado y procesos de m\u00faltiples pasos.</p> </li> <li> <p>langserve: Una herramienta para desplegar funciones ejecutables de LangChain como endpoints de API REST utilizando FastAPI. Principalmente soporta funciones ejecutables de LangChain, con compatibilidad limitada para LangGraph.</p> </li> <li> <p>Managing chat history: M\u00e9todos y t\u00e9cnicas para almacenar, recuperar y mantener el contexto conversacional a trav\u00e9s de m\u00faltiples interacciones en una aplicaci\u00f3n basada en chat.</p> </li> <li> <p>RunnableConfig: Un objeto de configuraci\u00f3n en LangChain para pasar par\u00e1metros de tiempo de ejecuci\u00f3n a funciones ejecutables, incluyendo <code>run_name</code>, <code>run_id</code>, etiquetas, metadatos, <code>max_concurrency</code>, <code>recursion_limit</code> y otras configuraciones personalizables.</p> </li> </ul>"},{"location":"Unidad2/modulo2/#evidencia-de-aprendizaje","title":"Evidencia de Aprendizaje","text":"M\u00f3dulo 2 Cadenas y Memoria EA1. **Generaci\u00f3n de Informes de Salud Utilizando Archivos CSV ** <p>En este proyecto practicar\u00e1s el uso de cadenas para desrrollar un sistema que reliza tareas secuenciales y ramificadas, cargar\u00e1s los resitado de estas operacones en el bufer de memoria de un chatbot.</p>"},{"location":"Unidad2/modulo2/#instrucciones","title":"Instrucciones","text":"<p>Descarga el archivo healthcare_report.csv proporcionado y, usando LECL, desarrolla un sistema capaz de: 1. Procesar un informe de salud original en espa\u00f1ol: lee el informe desde el archivo CSV. 2. Traducir el informe al ingl\u00e9s: usa una cadena para traducir el texto. 3. Resumir el informe traducido: genera un resumen breve en ingl\u00e9s. 4. Extraer indicadores clave de salud del resumen: identifica elementos clave (e.g., s\u00edntomas, duraci\u00f3n). 5. Generar un plan de tratamiento basado en los indicadores clave: prop\u00f3n pasos de tratamiento. 6. Detectar el idioma original del informe: determina si el informe original est\u00e1 en espa\u00f1ol. 7. Generar una recomendaci\u00f3n de seguimiento en el idioma detectado: devuelve una recomendaci\u00f3n en espa\u00f1ol.</p> <p>Finalmete carga el infome m\u00e9dico del paciente en la memoria y crea un chat bot que est\u00e9 en capacidad de responder preguntas sobre el tratamiento indicado. Demuestra su uso con algunas llamas al chat</p> <p>Guarda los documentos con la siguiente nomenclatura:</p> <ul> <li>Apellido_Nombre del estudiante.ipynb Ejemplo: </li> <li>L\u00f3pez_Karla.ipynb</li> </ul> <p>Finalmente, haz clic en el bot\u00f3n Cargar Tarea, sube tu archivo y presiona el bot\u00f3n Enviar para remitirlo a tu profesor con el fin de que lo eval\u00fae y retroalimente. |</p> <p>\ud83d\udcd6 Nota</p> <p>Conoce los criterios de evaluaci\u00f3n de esta evidencia de aprendizaje consultando la r\u00fabrica que encontrar\u00e1s a continuaci\u00f3n.</p> Criterios Ponderaci\u00f3n Totales 70 50 5 0 Calidad de las Soluciones Las soluciones a los ejercicios son correctas, demostrando una implementaci\u00f3n adecuada de los conceptos y t\u00e9cnicas requeridos. El estudiante muestra un dominio completo de los temas abordados. Aunque las soluciones no son completamente correctas, se observa un entendimiento y aplicaci\u00f3n adecuada de los conceptos y t\u00e9cnicas involucradas. Hay evidencia de esfuerzo y comprensi\u00f3n de los temas. Las soluciones presentadas son en su mayor\u00eda incorrectas. Se percibe un intento de resolver los ejercicios, pero hay una falta de comprensi\u00f3n de los conceptos y t\u00e9cnicas esenciales. No realiza la entrega 70 Calidad de la entrega El notebook es claro y f\u00e1cil de seguir, incluyendo comentarios detallados sobre el funcionamiento del c\u00f3digo en las celdas Markdown, lo que facilita la comprensi\u00f3n de las soluciones propuestas. El notebook no es particularmente f\u00e1cil de leer, pero a\u00fan as\u00ed incluye comentarios que explican el funcionamiento del c\u00f3digo en las celdas Markdown, mostrando un esfuerzo por aclarar la l\u00f3gica detr\u00e1s del c\u00f3digo. El notebook carece de comentarios acerca del funcionamiento del c\u00f3digo en las celdas Markdown, lo que dificulta la comprensi\u00f3n de las soluciones implementadas. No realiza la entrega 20 Tiempo de la entrega La entrega se realiza a tiempo, cumpliendo con el plazo establecido para la presentaci\u00f3n de la actividad. La entrega se realiza con una semana de atraso. Aunque fuera del plazo original, se considera adecuada para evaluar el trabajo presentado. La entrega se realiza con m\u00e1s de una semana de atraso, lo que indica un retraso significativo en la presentaci\u00f3n de la actividad. No realiza la entrega 10 Ponderaci\u00f3n de la actividad 100 puntos"},{"location":"Unidad2/modulo2/#referencias","title":"Referencias","text":"<p>Aurelio AI. (s.f.). LangChain Course. Recuperado el 21 de mayo de 2025, de https://www.aurelio.ai/course/langchain</p> <p>Chase, H., &amp; Ng, A. (2023). LangChain for LLM Application Development [Curso en l\u00ednea]. DeepLearning.AI. Disponible en https://www.deeplearning.ai/short-courses/langchain-for-llm-application-development/</p>"},{"location":"Unidad2/modulo2/#lecturas-y-material-complementario","title":"Lecturas y material complementario","text":""},{"location":"Unidad2/modulo2/#lecturas-recomendadas","title":"\ud83d\udcda Lecturas recomendadas","text":""},{"location":"Unidad2/modulo2/#titulo-how-trellix-uses-langchain-to-enhance-cybersecurity","title":"T\u00edtulo: How Trellix Uses LangChain to Enhance Cybersecurity","text":"<p>Autor: [LangChain] Fecha de recuperaci\u00f3n: 21 de mayo de 2025 URL: How Trellix Uses LangChain to Enhance Cybersecurity</p>"},{"location":"Unidad2/modulo2/#videos-recomendados","title":"\ud83c\udfa5 Videos recomendados","text":""},{"location":"Unidad2/modulo2/#titulo-langchain-prompts-parsers-and-chaining-for-beginners","title":"T\u00edtulo: LangChain: Prompts, Parsers and Chaining | for Beginners","text":"<p>Autor: [Anub Gupta on Learn4Tarakki] URL: LangChain: Prompts, Parsers and Chaining | for Beginners Este video ofrece una introducci\u00f3n amigable para principiantes sobre c\u00f3mo crear plantillas de prompts, utilizar parsers y encadenar componentes en LangChain.</p>"},{"location":"Unidad2/modulo2/#titulo-interrupt-2025-keynote-harrison-chase-langchain","title":"T\u00edtulo: Interrupt 2025 Keynote | Harrison Chase | LangChain","text":"<p>Autor: [LangChain] URL: Interrupt 2025 Keynote | Harrison Chase | LangChain</p> <p>Este video presenta la keynote de Harrison Chase en la conferencia Interrupt 2025 de LangChain, donde se discute la evoluci\u00f3n de la ingenier\u00eda de agentes y la visi\u00f3n de la compa\u00f1\u00eda para agentes inteligentes. Incluye reflexiones sobre la trayectoria de LangChain y anuncios de nuevas herramientas de desarrollo.</p>"},{"location":"Unidad3/modulo3/","title":"M\u00f3dulo 3: Proyecto Integrador: construcci\u00f3n y depliegue de un sisitema RAG","text":""},{"location":"Unidad3/modulo3/#introduccion-al-modulo","title":"Introducci\u00f3n al m\u00f3dulo","text":"<p>Bienvenidos al \u00faltima m\u00f3dulo de nuestro curso sobre aplicaciones asistidas por LLM. En este m\u00f3dulo aprenderemos sobre document loaders y desplegaremos una aplicaci\u00f3n RAG funcional para conversar con archivos en PDF. \u00a1Comencemos!</p>"},{"location":"Unidad3/modulo3/#cronograma-de-actividades-modulo-3","title":"Cronograma de actividades - M\u00f3dulo 3","text":"Actividad de aprendizaje Evidencia de aprendizaje Semana Ponderaci\u00f3n Reto Formativo 3: carga y divisi\u00f3n de documentos en PDF EA3: Chat con tus datos Semanas 6, 7 y 8 30% Total 30%"},{"location":"Unidad3/modulo3/#retival-aumented-generation-rag","title":"Retival Aumented generation (RAG)","text":"<p>El desempe\u00f1o de un modelo de lenguaje en un t\u00f3pico particular depende de cu\u00e1nto haya visto datos sobre este t\u00f3pico en su proceso de entrenamiento. Si un tema no aparece mucho en internet, el modelo tiene una capacidad limitada para responder preguntas sobre este, mientras que si este tema es muy bien difundido, el desempe\u00f1o del modelo en este tipo de temas ser\u00e1 mucho mejor. Por otro lado, existe informaci\u00f3n que el modelo nunca vio, por ejemplo, los contratos de compraventa de una empresa que se dedica a la venta de casas, los decretos y ordenanzas que los gobiernos emitieron despu\u00e9s de la \u00faltima actualizaci\u00f3n del modelo, etc.</p> <p>En una RAG, un LLM recupera documentos contextuales a partir de una base de datos externa como parte de su ejecuci\u00f3n. Esto es \u00fatil si queremos hacer preguntas sobre documentos espec\u00edficos. La figura muestra el esquema general de una aplicaci\u00f3n RAG.  Esquema de flujo de una aplicaci\u00f3n RAG. Fuente: LangChain Documentation.</p> <p>El sistema de Recuperaci\u00f3n (Retrieval) es el encargado de devolver los documentos relevantes para que el LMM (Modelo de Lenguaje de M\u00e1quina) elabore la respuesta a partir de una solicitud. Este sistema est\u00e1 compuesto por tres subsistemas:</p> <ul> <li>Cargador de Documentos (Document Loader): carga documentos desde diversas fuentes, como archivos locales, sitios web, bases de datos, etc.</li> <li>Divisor de Texto (Text Splitter): divide los documentos en fragmentos manejables para su procesamiento.</li> <li>Almac\u00e9n Vectorial (Vector Store): almacena representaciones vectoriales de los documentos para una recuperaci\u00f3n eficiente.</li> </ul> <p>La Figura ilustra c\u00f3mo estos subsistemas interact\u00faan en el pipeline de RAG:</p> <p> Fuente: Curso \"Chat with Your Data\" de DeepLearning.AI.</p>"},{"location":"Unidad3/modulo3/#cargadores-de-documentos","title":"Cargadores de Documentos","text":"<p>Comenzaremos experimentando con algunas de las herramientas de carga de documentos disponibles. Es importante resaltar que existe una gran variedad, como lo ilustra la imagen:</p> <p> Fuente: Curso \"Chat with Your Data\" de DeepLearning.AI.</p> <p>En esencia, un Cargador de Documentos en LangChain es un componente que obtiene datos de una fuente especificada y los transforma en un formato estandarizado que LangChain puede entender y con el que puede trabajar. Este formato estandarizado es t\u00edpicamente un objeto Documento.</p> <p>Una vez que un cargador de documentos ingiere datos, usualmente los transforma en uno o m\u00e1s objetos Documento. Piensa en esto como nuestro contenedor estandarizado para datos de texto. Un objeto Documento en LangChain consiste principalmente en dos atributos clave:</p> <ul> <li> <p>page_content: contiene el contenido de texto real del documento o un fragmento de \u00e9l.</p> </li> <li> <p>metadata (diccionario): es un diccionario que contiene informaci\u00f3n adicional sobre el contenido. \u00a1Esto es muy importante! Los metadatos pueden incluir informaci\u00f3n como la siguiente:</p> <ul> <li>La fuente del documento (por ejemplo, nombre del archivo, URL, ID de base de datos)</li> <li>Fecha de creaci\u00f3n, autor</li> <li>N\u00famero de p\u00e1gina (para PDFs)</li> <li>T\u00edtulos de secciones espec\u00edficas</li> <li>Cualquier otra informaci\u00f3n contextual que consideres relevante.</li> </ul> </li> </ul> <p>El uso efectivo de metadatos puede mejorar significativamente la capacidad de tu aplicaci\u00f3n para filtrar, buscar y entender el contexto de la informaci\u00f3n.</p>"},{"location":"Unidad3/modulo3/#categorias-y-ejemplos-de-cargadores-de-documentos","title":"Categor\u00edas y ejemplos de cargadores de documentos","text":"<p>LangChain ofrece un vasto ecosistema de cargadores de documentos, \u00a1actualmente m\u00e1s de 80, y la com sigue a\u00f1adiendo m\u00e1s! Exploremos algunos:</p>"},{"location":"Unidad3/modulo3/#cargadores-basados-en-archivos","title":"Cargadores Basados en Archivos:","text":"<p>Estos cargadores leen datos directamente de archivos. Vemos algunos:</p>"},{"location":"Unidad3/modulo3/#documentos-pdf","title":"Documentos PDF","text":""},{"location":"Unidad3/modulo3/#pypdfloader","title":"<code>PyPDFLoader</code>","text":"<p>Una de las herramientas diponibles para carga de documentos en PDF es  <code>PyPDFLoader</code>. Este cargador extrae texto y metadatos de archivos PDF basados en texto, es decir, no es apropiado si el PDF es un escaneo de una imagen. Supongamos que nuestro texto lo tenemos en la carpeta de contenidos <code>./content</code>. En este caso, cargaremos el archivo attention.pdf de la siguiente manera:</p> <p><pre><code>from langchain.document_loaders import PyPDFLoader\n\noutput_path = \"./content/\"\nfile_path = output_path + 'attention.pdf'\nloader = PyPDFLoader(file_path)  # Instancia del cargador.\npages = loader.load()\n</code></pre> La l\u00ednea <code>pages = loader.load()</code> ejecuta el m\u00e9todo <code>.load()</code>, que lee el PDF y devuelve una lista de objetos <code>Document</code>. Cada <code>Document</code> representa una p\u00e1gina del PDF, con <code>page_content</code> (el texto extra\u00eddo) y <code>metadata</code> (informaci\u00f3n como la fuente y el n\u00famero de p\u00e1gina).</p> <p><code>pages</code> ser\u00e1 una lista de objetos <code>Document</code>. Cada <code>Document</code> tendr\u00e1: - <code>page_content</code>: El texto extra\u00eddo de una p\u00e1gina del PDF. - <code>metadata</code>: Un diccionario con informaci\u00f3n como <code>{\"source\": \"./content/attention.pdf\", \"page\": n}</code> (donde <code>n</code> es el n\u00famero de p\u00e1gina, comenzando desde 0).</p> <p>Podemos inspeccionar el contenido cargado:</p> C\u00f3digoSalida <pre><code>pages\n</code></pre> <p><pre><code>[&lt;Document page_content=\"Texto de la p\u00e1gina 1\" metadata={\"source\": \"./content/attention.pdf\", \"page\": 0}&gt;,\n&lt;Document page_content=\"Texto de la p\u00e1gina 2\" metadata={\"source\": \"./content/attention.pdf\", \"page\": 1}&gt;,\n...]\n</code></pre> </p> <p>As\u00ed, por ejemplo, podremos acceder al contenido cargado de la primera p\u00e1gina haciendo:</p> C\u00f3digoSalida <pre><code>first_page = pages[0]\nprint(\"Contenido de la primera p\u00e1gina:\")\nprint(first_page.page_content)\n</code></pre> <pre><code>Contenido de la primera p\u00e1gina:\nProvided proper attribution is provided, Google hereby grants permission to\nreproduce the tables and figures in this paper solely for use in journalistic or\nscholarly works.\nAttention Is All You Need\nAshish Vaswani\u2217\nGoogle Brain\navaswani@google.com\nNoam Shazeer\u2217\nGoogle Brain\nnoam@google.com\nNiki Parmar\u2217\nGoogle Research\nnikip@google.com\nJakob Uszkoreit\u2217\nGoogle Research\nusz@google.com\nLlion Jones\u2217\nGoogle Research\nllion@google.com\nAidan N. Gomez\u2217 \u2020\nUniversity of Toronto\naidan@cs.toronto.edu\n\u0141ukasz Kaiser\u2217\nGoogle Brain\n...\n\u2020Work performed while at Google Brain.\n\u2021Work performed while at Google Research.\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023\n</code></pre> <p>\ud83d\udcd6 Para aprender m\u00e1s</p> <p>Puedes conocer m\u00e1s cargadores de documentos PDF consultando la documentaci\u00f3n de LangChain en: How to: load PDF files.</p>"},{"location":"Unidad3/modulo3/#unstructured","title":"Unstructured","text":"<p>El cargador de documentos Unstructured se utiliza para cargar archivos de muchos tipos. Actualmente, Unstructured admite la carga de archivos de texto, presentaciones de PowerPoint, HTML, PDFs, im\u00e1genes y m\u00e1s.</p> <p>El paquete Unstructured de Unstructured.IO extrae texto limpio de documentos fuente como PDFs y documentos de Word.</p> <p>Para tener en cuenta</p> <p>La API de Unstructured requiere claves de API para realizar solicitudes para opciones m\u00e1s avanzadas. Puedes solicitar una clave de API aqu\u00ed y comenzar a usarla.</p> <p>Para ilustrar su uso, usaremos el mismo archivo PDF del ejemplo anterior:</p> <pre><code>from unstructured.partition.pdf import partition_pdf\n\noutput_path = \".content/\"\nfile_path = output_path + 'attention.pdf'\n</code></pre> <p>En su forma m\u00e1s b\u00e1sica, sin ninguna configuraci\u00f3n adicional, podemos cargar el documento como:</p> <pre><code># Carga y procesa el pdf con la configuraci\u00f3n b\u00e1sica\nchunks = partition_pdf(filename=file_path)\n</code></pre> Salida <pre><code>[&lt;unstructured.documents.elements.Text at 0x702efe8e0a60&gt;,\n&lt;unstructured.documents.elements.NarrativeText at 0x702efe8e0ac0&gt;,\n...\n</code></pre> <p>Visualicemos los documentos extra\u00eddos:</p> <pre><code># Muestra los elementos extra\u00eddos\nfor chunk in chunks:\n    print(chunk)\n</code></pre> Salida <pre><code>3 2 0 2\ng u A 2\n] L C . s c [\n...\n</code></pre> <p>\ud83d\udcd6 Para aprender m\u00e1s</p> <p>Puedes explorar las diferentes funcionalidades de carga en el siguiente enlace: Documentaci\u00f3n de Unstructured</p> <p>Para un mayor control sobre la forma en que podemos extraer los diferentes tipos de datos del PDF, podemos configurar la funci\u00f3n <code>partition_pdf</code> de la siguiente manera:</p> <p><pre><code>chunks = partition_pdf(\n    filename=file_path,\n    infer_table_structure=True,            # Extraer tablas\n    strategy=\"hi_res\",                     # Necesario para inferir tablas\n\n    extract_image_block_types=[\"Image\"],   # Agregar 'Table' para extraer im\u00e1genes de tablas\n    # image_output_dir_path=output_path,   # Si es None, las im\u00e1genes y tablas se guardar\u00e1n en base64\n\n    extract_image_block_to_payload=True,   # Si es True, extraer\u00e1 base64 para uso en API\n\n    chunking_strategy=\"by_title\",          # O 'basic'\n    max_characters=10000,                  # Por defecto es 500\n    combine_text_under_n_chars=2000,       # Por defecto es 0\n    new_after_n_chars=6000,\n\n    # extract_images_in_pdf=True,          # Obsoleto\n)\n</code></pre> Aqu\u00ed, la funci\u00f3n <code>partition_pdf</code> procesa el archivo PDF extrayendo no solo texto, sino tambi\u00e9n im\u00e1genes y tablas. La funci\u00f3n divide el PDF en partes manejables, conocidas como \"chunks\", y permite ajustar la forma en que se extraen los diferentes tipos de contenido. En particular, se infiere la estructura de las tablas, se extraen bloques de im\u00e1genes, y se define c\u00f3mo dividir el contenido en chunks bas\u00e1ndose en t\u00edtulos y l\u00edmites de caracteres. Las im\u00e1genes extra\u00eddas se codifican en base64 para facilitar su transmisi\u00f3n o almacenamiento.</p> <p>\ud83d\udcd6 Para aprender m\u00e1s</p> <p>El formato base64 es un m\u00e9todo de codificaci\u00f3n que convierte datos binarios en texto ASCII, permitiendo que los datos sean f\u00e1cilmente transmitidos a trav\u00e9s de medios que solo soportan texto. Puedes aprender m\u00e1s sobre el formato base64 en la documentaci\u00f3n de Wikipedia.</p> <p>Verifiquemos el contenido extra\u00eddo:</p> <pre><code>chunks\n</code></pre> Salida <pre><code>[&lt;unstructured.documents.elements.CompositeElement at 0x702efeb07670&gt;,\n &lt;unstructured.documents.elements.CompositeElement at 0x702e358f2be0&gt;,\n &lt;unstructured.documents.elements.CompositeElement at 0x702efeb07be0&gt;,\n &lt;unstructured.documents.elements.CompositeElement at 0x702efeb07f40&gt;,\n ...\n</code></pre> <p>Con esta configuraci\u00f3n, obtenemos dos tipos de elementos:</p> <pre><code>set([str(type(el)) for el in chunks])\n</code></pre> Salida <pre><code>{\"&lt;class 'unstructured.documents.elements.CompositeElement'&gt;\",\n \"&lt;class 'unstructured.documents.elements.Table'&gt;\"}\n</code></pre> <p>Un <code>CompositeElement</code> generalmente contiene m\u00faltiples elementos secundarios de varios tipos, como texto, im\u00e1genes o tablas. Esto permite que la biblioteca gestione secciones de un documento compuestas por diferentes tipos de contenido como una sola entidad.</p> <p>En el procesamiento de documentos, es com\u00fan encontrar secciones que incluyen una mezcla de texto, im\u00e1genes y otros elementos que l\u00f3gicamente son parecidos.  Un <code>CompositeElement</code> puede representar dichas secciones. Cada objeto <code>CompositeElement</code> tiene un atributo <code>metadata</code>, que es una instancia de la clase <code>ElementMetadata</code>. Este atributo contiene informaci\u00f3n adicional sobre el chunk, como n\u00fameros de p\u00e1gina, detalles del archivo o datos estructurales. Por ejemplo:</p> <pre><code>chunks[3].metadata.orig_elements\n</code></pre> Salida <pre><code>[&lt;unstructured.documents.elements.Title at 0x702e35e7ed60&gt;,\n &lt;unstructured.documents.elements.NarrativeText at 0x702e35e7edf0&gt;,\n &lt;unstructured.documents.elements.Footer at 0x702e0cedd940&gt;,\n &lt;unstructured.documents.elements.Image at 0x702e0ceddd00&gt;,\n &lt;unstructured.documents.elements.Image at 0x702e0ceddac0&gt;,\n &lt;unstructured.documents.elements.NarrativeText at 0x702e0cedde50&gt;,\n &lt;unstructured.documents.elements.NarrativeText at 0x702e0d38a730&gt;,\n &lt;unstructured.documents.elements.Title at 0x702e0d38aca0&gt;,\n &lt;unstructured.documents.elements.NarrativeText at 0x702e0d38adc0&gt;,\n &lt;unstructured.documents.elements.NarrativeText at 0x702e0d16a7c0&gt;,\n &lt;unstructured.documents.elements.Formula at 0x702e0d16a0a0&gt;,\n &lt;unstructured.documents.elements.NarrativeText at 0x702e0d16a610&gt;,\n &lt;unstructured.documents.elements.NarrativeText at 0x702e0d433d30&gt;]\n</code></pre> <p>Aqu\u00ed, <code>orig_elements</code> es un campo espec\u00edfico dentro de la metadata que almacena una referencia a los elementos originales, no divididos, a partir de los cuales se cre\u00f3 el chunk. Generalmente, es una lista de objetos <code>Element</code> que fueron combinados o procesados para formar el chunk actual. En este chunk particular, tenemos los siguientes elementos:</p> <ul> <li>T\u00edtulo</li> <li>Texto narrativo</li> <li>Pie de p\u00e1gina</li> <li>Imagen</li> <li>F\u00f3rmula</li> </ul> <p>Separemos las im\u00e1genes en el chunk:</p> <pre><code>elements = chunks[3].metadata.orig_elements\n\nchunks_images = [el for el in elements if \"Image\" in str(type(el))]\nchunks_images\n</code></pre> Salida <pre><code>[&lt;unstructured.documents.elements.Image at 0x702e0ceddd00&gt;,\n &lt;unstructured.documents.elements.Image at 0x702e0ceddac0&gt;]\n</code></pre> <p>La siguiente funci\u00f3n extraer\u00e1 todas las im\u00e1genes del documento en una lista:</p> <p><pre><code># Extraer las im\u00e1genes de los CompositeElements\n\ndef get_images_base64(chunks):\n    images_b64 = []\n    for chunk in chunks:\n        if \"CompositeElement\" in str(type(chunk)):  # Filtramos los CompositeElements\n            chunks_els = chunk.metadata.orig_elements\n            for el in chunks_els:\n                if \"Image\" in str(type(el)):\n                    images_b64.append(el.metadata.image_base64)\n    return images_b64\n</code></pre> Y podemos verificar la extracci\u00f3n visualiz\u00e1ndola:</p> <pre><code>import base64\nfrom IPython.display import Image, display\n\ndef display_base64_image(base64_code):\n    image_data = base64.b64decode(base64_code)\n    display(Image(data=image_data))\n\ndisplay_base64_image(images[0])\n</code></pre> Salida <p></p>"},{"location":"Unidad3/modulo3/#web-based","title":"Web Based","text":"<p>Veamos ahora un ejemplo corto de carga de un documento web:</p> <pre><code>from langchain.document_loaders import WebBaseLoader\n\nloader = WebBaseLoader(\"https://concepto.de/reino-fungi/\")\ndocs = loader.load()\nprint(docs[0].page_content[:500])\n</code></pre> Salida <pre><code>Reino Fungi - Concepto, tipos, caracter\u00edsticas y ejemplos\n\n...\n\nArte\nConocimiento\nC\n</code></pre> <p>Este tipo de cargadores dejan como espacios en blanco los objetos que no son texto, como im\u00e1genes o tablas. Por lo tanto, requeriremos conectar otras herramientas que nos permitan hacer un pos-procesamiento adecuado. Una caracter\u00edstica de <code>WebBaseLoader</code> es que permite cargar varias URLs a la vez. Vamos a cargar contenido de dos p\u00e1ginas relacionadas:</p> <pre><code># Lista de URLs a cargar\nurls = [\n    \"https://es.wikipedia.org/wiki/Inteligencia_artificial\",\n    \"https://es.wikipedia.org/wiki/Aprendizaje_autom%C3%A1tico\"\n]\n\n# Cargamos m\u00faltiples p\u00e1ginas\nmulti_loader = WebBaseLoader(urls)\nmulti_documents = multi_loader.load()\n\n# Mostramos el n\u00famero de documentos cargados y un extracto de cada uno\nprint(f\"Se cargaron {len(multi_documents)} documentos.\")\nfor i, doc in enumerate(multi_documents):\n    print(f\"Documento {i + 1} (primeros 200 caracteres):\")\n    print(doc.page_content[:200])\n</code></pre> Salida <pre><code>Se cargaron 2 documentos.\nDocumento 1 (primeros 200 caracteres):\n\nInteligencia artificial - Wikipedia, la enciclopedia libre\n\n...\n\nEl algoritmo aprende observando el mundo que le rodea. Su informaci\u00f3n de entrada es la retroalimentaci\u00f3n que obtiene del mundo exterior como respuesta a sus acciones. Por lo tanto, el sistema aprende a base de ensayo-error.\n\nEl aprendizaje por refuerzo es el m\u00e1s general entre las tres categor\u00edas. En vez de que un instructor indique al agente qu\u00e9 hacer, el agente inteligente debe aprender c\u00f3mo se comporta el entorno mediante recompensas (refuerzos) o castigos, derivados del \u00e9xito o del fracaso respectivamente. El objetivo principal es aprender la funci\u00f3n de valor que le ayude al agente inteligente a maximizar la se\u00f1al de recompensa y as\u00ed optimizar sus pol\u00edticas de modo a comprender el comportamiento del entorno y a tomar buenas decisiones para el logro de sus objetivos formales.\n\nLos principales algoritmos de aprendizaje por refuerzo se desarrollan dentro de los m\u00e9todos de resoluci\u00f3n de problemas de decisi\u00f3n finitos de Markov, que incorporan las ecuaciones de Bellman y las funciones de valor. Los tres m\u00e9todos principales son: la programaci\u00f3n din\u00e1mica, los m\u00e9todos de Monte Carlo y el aprendizaje de diferencias temporales.\n\nEntre las implementaciones desarrolladas est\u00e1 AlphaGo, un programa de IA desarrollado por Google DeepMind para jugar el juego de mesa Go. En marzo de 2016 AlphaGo le gan\u00f3 una partida al jugador profesional Lee Se-Dol que tiene la categor\u00eda noveno dan y 18 t\u00edtulos mundiales. Entre los algoritmos que utiliza se encuentra el \u00e1rbol de b\u00fasqueda Monte Carlo, tambi\u00e9n utiliza aprendizaje profundo con redes neuronales. Puede ver lo ocurrido en el documental de Netflix \u201cAlp\n</code></pre>"},{"location":"Unidad3/modulo3/#youtube","title":"YouTube","text":"<p>La API <code>YouTubeTranscriptApi</code> es una herramienta \u00fatil para extraer transcripciones de videos de YouTube. Esta API permite obtener los subt\u00edtulos de un video en diferentes idiomas, facilitando el an\u00e1lisis de contenido de video de manera program\u00e1tica. A continuaci\u00f3n, ilustraremos su uso utilizando el video de YouTube sobre  retropropagaci\u00f3n .</p> <pre><code>from youtube_transcript_api import YouTubeTranscriptApi\n\n# Extraemos el ID del video desde la URL\nvideo_id = \"kbGu60QBx2o\"  # ID de https://www.youtube.com/watch?v=kbGu60QBx2o (Backpropagation: Data Science Concepts)\n\ntry:\n    transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=['es', 'en'])\n    print(\"Primeros 500 caracteres del transcrito manual:\")\n    print(\" \".join([entry['text'] for entry in transcript])[:500])\nexcept Exception as e:\n    print(f\"Error al obtener el transcrito manualmente: {str(e)}\")\n</code></pre> Salida <pre><code>[Music] hey everyone in this video we're going to do a followup to our initial video on neural networks and this video is going to be on back propagation now I'm making this video on back propagation mostly because it's a really difficult concept to understand at least it was for me I read through multiple blog posts and watch videos and everyone seems to have their own kind of way of understanding and explaining it and it was difficult for me to match up all those understandings into what this \n</code></pre>"},{"location":"Unidad3/modulo3/#text-splitters","title":"Text Splitters","text":"<p>Una vez hemos cargado el o los documentos sobre los cuales queremos realizar RAG , debemos separarlos en fragmentos sobre los cuales crearemos nuestra base de datos de embeddings, es decir, una base de datos vectorial (ver Figura 2). La herramienta que nos permite hacer esto son los llamados Text Splitters. LangChain ofrece varias implementaciones de Text Splitters para dividir texto bas\u00e1ndose en:</p> <ul> <li>Caracteres </li> <li>Tokens </li> <li>L\u00edneas </li> <li>Oraciones </li> </ul> <p>Por ejemplo, el CharacterTextSplitter divide el texto en funci\u00f3n de caracteres, mientras que el RecursiveCharacterTextSplitter intenta dividir por varios delimitadores (como <code>\\n</code> o espacios) hasta encontrar el tama\u00f1o adecuado.</p> <p>En general, el texto completo ser\u00e1 dividido seg\u00fan un par\u00e1metro que limita el tama\u00f1o del fragmento (<code>chunk_size</code>) y otro par\u00e1metro que le indica al text splitter el nivel de superposici\u00f3n de textos en dos fragmentos contiguos (<code>chunk_overlap</code>). El solapamiento es deseable para garantizar que las ideas que no est\u00e9n completas en un fragmento queden completas en el siguiente, como lo ilustra la figura:</p> <p></p> <p>Para usar el <code>RecursiveCharacterTextSplitter</code> y el <code>CharacterTextSplitter</code>, cargaremos el m\u00f3dulo:</p> <pre><code>from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n</code></pre>"},{"location":"Unidad3/modulo3/#recursivecharactertextsplitter","title":"RecursiveCharacterTextSplitter","text":"<p>Dividamos un texto usando un tama\u00f1o de fragmento de 26 caracteres y un solapamiento de 4 caracteres.</p> <p><pre><code>chunk_size = 26\nchunk_overlap = 4\n\n# Instanciamos los dos splitters\nr_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=chunk_size,\n    chunk_overlap=chunk_overlap\n)\n\nc_splitter = CharacterTextSplitter(\n    chunk_size=chunk_size,\n    chunk_overlap=chunk_overlap\n)\n</code></pre> Usemos el divisor en la cadena de texto:</p> <pre><code>text1 = 'abcdefghijklmnopqrstuvwxyz'\nr_splitter.split_text(text1)\n</code></pre> Salida <pre><code>['abcdefghijklmnopqrstuvwxyz']\n</code></pre> <p>Como el texto es demasiado corto, el splitter solo genera un fragmento. Un texto un poco m\u00e1s largo, como en:</p> <pre><code>text2 = 'abcdefghijklmnopqrstuvwxyznopqrstuvwxyz'\nr_splitter.split_text(text2)\n</code></pre> <p>Dividir\u00e1 el texto en dos fragmentos.</p> Salida <pre><code>['abcdefghijklmnopqrstuvwxyz', 'wxyznopqrstuvwxyz']\n</code></pre> <p>Ahora hag\u00e1moslo con el splitter de caracteres:</p> <pre><code>c_splitter.split_text(text3)\n</code></pre> <p>Lo que vemos es que no trata de dividirlo. \u00bfQu\u00e9 est\u00e1 pasando? Debemos escoger el car\u00e1cter separador. Por ejemplo, <code>separator = ' '</code>.</p> <pre><code>c_splitter = CharacterTextSplitter(\n    chunk_size=chunk_size,\n    chunk_overlap=chunk_overlap,\n    separator=' '\n)\nc_splitter.split_text(text3)\n</code></pre> Salida <pre><code>['a b c d e f g h i j k l m', 'l m n o p q r s t u v w x', 'w x y z']\n</code></pre> <p>Vemos otro ejemplo: <pre><code># Reemplaza la variable some_text por otro texto del mismo tama\u00f1o sobre lo que opinaba Einstein sobre termodin\u00e1mica\nsome_text = \"\"\"Albert Einstein consideraba que la termodin\u00e1mica era \nuna de las teor\u00edas m\u00e1s fundamentales y s\u00f3lidas de la f\u00edsica. En sus\n propias palabras, dec\u00eda que la termodin\u00e1mica era la \u00fanica teor\u00eda \n f\u00edsica que \u00e9l estaba convencido de que nunca ser\u00eda refutada, dentro \n del marco de aplicabilidad de sus conceptos b\u00e1sicos. Esto resaltaba \n su profundo respeto por la capacidad de la termodin\u00e1mica para describir \n fen\u00f3menos naturales con precisi\u00f3n. Einstein ve\u00eda en la termodin\u00e1mica una\n  belleza que se derivaba de su simplicidad y universalidad, y la consideraba\n   una piedra angular en el entendimiento cient\u00edfico del mundo f\u00edsico.\"\"\"\n\nlen(some_text)\n</code></pre></p> Salida <pre><code>625\n</code></pre> <pre><code>c_splitter = CharacterTextSplitter(\n    chunk_size=450,\n    chunk_overlap=0,\n    separator=' '\n)\nr_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=450,\n    chunk_overlap=0,\n    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n)\n</code></pre> <p>Cuando especificamos <code>separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]</code>, significa que el splitter usa estos separadores en ese orden hasta que encuentra un tipo de separador que le sirva.</p> <pre><code>c_splitter.split_text(some_text)\nfrom IPython.display import Markdown, display\nprint(c_splitter.split_text(some_text))\ndisplay(Markdown(c_splitter.split_text(some_text)[0]))\ndisplay(Markdown(c_splitter.split_text(some_text)[1]))\n</code></pre> Salida <p></p> <ul> <li> Reto formativo Planteamiento:   Utiliza PyPDFLoader para cargar el documento attention.pdf u otro de tu preferencia.</li> <li>Usa <code>CharacterTextSplitter</code> y experimenta con diferentes par\u00e1metros, por ejemplo:      <pre><code>separator=\"\\n\",\nchunk_size=1000,\nchunk_overlap=150,\nlength_function=len\n</code></pre></li> <li>Visualiza el n\u00famero de documentos cargados.</li> <li>\u00bfCu\u00e1ntas p\u00e1ginas tiene el documento?</li> </ul>"},{"location":"Unidad3/modulo3/#almacenamiento-en-la-base-de-datos-vectorial","title":"Almacenamiento en la base de datos vectorial","text":"<p>El paso siguiente en la construcci\u00f3n de nuestro RAG (ver figura 2) consiste en crear representaciones vectoriales de los fragmentos. Las representaciones vectoriales de texto, o embeddings, son creadas a partir de modelos de lenguaje (LLM). Un embedding se genera utilizando un LLM que convierte texto en vectores num\u00e9ricos, capturando el contexto y el significado sem\u00e1ntico del texto. El flujo es mostrado en la figura:</p> <p></p> <p>Para aprender m\u00e1s</p> <p>Las bases de datos vectoriales son sistemas especializados en el almacenamiento, indexaci\u00f3n y recuperaci\u00f3n eficiente de vectores de alta dimensi\u00f3n. Estos vectores, que com\u00fanmente representan caracter\u00edsticas num\u00e9ricas extra\u00eddas de datos como texto, im\u00e1genes, audio o video, permiten realizar b\u00fasquedas basadas en similitud (por ejemplo, utilizando distancia euclidiana, coseno o HNSW). A diferencia de las bases de datos tradicionales orientadas a registros, las bases de datos vectoriales est\u00e1n optimizadas para operaciones de b\u00fasqueda aproximada de vecinos m\u00e1s cercanos (Approximate Nearest Neighbor Search, ANNS), lo que las hace ideales para tareas de recuperaci\u00f3n sem\u00e1ntica, sistemas de recomendaci\u00f3n, reconocimiento de patrones y aplicaciones en inteligencia artificial.</p> <p>Estos sistemas son clave en entornos de aprendizaje autom\u00e1tico y procesamiento de lenguaje natural (PLN), donde se requiere comparar representaciones vectoriales de datos embebidos (embeddings). Algunas implementaciones populares incluyen FAISS (Facebook AI Similarity Search), Annoy (Spotify), Milvus y Weaviate, que ofrecen distintos enfoques y estructuras para manejar escalabilidad, latencia y precisi\u00f3n.</p> <p>Para profundizar en el tema, se recomienda la siguiente referencia t\u00e9cnica: Johnson, J., Douze, M., &amp; J\u00e9gou, H. (2019). Billion-scale similarity search with GPUs. arXiv preprint arXiv:1702.08734.</p>"},{"location":"Unidad3/modulo3/#embeddings","title":"Embeddings","text":"<p>Los embeddings son las representaciones vectoriales de los fragmentos de texto entregados por el <code>TextSplitter</code>. Textos similares tienen representaciones similares en el espacio de embeddings, lo que significa que podemos comparar estos vectores y encontrar fragmentos de texto que son similares o relevantes dada una consulta. Crearemos nuestros embeddings usando <code>OpenAIEmbeddings</code>.</p> <p>Veamos su funcionamiento con un ejemplo sencillo: <pre><code>from dotenv import load_dotenv\nimport os\nfrom langchain_openai import OpenAIEmbeddings\nimport numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Cargar variables de entorno (por ejemplo, OPENAI_API_KEY)\nload_dotenv()\n\n# Definir oraciones menos creativas con temas de comida/cocina y clima\nsentence1 = \"I like pasta.\"\nsentence2 = \"I like noodles.\"\nsentence3 = \"The soup I made is bad.\"\nsentence4 = \"The sky is blue.\"\n\n# Lista de todas las oraciones\nsentences = [sentence1, sentence2, sentence3, sentence4]\n\n# Inicializar embeddings de OpenAI\nembeddings_model = OpenAIEmbeddings() \n\n# Generar embeddings para todas las oraciones\nembeddings = embeddings_model.embed_documents(sentences)\n\n# Convertir embeddings a un array de numpy para el c\u00e1lculo de similitud\nembeddings_array = np.array(embeddings)\n\n# Calcular la similitud coseno entre todos los pares de oraciones\nsimilarity_matrix = cosine_similarity(embeddings_array)\n\n# Imprimir puntuaciones de similitud\nprint(\"Cosine Similarity Matrix:\")\nprint(\"Sentences:\")\nfor i, sentence in enumerate(sentences):\n    print(f\"{i+1}. {sentence}\")\nprint(\"\\nSimilarity Scores:\")\nfor i in range(len(sentences)):\n    for j in range(i + 1, len(sentences)):\n        print(f\"Similarity between '{sentences[i][:50]}...' and '{sentences[j][:50]}...': {similarity_matrix[i][j]:.4f}\")\n</code></pre></p> Salida <p><pre><code>Cosine Similarity Matrix:\nSentences:\n1. I like pasta.\n2. I like noodles.\n3. The soup I made is bad.\n4. The sky is blue.\n\nSimilarity Scores:\nSimilarity between 'I like pasta....' and 'I like noodles....': 0.9300\nSimilarity between 'I like pasta....' and 'The soup I made is bad....': 0.6500\nSimilarity between 'I like pasta....' and 'The sky is blue....': 0.6100\nSimilarity between 'I like noodles....' and 'The soup I made is bad....': 0.6400\nSimilarity between 'I like noodles....' and 'The sky is blue....': 0.6000\nSimilarity between 'The soup I made is bad....' and 'The sky is blue....': 0.6700\n</code></pre> Con mayor similitud entre la oraci\u00f3n 3 y la oraci\u00f3n 4 debido a que ambas est\u00e1n relacionadas con el clima, e incluso una similitud mayor entre las oraciones 1 y 2 ya que est\u00e1n relacionadas con comidas italianas.</p> <p>El modelo <code>text-embedding-ada-002</code> es un Transformer optimizado para tareas de embeddings, entrenado en grandes cantidades de datos de texto para capturar relaciones sem\u00e1nticas.</p> <p>Para tener en cuenta</p> <p>El modelo <code>text-embedding-ada-002</code> de OpenAI es un Transformer optimizado para tareas de embeddings, entrenado en grandes cantidades de datos de texto para capturar relaciones sem\u00e1nticas. El texto de entrada (por ejemplo, \"I like pasta.\") se tokeniza, dividiendo las palabras o subpalabras en unidades (tokens) que el modelo entiende.</p> <p>Cada token se convierte en un vector inicial (word embedding) basado en un vocabulario aprendido durante el entrenamiento.</p> <p>Un Transformer procesa los tokens a trav\u00e9s de m\u00faltiples capas de redes neuronales.</p> <p>Las capas de atenci\u00f3n capturan la importancia de cada token en relaci\u00f3n con los dem\u00e1s, es decir, el contexto. Por ejemplo, en \"I like pasta.\", \"pasta\" se interpreta en el contexto de \"like\", lo que da un significado positivo.</p> <p>Esto produce representaciones contextuales que reflejan no solo las palabras individuales, sino tambi\u00e9n su relaci\u00f3n en la frase. Las capas finales del modelo combinan las representaciones contextuales en un solo vector fijo (por ejemplo, 1536 dimensiones para <code>text-embedding-ada-002</code>).</p> <p>Este vector es una representaci\u00f3n densa del significado sem\u00e1ntico del texto, donde textos con significados similares (por ejemplo, \"I like pasta.\" y \"I like noodles.\") tienen vectores cercanos en el espacio.</p> <p>Hemos usado el producto escalar para compararlos, ya que este es proporcional a la distancia entre los vectores. Recordemos que dados dos vectores \\(\\mathbf{A}\\) y \\(\\mathbf{B}\\), el producto escalar est\u00e1 definido como \\(\\mathbf{A} \\cdot \\mathbf{B} = AB \\cos(\\theta)\\), como \\(\\cos(0) = 1\\) entonces los vectores son m\u00e1s cercanos entre m\u00e1s pr\u00f3ximo est\u00e9 este n\u00famero de 1.</p>"},{"location":"Unidad3/modulo3/#creando-la-vectorstore","title":"Creando la Vectorstore","text":"<p>Existen diversas opciones de almacenamiento vectorial, cada una con caracter\u00edsticas espec\u00edficas. Algunas de las opciones m\u00e1s populares incluyen:</p> <ul> <li>Pinecone: Ofrece un servicio escalable y r\u00e1pido para almacenar y buscar vectores, ideal para aplicaciones en la nube.</li> <li>Faiss: Un framework de Facebook AI Research que es eficiente para b\u00fasquedas de similitud y clustering, especialmente \u00fatil para grandes cantidades de datos.</li> <li>Annoy: Desarrollado por Spotify, es adecuado para b\u00fasquedas aproximadas en grandes datasets, optimizando el uso de memoria.</li> <li>Chroma: Es una opci\u00f3n vers\u00e1til que puede funcionar tanto localmente como en la nube, facilitando la integraci\u00f3n con aplicaciones que requieren b\u00fasquedas r\u00e1pidas y eficientes.</li> </ul> <p>En este ejemplo, utilizaremos Chroma debido a su flexibilidad y facilidad de uso cuando se implementa localmente. Chroma permite almacenar los embeddings generados y realizar b\u00fasquedas de similitud de manera eficiente, lo cual es ideal para aplicaciones que requieren procesamiento r\u00e1pido sin depender de servicios externos.</p> <p>Volvemos al ejemplo del PDF (<code>attention.pdf</code>). Lo cargaremos con <code>PyPDFLoader</code> y lo dividiremos en fragmentos usando <code>RecursiveCharacterTextSplitter</code>.</p> <pre><code>from langchain.document_loaders import PyPDFLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\n# Cargamos el PDF\noutput_path = \".content/\"\nfile_path = output_path + \"attention.pdf\"\nloader = PyPDFLoader(file_path)\ndocuments = loader.load()\n\n# Dividimos en fragmentos\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=500,  # Tama\u00f1o de cada fragmento\n    chunk_overlap=50  # Solapamiento entre fragmentos\n)\nsplit_docs = text_splitter.split_documents(documents)\n\n# Verificamos cu\u00e1ntos fragmentos tenemos\nprint(f\"Se generaron {len(split_docs)} fragmentos del PDF.\")\n</code></pre> <pre><code>Se generaron 93 fragmentos del PDF.\n</code></pre> <p>A continuaci\u00f3n, crearemos los embeddings.</p> <pre><code>from langchain_openai import OpenAIEmbeddings\n\n# Configuramos el modelo de embeddings de OpenAI\nembedding_model = OpenAIEmbeddings()  # Usa la variable de entorno OPENAI_API_KEY\n\n# Probamos con un fragmento para verificar\nsample_chunk = split_docs[0].page_content\nsample_embedding = embedding_model.embed_query(sample_chunk)\nprint(f\"Dimensi\u00f3n del embedding: {len(sample_embedding)}\")\nprint(f\"Primeros 5 valores: {sample_embedding[:5]}\")\n</code></pre> <pre><code>Dimensi\u00f3n del embedding: 1536\nPrimeros 5 valores: [-0.012028052471578121, 0.01199324894696474, 0.017206797376275063, -0.027745263651013374, -0.0014739236794412136]\n`### Creando la base de datos\n\nGuardaremos los fragmentos y sus embeddings generados por OpenAI en una vectorstore de `Chroma`.\n\n```python\nfrom langchain.vectorstores import Chroma\n\n# Creamos la vectorstore con Chroma\nvectorstore = Chroma.from_documents(\n    documents=split_docs,  # Los fragmentos del PDF\n    embedding=embedding_model,  # Embeddings de OpenAI\n    persist_directory=\"./chroma_db_openai\"  # Directorio para esta versi\u00f3n\n)\n\n# Verificamos cu\u00e1ntos documentos se almacenaron\nprint(f\"Se almacenaron {vectorstore._collection.count()} fragmentos en la vectorstore.\")\n</code></pre> <pre><code>Se almacenaron 93 fragmentos en la vectorstore.\n</code></pre> <p>Con la vectorstore lista, podemos buscar fragmentos relevantes para una consulta como \"attention mechanism\".</p> <pre><code># Realizamos una b\u00fasqueda sem\u00e1ntica\nquery = \"attention mechanism\"\nresults = vectorstore.similarity_search(query, k=3)  # Top 3 fragmentos m\u00e1s similares\n\n# Mostramos los resultados\nfor i, result in enumerate(results):\n    print(f\"Resultado {i + 1}:\")\n    print(result.page_content)\n    print(f\"Metadatos: {result.metadata}\")\n    print(\"-\" * 50)\n</code></pre> <pre><code>Resultado 1:\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\nare used in conjunction with a recurrent network.\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\nMetadatos: {'page': 1, 'page_label': '2', 'source': '.content/attention.pdf'}\n--------------------------------------------------\nResultado 2:\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\n...\n</code></pre>"},{"location":"Unidad3/modulo3/#persistencia-de-la-vectorstore","title":"Persistencia de la Vectorstore","text":"<p>La base de datos se guarda en <code>./chroma_db_openai</code>. Para cargarla en otros scripts, de esta manera no tendremos que pagar por los embeddings cada vez que un script requiera realizar consultas sobre el documento.</p> <pre><code># Cargar la vectorstore existente\nloaded_vectorstore = Chroma(\n    persist_directory=\"./chroma_db_openai\",\n    embedding_function=embedding_model\n)\n\n# Verificamos que se carg\u00f3\nprint(f\"Fragmentos cargados desde disco: {loaded_vectorstore._collection.count()}\")\n</code></pre> <pre><code>Fragmentos cargados desde disco: 93\n</code></pre> <p>Veamos:</p> <pre><code>query = \"embeddings\"\nresults = loaded_vectorstore.similarity_search(query, k=3)  # Top 3 fragmentos m\u00e1s similares\n\n# Mostramos los resultados\nfor i, result in enumerate(results):\n    print(f\"Resultado {i + 1}:\")\n    print(result.page_content)\n    print(f\"Metadatos: {result.metadata}\")\n    print(\"-\" * 50)\n</code></pre> <pre><code>Resultado 1:\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\narXiv:1703.03130, 2017.\n[23] Minh-Thang Luong, Quoc V . Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\nsequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\n11\nMetadatos: {'page': 10, 'page_label': '11', 'source': '.content/attention.pdf'}\n--------------------------------------------------\nResultado 2:\n2006.\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\npreprint arXiv:1608.05859, 2016.\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\nMetadatos: {'page': 11, 'page_label': '12', 'source': '.content/attention.pdf'}\n--------------------------------------------------\n...\n</code></pre>"},{"location":"Unidad3/modulo3/#retrievalqa","title":"RetrievalQA","text":"<p>Combinaremos la vectorstore con un modelo de lenguaje de OpenAI (<code>gpt-3.5-turbo</code> por defecto) para responder preguntas. <code>RetrievalQA</code> buscar\u00e1 fragmentos relevantes en la vectorstore y los usar\u00e1 como contexto para generar respuestas.</p> <pre><code>from langchain.chains import RetrievalQA\nfrom langchain.chat_models import ChatOpenAI\n\n# Configuramos el modelo de lenguaje\nllm = ChatOpenAI(\n    model_name=\"gpt-3.5-turbo\",\n    temperature=0  # Respuestas m\u00e1s precisas y menos creativas\n)\n\n# Creamos la cadena RetrievalQA\nqa_chain = RetrievalQA.from_chain_type(\n    llm=llm,\n    chain_type=\"stuff\",  # Usa todos los fragmentos relevantes directamente\n    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 3}),  # Top 3 fragmentos m\u00e1s similares\n    return_source_documents=True  # Devuelve los fragmentos usados como fuente\n)\n</code></pre>"},{"location":"Unidad3/modulo3/#preguntas-al-pdf","title":"Preguntas al PDF","text":"<p>Ahora podemos preguntar algo sobre el contenido del PDF:</p> C\u00f3digoSalida <pre><code>query2 = \"\u00bfC\u00f3mo se usa la atenci\u00f3n?\"\nresult2 = qa_chain({\"query\": query2})\n\n# Mostramos la respuesta\nprint(\"Respuesta:\")\nprint(result2[\"result\"])\nprint(\"Fragmentos utilizados como fuente:\")\nfor i, doc in enumerate(result2[\"source_documents\"]):\n    print(f\"Fuente {i + 1}:\")\n    print(doc.page_content)\n    print(f\"Metadatos: {doc.metadata}\")\n    print(\"-\" * 50)\n</code></pre> <pre><code>Respuesta:\nLa atenci\u00f3n se utiliza en el contexto de modelos de aprendizaje autom\u00e1tico, como en el caso de la atenci\u00f3n en el mecanismo de atenci\u00f3n de un modelo de lenguaje. La atenci\u00f3n se aplica para que el modelo pueda enfocarse en partes espec\u00edficas de la entrada durante el proceso de aprendizaje y toma de decisiones.\n\nFragmentos utilizados como fuente:\nFuente 1:\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n&lt;EOS&gt;\n&lt;pad&gt;\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\nsentence. We give two such examples above, from two different heads from the encoder self-attention\nat layer 5 of 6. The heads clearly learned to perform different tasks.\n15\nMetadatos: {'page_label': '15', 'page': 14, 'source': '.content/attention.pdf'}\n--------------------------------------------------\nFuente 2:\n.\n&lt;EOS&gt;\n&lt;pad&gt;\n&lt;pad&gt;\n&lt;pad&gt;\n&lt;pad&gt;\n&lt;pad&gt;\n&lt;pad&gt;\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\nthe verb \u2018making\u2019, completing the phrase \u2018making...more difficult\u2019. Attentions here shown only for\nthe word \u2018making\u2019. Different colors represent different heads. Best viewed in color.\n13\nMetadatos: {'page_label': '13', 'page': 12, 'source': '.content/attention.pdf'}\n--------------------------------------------------\nFuente 3:\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n&lt;EOS&gt;\n&lt;pad&gt;\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\nFull attentions for head 5. Bottom: Isolated attentions from just the word \u2018its\u2019 for attention heads 5\nand 6. Note that the attentions are very sharp for this word.\n14\nMetadatos: {'source': '.content/attention.pdf', 'page_label': '14', 'page': 13}\n--------------------------------------------------\n</code></pre> <p>\u00a1Felicidades por llegar hasta el final del m\u00f3dulo y del curso! Has aprendido a integrar plantillas de prompt con cadenas y parsers de salida, implementaste cadenas con memoria y, finalmente, has practicado c\u00f3mo dividir documentos en fragmentos, almacenar embeddings de los fragmentos en una base de datos vectorial y realizar RAG sobre esta base de datos. Te invito a realizar la actividad de aprendizaje, donde crear\u00e1s y desplegar\u00e1s tu aplicaci\u00f3n RAG siguiendo los pasos que acabas de estudiar.</p>"},{"location":"Unidad3/modulo3/#glosario","title":"Glosario","text":"<ul> <li>AIMessageChunk: Una respuesta parcial de un mensaje de IA. Se utiliza al transmitir respuestas de un modelo de chat.</li> <li>Configurable runnables: Creaci\u00f3n de Runnables configurables.</li> <li>Context window: El tama\u00f1o m\u00e1ximo de entrada que un modelo de chat puede procesar.</li> <li>Document: Representaci\u00f3n de un documento en LangChain.</li> <li>Embedding models: Modelos que generan embeddings vectoriales para varios tipos de datos.</li> <li>HumanMessage: Representa un mensaje de un usuario humano.</li> <li>Vector stores: Almacenes de datos especializados para almacenar y buscar eficientemente embeddings vectoriales.</li> </ul>"},{"location":"Unidad3/modulo3/#evidencia-de-aprendizaje","title":"Evidencia de Aprendizaje","text":"M\u00f3dulo 3 Proyecto Integrador: construcci\u00f3n y despliegue de un sistema RAG EA3. Chat con tus datos <p>\u00a1Felicidades por llegar al final del curso! En tu \u00faltima entrega, practicar\u00e1s las siguientes habilidades:</p> <p>Instrucciones 1. Carga de documentos: usa PyPDFLoader para cargar 5 documentos en PDF de tu inter\u00e9s.</p> <ol> <li> <p>Divisi\u00f3n de documentos: Utiliza RecursiveCharacterTextSplitter o CharacterTextSplitter para dividir los documentos en fragmentos.</p> </li> <li> <p>Embeddings: emplea OpenAIEmbeddings para crear embeddings para tus fragmentos.</p> </li> <li> <p>Almacenamiento vectorial: carga los embeddings en una base de datos vectorial, como Chroma.</p> </li> <li> <p>Instrucciones: ilustra el uso de consultas sobre tus datos cargados en la base de datos vectorial, utilizando consultas por similitud y consultas usando el algoritmo MMR.</p> </li> </ol> <p>Desarrolla tu proyecto en un Jupyter Notebook y carga tu soluci\u00f3n. No olvides agregar comentarios en celdas de Markdown que expliquen el c\u00f3digo y tus razonamientos.</p> <p>Opcional: investiga sobre plataformas de despliegue de tu aplicaci\u00f3n, como Streamlit, Hugging Face, Gradio, etc., y despliega tu RAG para que otros usuarios puedan usarla. Tambi\u00e9n, investiga sobre retrievers y escoge la estrategia de retrieval que mejor se adapte a las necesidades de tu app desplegada.</p> <p>Guarda los documentos con la siguiente nomenclatura:</p> <ul> <li>Apellido_Nombre del estudiante.ipynb Ejemplo: </li> <li>L\u00f3pez_Karla.ipynb</li> </ul> <p>Finalmente, haz clic en el bot\u00f3n Cargar Tarea, sube tu archivo y presiona el bot\u00f3n Enviar para remitirlo a tu profesor con el fin de que lo eval\u00fae y retroalimente. |</p> <p>\ud83d\udcd6 Nota</p> <p>Conoce los criterios de evaluaci\u00f3n de esta evidencia de aprendizaje consultando la r\u00fabrica que encontrar\u00e1s a continuaci\u00f3n.</p> Criterios Ponderaci\u00f3n Totales 70 50 5 0 Calidad de las Soluciones Las soluciones a los ejercicios son correctas, demostrando una implementaci\u00f3n adecuada de los conceptos y t\u00e9cnicas requeridos. El estudiante muestra un dominio completo de los temas abordados. Aunque las soluciones no son completamente correctas, se observa un entendimiento y aplicaci\u00f3n adecuada de los conceptos y t\u00e9cnicas involucradas. Hay evidencia de esfuerzo y comprensi\u00f3n de los temas. Las soluciones presentadas son en su mayor\u00eda incorrectas. Se percibe un intento de resolver los ejercicios, pero hay una falta de comprensi\u00f3n de los conceptos y t\u00e9cnicas esenciales. No realiza la entrega 70 Calidad de la entrega El notebook es claro y f\u00e1cil de seguir, incluyendo comentarios detallados sobre el funcionamiento del c\u00f3digo en las celdas Markdown, lo que facilita la comprensi\u00f3n de las soluciones propuestas. El notebook no es particularmente f\u00e1cil de leer, pero a\u00fan as\u00ed incluye comentarios que explican el funcionamiento del c\u00f3digo en las celdas Markdown, mostrando un esfuerzo por aclarar la l\u00f3gica detr\u00e1s del c\u00f3digo. El notebook carece de comentarios acerca del funcionamiento del c\u00f3digo en las celdas Markdown, lo que dificulta la comprensi\u00f3n de las soluciones implementadas. No realiza la entrega 20 Tiempo de la entrega La entrega se realiza a tiempo, cumpliendo con el plazo establecido para la presentaci\u00f3n de la actividad. La entrega se realiza con una semana de atraso. Aunque fuera del plazo original, se considera adecuada para evaluar el trabajo presentado. La entrega se realiza con m\u00e1s de una semana de atraso, lo que indica un retraso significativo en la presentaci\u00f3n de la actividad. No realiza la entrega 10 Ponderaci\u00f3n de la actividad 100 puntos"},{"location":"Unidad3/modulo3/#referencias","title":"Referencias","text":"<p>DeepLearning.AI. (2025). LangChain: Chat with Your Data [Curso en l\u00ednea]. https://learn.deeplearning.ai/langchain-chat-with-your-data</p> <p>LangChain. (2024). Document loaders. Python LangChain Documentation. https://python.langchain.com/docs/integrations/document_loaders/</p>"},{"location":"Unidad3/modulo3/#lecturas-y-material-complementario","title":"Lecturas y material complementario","text":"<p>Te invitamos a explorar el siguiente material para ampliar tus conocimientos sobre Retrieval-Augmented Generation (RAG) y su implementaci\u00f3n con LangChain.</p>"},{"location":"Unidad3/modulo3/#lecturas-recomendadas","title":"\ud83d\udcda Lecturas recomendadas","text":""},{"location":"Unidad3/modulo3/#titulo-langchain-chat-with-your-data","title":"T\u00edtulo: LangChain: Chat with Your Data","text":"<p>Autor: DeepLearning.AI URL: https://learn.deeplearning.ai/langchain-chat-with-your-data Este curso gratuito de DeepLearning.AI ofrece una introducci\u00f3n pr\u00e1ctica a la creaci\u00f3n de aplicaciones RAG utilizando LangChain. Cubre la carga de documentos, la generaci\u00f3n de embeddings, la recuperaci\u00f3n de informaci\u00f3n relevante y la integraci\u00f3n con modelos de lenguaje.</p>"},{"location":"Unidad3/modulo3/#titulo-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks","title":"T\u00edtulo: Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks","text":"<p>Autor: Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., ... &amp; Kiela, D. URL: https://arxiv.org/abs/2005.11401 Este art\u00edculo seminal introduce el concepto de Retrieval-Augmented Generation (RAG), explicando c\u00f3mo combina modelos de recuperaci\u00f3n de informaci\u00f3n con generaci\u00f3n de texto para mejorar el rendimiento en tareas intensivas en conocimiento.</p>"},{"location":"Unidad3/modulo3/#titulo-langchain-documentation-retrieval-augmented-generation","title":"T\u00edtulo: LangChain Documentation: Retrieval-Augmented Generation","text":"<p>Autor: LangChain URL: https://python.langchain.com/docs/use_cases/question_answering/ La documentaci\u00f3n oficial de LangChain ofrece una gu\u00eda detallada sobre c\u00f3mo implementar flujos de trabajo RAG, incluyendo ejemplos pr\u00e1cticos de carga de documentos, creaci\u00f3n de \u00edndices vectoriales y uso de retrievers para aplicaciones de preguntas y respuestas.</p>"},{"location":"Unidad3/modulo3/#videos-recomendados","title":"\ud83c\udfa5 Videos recomendados","text":""},{"location":"Unidad3/modulo3/#titulo-building-rag-applications-with-langchain","title":"T\u00edtulo: Building RAG Applications with LangChain","text":"<p>Autor: DataCamp URL: [https://www.datacamp.com/courses/building-rag-applications-with-langchain] Este curso en video explora paso a paso c\u00f3mo construir aplicaciones RAG utilizando LangChain, con ejemplos pr\u00e1cticos de integraci\u00f3n de bases de datos vectoriales y modelos de lenguaje.</p>"},{"location":"Unidad3/modulo3/#titulo-what-is-retrieval-augmented-generation-rag","title":"T\u00edtulo: What is Retrieval-Augmented Generation (RAG)?","text":"<p>Autor: IBM Technology URL: https://www.youtube.com/watch?v=T-D1OfcDW1M Este video proporciona una explicaci\u00f3n concisa de RAG, destacando c\u00f3mo combina recuperaci\u00f3n de informaci\u00f3n y generaci\u00f3n de texto para mejorar las respuestas de modelos de lenguaje.</p>"},{"location":"assets/admonitions/","title":"Admonitions","text":"<p>Para tener en cuenta</p> <p>Aseg\u00farate de evaluar los posibles sesgos en los datos antes de implementar un modelo de IA. Los sesgos no detectados pueden llevar a decisiones injustas, afectando la equidad y la confianza en la tecnolog\u00eda.</p> <p>-</p> <p>\ud83d\udcd6 Para aprender m\u00e1s</p> <p>Si deseas conocer m\u00e1s sobre [tema], lee el siguiente material: Art\u00edculo de [nombre del art\u00edculo]: URL: [enlace]</p> <ul> <li> Video: Oportunidades en IA Autor: Andrew Ng   Para obtener una visi\u00f3n sobre el panorama actual de la inteligencia artificial, te invito a que veas la conferencia del profesor Andrew Ng Oportunidades en IA. Ver Video</li> </ul> <ul> <li> Sabias que Autor: Andrew Ng   Para obtener una visi\u00f3n sobre el panorama actual de la inteligencia artificial, te invito a que veas la conferencia del profesor Andrew Ng Oportunidades en IA. Ver Video</li> </ul> <ul> <li> Reto formativo Plantemiento:   Para obtener una visi\u00f3n sobre el panorama actual de la inteligencia artificial, te invito a que veas la conferencia del profesor Andrew Ng Oportunidades en IA. Ver Video</li> </ul> <ul> <li> Recurso formativo Plantemiento:   Para obtener una visi\u00f3n sobre el panorama actual de la inteligencia artificial, te invito a que veas la conferencia del profesor Andrew Ng Oportunidades en IA. Ver Video</li> </ul> Divisi\u00f3n en tokens de una frase utilizando el tokenizador de GPT-4. Fuente: ."},{"location":"assets/resources/conociemientosPrevios_foro/","title":"Foro: Transformers y Mecanismo de Atenci\u00f3n","text":"<p>Como actividad inicial del curso, organizaremos una tertulia en nuestra primera sesi\u00f3n. Para participar activamente, debes ver la serie sobre Transformers del canal 3Blue1Brown y estar listo para discutir el funcionamiento de los Modelos de Lenguaje de Gran Escala (LLM) y participar en debates sobre esta arquitectura de red neuronal.</p>"},{"location":"assets/resources/conociemientosPrevios_foro/#desarrollo-o-ruta-metodologica","title":"Desarrollo o ruta metodol\u00f3gica","text":"<p>A continuaci\u00f3n, se detallan los temas clave que deber\u00edas comprender:</p> <ul> <li> <p>Embeddings: Comprende qu\u00e9 son los embeddings y su papel en la representaci\u00f3n de datos en transformers.</p> </li> <li> <p>Tokens: Familiar\u00edzate con el concepto de tokens y su importancia en el procesamiento de datos secuenciales.</p> </li> <li> <p>B\u00fasquedas por Similitud: Investiga c\u00f3mo las b\u00fasquedas por similitud est\u00e1n relacionadas con la distancia euclidiana y la funci\u00f3n coseno, y c\u00f3mo se aplican en transformers.</p> </li> <li> <p>Mecanismo de Atenci\u00f3n y Autoatenci\u00f3n: Entiende el funcionamiento del mecanismo de atenci\u00f3n, incluyendo la autoatenci\u00f3n.</p> </li> <li> <p>Capas de Perceptrones en un Transformer: Aprende c\u00f3mo funcionan las capas de perceptrones dentro de la arquitectura de un transformer.</p> </li> <li> <p>Softmax: Aseg\u00farate de comprender qu\u00e9 es la funci\u00f3n softmax y su aplicaci\u00f3n en la normalizaci\u00f3n de las salidas de los transformers.</p> </li> </ul>"},{"location":"assets/resources/conociemientosPrevios_foro/#como-entregar-la-actividad","title":"C\u00f3mo entregar la actividad","text":"<p>No hay entrega formal; debes asistir al encuentro sincr\u00f3nico y estar preparado para responder preguntas del profesor y tus compa\u00f1eros. Prep\u00e1rate para participar activamente en la discusi\u00f3n, aportando tus ideas y preguntas sobre estos temas.</p> <p>\u00a1Mucha suerte!</p>"}]}