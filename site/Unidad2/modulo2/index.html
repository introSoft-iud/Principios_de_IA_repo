
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Diplomado en Aplicaciones asistidas por IA">
      
      
        <meta name="author" content="Institución Universitaria Digital de Antioquia">
      
      
        <link rel="canonical" href="https://introsoft-iud.github.io/Diplomado_IA_IUDigital/Unidad2/modulo2/">
      
      
        <link rel="prev" href="../../Unidad%201/modulo1/">
      
      
        <link rel="next" href="../../Unidad3/modulo3/">
      
      
      <link rel="icon" href="../../assets/favicon.ico">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.8">
    
    
      
        <title>Módulo 2 - Diplomado en Aplicaciones asistidas por IA</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.8608ea7d.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat:300,300i,400,400i,700,700i%7CRed+Hat+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Montserrat";--md-code-font:"Red Hat Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../css/custom.css">
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="slate" data-md-color-primary="green" data-md-color-accent="deep-purple">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#modulo-2-cadenas-y-memoria" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="Diplomado en Aplicaciones asistidas por IA" class="md-header__button md-logo" aria-label="Diplomado en Aplicaciones asistidas por IA" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 2a2 2 0 0 1 2 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 0 1 7 7h1a1 1 0 0 1 1 1v3a1 1 0 0 1-1 1h-1v1a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2v-1H2a1 1 0 0 1-1-1v-3a1 1 0 0 1 1-1h1a7 7 0 0 1 7-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 0 1 2-2M7.5 13A2.5 2.5 0 0 0 5 15.5 2.5 2.5 0 0 0 7.5 18a2.5 2.5 0 0 0 2.5-2.5A2.5 2.5 0 0 0 7.5 13m9 0a2.5 2.5 0 0 0-2.5 2.5 2.5 2.5 0 0 0 2.5 2.5 2.5 2.5 0 0 0 2.5-2.5 2.5 2.5 0 0 0-2.5-2.5"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Diplomado en Aplicaciones asistidas por IA
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Módulo 2
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="green" data-md-color-accent="deep-purple"  aria-label="Dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5s-1.65.15-2.39.42zM3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29zm.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14zM20.65 7l-1.77 3.79a7.02 7.02 0 0 0-2.38-4.15zm-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29zM12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="blue" data-md-color-accent="deep-orange"  aria-label="Light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3zm3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95zm-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="Diplomado en Aplicaciones asistidas por IA" class="md-nav__button md-logo" aria-label="Diplomado en Aplicaciones asistidas por IA" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 2a2 2 0 0 1 2 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 0 1 7 7h1a1 1 0 0 1 1 1v3a1 1 0 0 1-1 1h-1v1a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2v-1H2a1 1 0 0 1-1-1v-3a1 1 0 0 1 1-1h1a7 7 0 0 1 7-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 0 1 2-2M7.5 13A2.5 2.5 0 0 0 5 15.5 2.5 2.5 0 0 0 7.5 18a2.5 2.5 0 0 0 2.5-2.5A2.5 2.5 0 0 0 7.5 13m9 0a2.5 2.5 0 0 0-2.5 2.5 2.5 2.5 0 0 0 2.5 2.5 2.5 2.5 0 0 0 2.5-2.5 2.5 2.5 0 0 0-2.5-2.5"/></svg>

    </a>
    Diplomado en Aplicaciones asistidas por IA
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Presentación del curso
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Unidad%201/modulo1/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Módulo 1
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    Módulo 2
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    Módulo 2
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#introduccion-al-modulo" class="md-nav__link">
    <span class="md-ellipsis">
      Introducción al módulo
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#resultados-de-aprendizaje" class="md-nav__link">
    <span class="md-ellipsis">
      Resultados de aprendizaje
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#cronograma-de-actividades-modulo-2" class="md-nav__link">
    <span class="md-ellipsis">
      Cronograma de actividades - Módulo 2
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#cadenas" class="md-nav__link">
    <span class="md-ellipsis">
      Cadenas
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Cadenas">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#langchain-expression-language-lcel" class="md-nav__link">
    <span class="md-ellipsis">
      LangChain Expression Language (LCEL)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#funciones-envueltas-con-runnablelambda" class="md-nav__link">
    <span class="md-ellipsis">
      Funciones (envueltas con RunnableLambda)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#memoria" class="md-nav__link">
    <span class="md-ellipsis">
      Memoria
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#la-ventana-de-contexto" class="md-nav__link">
    <span class="md-ellipsis">
      La ventana de contexto
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#tipos-de-memoria-en-langchain" class="md-nav__link">
    <span class="md-ellipsis">
      Tipos de Memoria en LangChain
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#tipos-de-memoria-en-langchain_1" class="md-nav__link">
    <span class="md-ellipsis">
      Tipos de Memoria en LangChain
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Unidad3/modulo3/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Módulo 3
    
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#introduccion-al-modulo" class="md-nav__link">
    <span class="md-ellipsis">
      Introducción al módulo
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#resultados-de-aprendizaje" class="md-nav__link">
    <span class="md-ellipsis">
      Resultados de aprendizaje
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#cronograma-de-actividades-modulo-2" class="md-nav__link">
    <span class="md-ellipsis">
      Cronograma de actividades - Módulo 2
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#cadenas" class="md-nav__link">
    <span class="md-ellipsis">
      Cadenas
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Cadenas">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#langchain-expression-language-lcel" class="md-nav__link">
    <span class="md-ellipsis">
      LangChain Expression Language (LCEL)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#funciones-envueltas-con-runnablelambda" class="md-nav__link">
    <span class="md-ellipsis">
      Funciones (envueltas con RunnableLambda)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#memoria" class="md-nav__link">
    <span class="md-ellipsis">
      Memoria
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#la-ventana-de-contexto" class="md-nav__link">
    <span class="md-ellipsis">
      La ventana de contexto
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#tipos-de-memoria-en-langchain" class="md-nav__link">
    <span class="md-ellipsis">
      Tipos de Memoria en LangChain
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#tipos-de-memoria-en-langchain_1" class="md-nav__link">
    <span class="md-ellipsis">
      Tipos de Memoria en LangChain
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="modulo-2-cadenas-y-memoria">Módulo 2. Cadenas y Memoria</h1>
<h2 id="introduccion-al-modulo">Introducción al módulo</h2>
<p>Bienvenidos al segundo módulo de nuestro diplomado sobre construcción de aplicaciones asistidas por LLM. En el primer módulo aprendiste a confeccionar instrucciones reutilizables para LLM, los prompt templates, y exploraste cómo acoplar estas instrucciones en cadenas con especificadores de formato llamados output parsers. Sin embargo, estas cadenas de ejecución eran cadenas de un solo turno de interacción entre la IA y el usuario humano. En este módulo aprenderás a darle memoria y contexto a tus cadenas de ejecución. Profundizaremos aún más en el funcionamiento de las cadenas y como limitar su memoria en el contexto del LCEL. Finalmente, pondrás a prueba tu conocimiento creando un chatbot que asiste las labores de un médico realizando tareas secuenciales y que tiene como contexto en su memoria los datos específicos de un paciente.</p>
<p>¡Comencemos!</p>
<h2 id="resultados-de-aprendizaje">Resultados de aprendizaje</h2>
<p>Al finalizar esta módulo, estarás en capacidad de:</p>
<ul>
<li>Usar cadenas dotadas de memoria usando el LECL.</li>
<li>Limitar el tamaño del contexto cargado en la memoria de tus cadenas.</li>
</ul>
<h2 id="cronograma-de-actividades-modulo-2">Cronograma de actividades - Módulo 2</h2>
<table>
<thead>
<tr>
<th>Actividad de aprendizaje</th>
<th>Evidencia de aprendizaje</th>
<th>Semana</th>
<th>Ponderación</th>
</tr>
</thead>
<tbody>
<tr>
<td>EA1:  Cadenas y memoria</td>
<td>EA1: Cadenas y memoria</td>
<td>Semana 4 y 5</td>
<td>25%</td>
</tr>
<tr>
<td><strong>Total</strong></td>
<td></td>
<td></td>
<td><strong>25 %</strong></td>
</tr>
</tbody>
</table>
<!--
Desarrollo temático
-->

<h2 id="cadenas">Cadenas</h2>
<p>Hay varias maneras de instanciar cadenas de ejecución en LangChain, algunas de las cuales fueron exploradas en el módulo 1. Recientemente, LangChain introdujo LangChain Expression Language (LCEL) como el estándar para construir cadenas. Revisemos más detalladamente de qué se trata:</p>
<div class="admonition warning">
<p class="admonition-title">Para tener en cuenta</p>
<p>Recuerda cargar tu llave en las variables de sistema si no lo has hecho:
<div class="language-python highlight"><pre><span></span><code><span id="__span-0-1"><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
</span><span id="__span-0-2"><a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a><span class="kn">import</span><span class="w"> </span><span class="nn">openai</span>
</span><span id="__span-0-3"><a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a><span class="kn">from</span><span class="w"> </span><span class="nn">dotenv</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_dotenv</span><span class="p">,</span> <span class="n">find_dotenv</span>
</span><span id="__span-0-4"><a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a>
</span><span id="__span-0-5"><a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a><span class="c1"># Cargar el archivo .env local</span>
</span><span id="__span-0-6"><a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a><span class="n">_</span> <span class="o">=</span> <span class="n">load_dotenv</span><span class="p">(</span><span class="n">find_dotenv</span><span class="p">())</span> 
</span><span id="__span-0-7"><a id="__codelineno-0-7" name="__codelineno-0-7" href="#__codelineno-0-7"></a><span class="n">openai</span><span class="o">.</span><span class="n">api_key</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;OPENAI_API_KEY&#39;</span><span class="p">]</span>
</span></code></pre></div></p>
</div>
<h3 id="langchain-expression-language-lcel">LangChain Expression Language (LCEL)</h3>
<p>LangChain Expression Language (LCEL) es una sintaxis para definir cadenas. Permite componer objetos ejecutables (runnables)—objetos que pueden ser ejecutados o encadenados—usando el operador <strong>pipe</strong> (|). Un runnable es cualquier componente que implementa la interfaz Runnable, lo que significa que puede procesar entradas y producir salidas. Ejemplos incluyen:</p>
<ul>
<li>Plantillas de prompt (ChatPromptTemplate), como lo hicimos en el módulo 1.</li>
</ul>
<p>El operador de tubería (|) conecta estos componentes, pasando la salida de un Runnable como la entrada al siguiente. Por ejemplo:</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-1-1"><a id="__codelineno-1-1" name="__codelineno-1-1" href="#__codelineno-1-1"></a><span class="n">chain</span> <span class="o">=</span> <span class="n">prompt</span> <span class="o">|</span> <span class="n">llm</span> <span class="o">|</span> <span class="n">output_parser</span>
</span></code></pre></div>
<p>Veamos un ejemplo en detalle:</p>
<div class="tabbed-set tabbed-alternate" data-tabs="1:2"><input checked="checked" id="__tabbed_1_1" name="__tabbed_1" type="radio" /><input id="__tabbed_1_2" name="__tabbed_1" type="radio" /><div class="tabbed-labels"><label for="__tabbed_1_1">Código</label><label for="__tabbed_1_2">Salida</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<div class="language-python highlight"><pre><span></span><code><span id="__span-2-1"><a id="__codelineno-2-1" name="__codelineno-2-1" href="#__codelineno-2-1"></a><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.prompts</span><span class="w"> </span><span class="kn">import</span> <span class="n">ChatPromptTemplate</span>
</span><span id="__span-2-2"><a id="__codelineno-2-2" name="__codelineno-2-2" href="#__codelineno-2-2"></a><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_openai</span><span class="w"> </span><span class="kn">import</span> <span class="n">ChatOpenAI</span>
</span><span id="__span-2-3"><a id="__codelineno-2-3" name="__codelineno-2-3" href="#__codelineno-2-3"></a><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.output_parsers</span><span class="w"> </span><span class="kn">import</span> <span class="n">StrOutputParser</span>
</span><span id="__span-2-4"><a id="__codelineno-2-4" name="__codelineno-2-4" href="#__codelineno-2-4"></a>
</span><span id="__span-2-5"><a id="__codelineno-2-5" name="__codelineno-2-5" href="#__codelineno-2-5"></a><span class="c1"># Define the prompt template</span>
</span><span id="__span-2-6"><a id="__codelineno-2-6" name="__codelineno-2-6" href="#__codelineno-2-6"></a><span class="n">prompt</span> <span class="o">=</span> <span class="n">ChatPromptTemplate</span><span class="o">.</span><span class="n">from_messages</span><span class="p">([</span>
</span><span id="__span-2-7"><a id="__codelineno-2-7" name="__codelineno-2-7" href="#__codelineno-2-7"></a>    <span class="p">(</span><span class="s2">&quot;system&quot;</span><span class="p">,</span> <span class="s2">&quot;You are a world-class technical documentation writer.&quot;</span><span class="p">),</span>
</span><span id="__span-2-8"><a id="__codelineno-2-8" name="__codelineno-2-8" href="#__codelineno-2-8"></a>    <span class="p">(</span><span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;</span><span class="si">{input}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="__span-2-9"><a id="__codelineno-2-9" name="__codelineno-2-9" href="#__codelineno-2-9"></a><span class="p">])</span>
</span><span id="__span-2-10"><a id="__codelineno-2-10" name="__codelineno-2-10" href="#__codelineno-2-10"></a>
</span><span id="__span-2-11"><a id="__codelineno-2-11" name="__codelineno-2-11" href="#__codelineno-2-11"></a><span class="c1"># Initialize the LLM</span>
</span><span id="__span-2-12"><a id="__codelineno-2-12" name="__codelineno-2-12" href="#__codelineno-2-12"></a><span class="n">llm</span> <span class="o">=</span> <span class="n">ChatOpenAI</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-4&quot;</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
</span><span id="__span-2-13"><a id="__codelineno-2-13" name="__codelineno-2-13" href="#__codelineno-2-13"></a>
</span><span id="__span-2-14"><a id="__codelineno-2-14" name="__codelineno-2-14" href="#__codelineno-2-14"></a><span class="c1"># Create the output parser</span>
</span><span id="__span-2-15"><a id="__codelineno-2-15" name="__codelineno-2-15" href="#__codelineno-2-15"></a><span class="n">output_parser</span> <span class="o">=</span> <span class="n">StrOutputParser</span><span class="p">()</span>
</span><span id="__span-2-16"><a id="__codelineno-2-16" name="__codelineno-2-16" href="#__codelineno-2-16"></a>
</span><span id="__span-2-17"><a id="__codelineno-2-17" name="__codelineno-2-17" href="#__codelineno-2-17"></a><span class="c1"># Compose the chain</span>
</span><span id="__span-2-18"><a id="__codelineno-2-18" name="__codelineno-2-18" href="#__codelineno-2-18"></a><span class="n">chain</span> <span class="o">=</span> <span class="n">prompt</span> <span class="o">|</span> <span class="n">llm</span> <span class="o">|</span> <span class="n">output_parser</span>
</span><span id="__span-2-19"><a id="__codelineno-2-19" name="__codelineno-2-19" href="#__codelineno-2-19"></a>
</span><span id="__span-2-20"><a id="__codelineno-2-20" name="__codelineno-2-20" href="#__codelineno-2-20"></a><span class="c1"># Invoke the chain</span>
</span><span id="__span-2-21"><a id="__codelineno-2-21" name="__codelineno-2-21" href="#__codelineno-2-21"></a><span class="n">response</span> <span class="o">=</span> <span class="n">chain</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span><span class="s2">&quot;input&quot;</span><span class="p">:</span> <span class="s2">&quot;How can LangSmith help with testing?&quot;</span><span class="p">})</span>
</span><span id="__span-2-22"><a id="__codelineno-2-22" name="__codelineno-2-22" href="#__codelineno-2-22"></a><span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
</span></code></pre></div>
</div>
<div class="tabbed-block">
<div class="language-bash highlight"><pre><span></span><code><span id="__span-3-1"><a id="__codelineno-3-1" name="__codelineno-3-1" href="#__codelineno-3-1"></a>LangSmith<span class="w"> </span>provides<span class="w"> </span>a<span class="w"> </span>suite<span class="w"> </span>of<span class="w"> </span>tools<span class="w"> </span>designed<span class="w"> </span>to<span class="w"> </span>assist<span class="w"> </span>with<span class="w"> </span>software<span class="w"> </span>testing,<span class="w"> </span>particularly<span class="w"> </span><span class="k">in</span><span class="w"> </span>the<span class="w"> </span>realm<span class="w"> </span>of<span class="w"> </span>language<span class="w"> </span>translation<span class="w"> </span>and<span class="w"> </span>localization.<span class="w"> </span>It<span class="w"> </span>can<span class="w"> </span>be<span class="w"> </span>invaluable<span class="w"> </span><span class="k">for</span><span class="w"> </span>teams<span class="w"> </span>developing<span class="w"> </span>software<span class="w"> </span>that<span class="w"> </span>needs<span class="w"> </span>to<span class="w"> </span><span class="k">function</span><span class="w"> </span><span class="k">in</span><span class="w"> </span>multiple<span class="w"> </span>languages<span class="w"> </span>and<span class="w"> </span>regions.
</span><span id="__span-3-2"><a id="__codelineno-3-2" name="__codelineno-3-2" href="#__codelineno-3-2"></a>
</span><span id="__span-3-3"><a id="__codelineno-3-3" name="__codelineno-3-3" href="#__codelineno-3-3"></a><span class="m">1</span>.<span class="w"> </span>**Automated<span class="w"> </span>Testing:**<span class="w"> </span>LangSmith<span class="w"> </span>can<span class="w"> </span>automatically<span class="w"> </span><span class="nb">test</span><span class="w"> </span>your<span class="w"> </span>software’s<span class="w"> </span>language<span class="w"> </span>functionality,<span class="w"> </span>ensuring<span class="w"> </span>translations<span class="w"> </span>are<span class="w"> </span>accurate,<span class="w"> </span>context-appropriate,<span class="w"> </span>and<span class="w"> </span>that<span class="w"> </span>all<span class="w"> </span>elements<span class="w"> </span>of<span class="w"> </span>the<span class="w"> </span>user<span class="w"> </span>interface<span class="w"> </span>are<span class="w"> </span>correctly<span class="w"> </span>localized.
</span><span id="__span-3-4"><a id="__codelineno-3-4" name="__codelineno-3-4" href="#__codelineno-3-4"></a>
</span><span id="__span-3-5"><a id="__codelineno-3-5" name="__codelineno-3-5" href="#__codelineno-3-5"></a><span class="m">2</span>.<span class="w"> </span>**Quality<span class="w"> </span>Assurance:**<span class="w"> </span>By<span class="w"> </span>checking<span class="w"> </span>translations<span class="w"> </span>against<span class="w"> </span>a<span class="w"> </span>comprehensive<span class="w"> </span>database,<span class="w"> </span>LangSmith<span class="w"> </span>aids<span class="w"> </span><span class="k">in</span><span class="w"> </span>maintaining<span class="w"> </span>high<span class="w"> </span>translation<span class="w"> </span>quality,<span class="w"> </span>reducing<span class="w"> </span>the<span class="w"> </span>risk<span class="w"> </span>of<span class="w"> </span>miscommunication<span class="w"> </span>or<span class="w"> </span>confusion<span class="w"> </span><span class="k">for</span><span class="w"> </span>users.
</span><span id="__span-3-6"><a id="__codelineno-3-6" name="__codelineno-3-6" href="#__codelineno-3-6"></a>
</span><span id="__span-3-7"><a id="__codelineno-3-7" name="__codelineno-3-7" href="#__codelineno-3-7"></a><span class="m">3</span>.<span class="w"> </span>**Regression<span class="w"> </span>Testing:**<span class="w"> </span>When<span class="w"> </span>updates<span class="w"> </span>or<span class="w"> </span>changes<span class="w"> </span>are<span class="w"> </span>made<span class="w"> </span>to<span class="w"> </span>the<span class="w"> </span>software,<span class="w"> </span>LangSmith<span class="w"> </span>can<span class="w"> </span><span class="nb">help</span><span class="w"> </span>ensure<span class="w"> </span>that<span class="w"> </span>these<span class="w"> </span>changes<span class="w"> </span>have<span class="w"> </span>not<span class="w"> </span>negatively<span class="w"> </span>affected<span class="w"> </span>the<span class="w"> </span>language<span class="w"> </span>functionality.
</span><span id="__span-3-8"><a id="__codelineno-3-8" name="__codelineno-3-8" href="#__codelineno-3-8"></a>
</span><span id="__span-3-9"><a id="__codelineno-3-9" name="__codelineno-3-9" href="#__codelineno-3-9"></a><span class="m">4</span>.<span class="w"> </span>**Cultural<span class="w"> </span>Accuracy:**<span class="w"> </span>Beyond<span class="w"> </span>simple<span class="w"> </span>language<span class="w"> </span>translation,<span class="w"> </span>LangSmith<span class="w"> </span>can<span class="w"> </span>also<span class="w"> </span>verify<span class="w"> </span>that<span class="w"> </span>cultural<span class="w"> </span>nuances<span class="w"> </span>and<span class="w"> </span><span class="nb">local</span><span class="w"> </span>customs<span class="w"> </span>are<span class="w"> </span>appropriately<span class="w"> </span>considered,<span class="w"> </span>which<span class="w"> </span>can<span class="w"> </span>greatly<span class="w"> </span>improve<span class="w"> </span>user<span class="w"> </span>experience.
</span><span id="__span-3-10"><a id="__codelineno-3-10" name="__codelineno-3-10" href="#__codelineno-3-10"></a>
</span><span id="__span-3-11"><a id="__codelineno-3-11" name="__codelineno-3-11" href="#__codelineno-3-11"></a><span class="m">5</span>.<span class="w"> </span>**Reporting<span class="w"> </span>and<span class="w"> </span>Analytics:**<span class="w"> </span>LangSmith<span class="w"> </span>provides<span class="w"> </span>detailed<span class="w"> </span>reports<span class="w"> </span>on<span class="w"> </span>testing<span class="w"> </span>outcomes,<span class="w"> </span>highlighting<span class="w"> </span>any<span class="w"> </span>issues<span class="w"> </span>or<span class="w"> </span>potential<span class="w"> </span>areas<span class="w"> </span><span class="k">for</span><span class="w"> </span>improvement.<span class="w"> </span>This<span class="w"> </span>can<span class="w"> </span>provide<span class="w"> </span>valuable<span class="w"> </span>insights<span class="w"> </span><span class="k">for</span><span class="w"> </span>your<span class="w"> </span>development<span class="w"> </span>team.
</span><span id="__span-3-12"><a id="__codelineno-3-12" name="__codelineno-3-12" href="#__codelineno-3-12"></a>
</span><span id="__span-3-13"><a id="__codelineno-3-13" name="__codelineno-3-13" href="#__codelineno-3-13"></a>By<span class="w"> </span>integrating<span class="w"> </span>LangSmith<span class="w"> </span>into<span class="w"> </span>your<span class="w"> </span>testing<span class="w"> </span>process,<span class="w"> </span>you<span class="w"> </span>can<span class="w"> </span>more<span class="w"> </span>effectively<span class="w"> </span>ensure<span class="w"> </span>the<span class="w"> </span>quality<span class="w"> </span>and<span class="w"> </span>accuracy<span class="w"> </span>of<span class="w"> </span>your<span class="w"> </span>software<span class="err">&#39;</span>s<span class="w"> </span>multilingual<span class="w"> </span>and<span class="w"> </span>multicultural<span class="w"> </span>features,<span class="w"> </span>improving<span class="w"> </span>overall<span class="w"> </span>user<span class="w"> </span>experience<span class="w"> </span>and<span class="w"> </span>satisfaction.
</span></code></pre></div>
</div>
</div>
</div>
<h3 id="funciones-envueltas-con-runnablelambda">Funciones (envueltas con RunnableLambda)</h3>
<p>Podemos usar cadenas con funciones al comienzo de la línea de ejecución, un cierto tipo de función muy versátil son las funciones de la clase RunnableLambda. Estas están diseñadas para integrar funciones personalizadas de Python en cadenas de LangChain. Permiten a los desarrolladores envolver funciones de Python arbitrarias (o funciones lambda) en un objeto ejecutable, haciéndolas compatibles con la sintaxis del LCEL. Es decir, convierte una función de Python en un componente encadenable.</p>
<p>La función debe aceptar una entrada compatible con la salida del paso anterior y producir una salida compatible con el siguiente paso.</p>
<p>Por ejemplo:</p>
<p><div class="language-python highlight"><pre><span></span><code><span id="__span-4-1"><a id="__codelineno-4-1" name="__codelineno-4-1" href="#__codelineno-4-1"></a><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.runnables</span><span class="w"> </span><span class="kn">import</span> <span class="n">RunnableLambda</span>
</span><span id="__span-4-2"><a id="__codelineno-4-2" name="__codelineno-4-2" href="#__codelineno-4-2"></a>
</span><span id="__span-4-3"><a id="__codelineno-4-3" name="__codelineno-4-3" href="#__codelineno-4-3"></a><span class="c1"># Wrap a function</span>
</span><span id="__span-4-4"><a id="__codelineno-4-4" name="__codelineno-4-4" href="#__codelineno-4-4"></a><span class="n">runnable_sumaUno</span> <span class="o">=</span> <span class="n">RunnableLambda</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># suma 1 a la entrada</span>
</span><span id="__span-4-5"><a id="__codelineno-4-5" name="__codelineno-4-5" href="#__codelineno-4-5"></a><span class="n">runnable_cuadrado</span> <span class="o">=</span> <span class="n">RunnableLambda</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># eleva al cuadrado la entrada</span>
</span><span id="__span-4-6"><a id="__codelineno-4-6" name="__codelineno-4-6" href="#__codelineno-4-6"></a>
</span><span id="__span-4-7"><a id="__codelineno-4-7" name="__codelineno-4-7" href="#__codelineno-4-7"></a><span class="c1"># La cadena de ejecución sería</span>
</span><span id="__span-4-8"><a id="__codelineno-4-8" name="__codelineno-4-8" href="#__codelineno-4-8"></a><span class="n">chain</span> <span class="o">=</span> <span class="n">runnable_sumaUno</span> <span class="o">|</span> <span class="n">runnable_cuadrado</span>  <span class="c1"># encadena las funciones</span>
</span><span id="__span-4-9"><a id="__codelineno-4-9" name="__codelineno-4-9" href="#__codelineno-4-9"></a><span class="n">result</span> <span class="o">=</span> <span class="n">chain</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>  <span class="c1"># (5 + 1) ** 2 = 36</span>
</span><span id="__span-4-10"><a id="__codelineno-4-10" name="__codelineno-4-10" href="#__codelineno-4-10"></a><span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>  <span class="c1"># Output: 36</span>
</span></code></pre></div>
Vemos otro ejemplo:</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-5-1"><a id="__codelineno-5-1" name="__codelineno-5-1" href="#__codelineno-5-1"></a><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.runnables</span><span class="w"> </span><span class="kn">import</span> <span class="n">RunnableLambda</span>
</span><span id="__span-5-2"><a id="__codelineno-5-2" name="__codelineno-5-2" href="#__codelineno-5-2"></a>
</span><span id="__span-5-3"><a id="__codelineno-5-3" name="__codelineno-5-3" href="#__codelineno-5-3"></a><span class="c1"># Define funciones lambda individuales</span>
</span><span id="__span-5-4"><a id="__codelineno-5-4" name="__codelineno-5-4" href="#__codelineno-5-4"></a><span class="n">add_prefix</span> <span class="o">=</span> <span class="n">RunnableLambda</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="sa">f</span><span class="s2">&quot;Hello, </span><span class="si">{</span><span class="n">x</span><span class="si">}</span><span class="s2">!&quot;</span><span class="p">)</span>
</span><span id="__span-5-5"><a id="__codelineno-5-5" name="__codelineno-5-5" href="#__codelineno-5-5"></a><span class="n">to_upper</span> <span class="o">=</span> <span class="n">RunnableLambda</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">upper</span><span class="p">())</span>
</span><span id="__span-5-6"><a id="__codelineno-5-6" name="__codelineno-5-6" href="#__codelineno-5-6"></a>
</span><span id="__span-5-7"><a id="__codelineno-5-7" name="__codelineno-5-7" href="#__codelineno-5-7"></a><span class="c1"># Construir la cadena</span>
</span><span id="__span-5-8"><a id="__codelineno-5-8" name="__codelineno-5-8" href="#__codelineno-5-8"></a><span class="n">chain</span> <span class="o">=</span> <span class="n">add_prefix</span> <span class="o">|</span> <span class="n">to_upper</span>
</span><span id="__span-5-9"><a id="__codelineno-5-9" name="__codelineno-5-9" href="#__codelineno-5-9"></a>
</span><span id="__span-5-10"><a id="__codelineno-5-10" name="__codelineno-5-10" href="#__codelineno-5-10"></a><span class="c1"># Invocar la cadena</span>
</span><span id="__span-5-11"><a id="__codelineno-5-11" name="__codelineno-5-11" href="#__codelineno-5-11"></a><span class="n">result</span> <span class="o">=</span> <span class="n">chain</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="s2">&quot;Alice&quot;</span><span class="p">)</span>
</span><span id="__span-5-12"><a id="__codelineno-5-12" name="__codelineno-5-12" href="#__codelineno-5-12"></a><span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>  <span class="c1"># Salida: HELLO, ALICE!</span>
</span></code></pre></div>
<p>Veamos cómo construir una cadena que toma una consulta de usuario, la formatea en un prompt, la procesa con un LLM y extrae la primera palabra de la respuesta.</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-6-1"><a id="__codelineno-6-1" name="__codelineno-6-1" href="#__codelineno-6-1"></a><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.runnables</span><span class="w"> </span><span class="kn">import</span> <span class="n">RunnableLambda</span>
</span><span id="__span-6-2"><a id="__codelineno-6-2" name="__codelineno-6-2" href="#__codelineno-6-2"></a><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.prompts</span><span class="w"> </span><span class="kn">import</span> <span class="n">PromptTemplate</span>
</span><span id="__span-6-3"><a id="__codelineno-6-3" name="__codelineno-6-3" href="#__codelineno-6-3"></a><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_openai</span><span class="w"> </span><span class="kn">import</span> <span class="n">ChatOpenAI</span>  <span class="c1"># o usar otro LLM</span>
</span><span id="__span-6-4"><a id="__codelineno-6-4" name="__codelineno-6-4" href="#__codelineno-6-4"></a>
</span><span id="__span-6-5"><a id="__codelineno-6-5" name="__codelineno-6-5" href="#__codelineno-6-5"></a><span class="c1"># Definir componentes</span>
</span><span id="__span-6-6"><a id="__codelineno-6-6" name="__codelineno-6-6" href="#__codelineno-6-6"></a>
</span><span id="__span-6-7"><a id="__codelineno-6-7" name="__codelineno-6-7" href="#__codelineno-6-7"></a><span class="c1"># Crear un prompt template y llenarlo</span>
</span><span id="__span-6-8"><a id="__codelineno-6-8" name="__codelineno-6-8" href="#__codelineno-6-8"></a><span class="n">format_prompt</span> <span class="o">=</span> <span class="n">RunnableLambda</span><span class="p">(</span>
</span><span id="__span-6-9"><a id="__codelineno-6-9" name="__codelineno-6-9" href="#__codelineno-6-9"></a>    <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">PromptTemplate</span><span class="o">.</span><span class="n">from_template</span><span class="p">(</span><span class="s2">&quot;Answer briefly: </span><span class="si">{query}</span><span class="s2">&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">query</span><span class="o">=</span><span class="n">x</span><span class="p">)</span>
</span><span id="__span-6-10"><a id="__codelineno-6-10" name="__codelineno-6-10" href="#__codelineno-6-10"></a><span class="p">)</span>
</span><span id="__span-6-11"><a id="__codelineno-6-11" name="__codelineno-6-11" href="#__codelineno-6-11"></a>
</span><span id="__span-6-12"><a id="__codelineno-6-12" name="__codelineno-6-12" href="#__codelineno-6-12"></a><span class="c1"># Extraer la primera palabra de la respuesta</span>
</span><span id="__span-6-13"><a id="__codelineno-6-13" name="__codelineno-6-13" href="#__codelineno-6-13"></a><span class="n">extract_first_word</span> <span class="o">=</span> <span class="n">RunnableLambda</span><span class="p">(</span>
</span><span id="__span-6-14"><a id="__codelineno-6-14" name="__codelineno-6-14" href="#__codelineno-6-14"></a>    <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">content</span><span class="o">.</span><span class="n">split</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span> <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="s1">&#39;content&#39;</span><span class="p">)</span> <span class="k">else</span> <span class="n">x</span><span class="o">.</span><span class="n">split</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
</span><span id="__span-6-15"><a id="__codelineno-6-15" name="__codelineno-6-15" href="#__codelineno-6-15"></a><span class="p">)</span>
</span><span id="__span-6-16"><a id="__codelineno-6-16" name="__codelineno-6-16" href="#__codelineno-6-16"></a>
</span><span id="__span-6-17"><a id="__codelineno-6-17" name="__codelineno-6-17" href="#__codelineno-6-17"></a><span class="n">llm</span> <span class="o">=</span> <span class="n">ChatOpenAI</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-3.5-turbo&quot;</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span><span id="__span-6-18"><a id="__codelineno-6-18" name="__codelineno-6-18" href="#__codelineno-6-18"></a>
</span><span id="__span-6-19"><a id="__codelineno-6-19" name="__codelineno-6-19" href="#__codelineno-6-19"></a><span class="c1"># Construir la cadena</span>
</span><span id="__span-6-20"><a id="__codelineno-6-20" name="__codelineno-6-20" href="#__codelineno-6-20"></a><span class="n">chain</span> <span class="o">=</span> <span class="n">format_prompt</span> <span class="o">|</span> <span class="n">llm</span> <span class="o">|</span> <span class="n">extract_first_word</span>
</span><span id="__span-6-21"><a id="__codelineno-6-21" name="__codelineno-6-21" href="#__codelineno-6-21"></a>
</span><span id="__span-6-22"><a id="__codelineno-6-22" name="__codelineno-6-22" href="#__codelineno-6-22"></a><span class="c1"># Invocar la cadena</span>
</span><span id="__span-6-23"><a id="__codelineno-6-23" name="__codelineno-6-23" href="#__codelineno-6-23"></a><span class="n">result</span> <span class="o">=</span> <span class="n">chain</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="s2">&quot;¿Cómo se llama el presidente de Colombia?&quot;</span><span class="p">)</span>
</span><span id="__span-6-24"><a id="__codelineno-6-24" name="__codelineno-6-24" href="#__codelineno-6-24"></a><span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>  <span class="c1"># Salida: Iván (recuerda que gpt-3.5 fue entrenado en datos hasta octubre de 2023)</span>
</span></code></pre></div>
<p>La cadena procesa la consulta de entrada paso a paso: formatea la consulta, la procesa con el LLM y extrae la primera palabra de la respuesta. Puedes reemplazar <code>ChatOpenAI</code> con otro modelo (por ejemplo, <code>HuggingFaceHub</code>) si es necesario.
Language models (ChatOpenAI).</p>
<h2 id="memoria">Memoria</h2>
<p>La memoria es el mecanismo por el cual le damos al LLM contexto de nuestras interacciones previas.</p>
<p>Para explorar el uso de la memoria, instanciaremos una cadena de conversación a partir de la clase preconstruida <code>ConversationChain</code>. Importamos los módulos necesarios:</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-7-1"><a id="__codelineno-7-1" name="__codelineno-7-1" href="#__codelineno-7-1"></a><span class="kn">from</span><span class="w"> </span><span class="nn">langchain.chains.conversation.base</span><span class="w"> </span><span class="kn">import</span> <span class="n">ConversationChain</span>
</span><span id="__span-7-2"><a id="__codelineno-7-2" name="__codelineno-7-2" href="#__codelineno-7-2"></a><span class="kn">from</span><span class="w"> </span><span class="nn">langchain.memory</span><span class="w"> </span><span class="kn">import</span> <span class="n">ConversationBufferMemory</span>
</span><span id="__span-7-3"><a id="__codelineno-7-3" name="__codelineno-7-3" href="#__codelineno-7-3"></a><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_openai</span><span class="w"> </span><span class="kn">import</span> <span class="n">ChatOpenAI</span>
</span></code></pre></div>
<p>Usaremos GPT-4 y el modelo de chat de OpenAI:</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-8-1"><a id="__codelineno-8-1" name="__codelineno-8-1" href="#__codelineno-8-1"></a><span class="n">chat_model</span> <span class="o">=</span> <span class="n">ChatOpenAI</span><span class="p">(</span><span class="n">model_name</span><span class="o">=</span><span class="s2">&quot;gpt-4&quot;</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">streaming</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></code></pre></div>
<p>Para instanciar una cadena de conversación de la clase <code>ConversationChain</code>, necesitamos instanciar primero el objeto de clase <code>Memory</code> que nos servirá para administrar la memoria de las interacciones en la conversación. Lo hacemos de la siguiente manera:</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-9-1"><a id="__codelineno-9-1" name="__codelineno-9-1" href="#__codelineno-9-1"></a><span class="n">memory</span> <span class="o">=</span> <span class="n">ConversationBufferMemory</span><span class="p">()</span> <span class="c1"># Es una instancia de la clase que contiene los controles de la memoria</span>
</span></code></pre></div>
<p>Así, la cadena de conversación la instanciaremos como:</p>
<p><div class="language-python highlight"><pre><span></span><code><span id="__span-10-1"><a id="__codelineno-10-1" name="__codelineno-10-1" href="#__codelineno-10-1"></a><span class="n">chain</span> <span class="o">=</span> <span class="n">ConversationChain</span><span class="p">(</span><span class="n">llm</span><span class="o">=</span><span class="n">chat_model</span><span class="p">,</span> <span class="n">memory</span><span class="o">=</span><span class="n">memory</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></code></pre></div>
Ahora está todo listo para que comencemos nuestra conversación:</p>
<div class="tabbed-set tabbed-alternate" data-tabs="2:2"><input checked="checked" id="__tabbed_2_1" name="__tabbed_2" type="radio" /><input id="__tabbed_2_2" name="__tabbed_2" type="radio" /><div class="tabbed-labels"><label for="__tabbed_2_1">Código</label><label for="__tabbed_2_2">Salida</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<div class="language-python highlight"><pre><span></span><code><span id="__span-11-1"><a id="__codelineno-11-1" name="__codelineno-11-1" href="#__codelineno-11-1"></a><span class="n">chain</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="s2">&quot;Hola, mi nombre es Juan, ¿cómo estás?&quot;</span><span class="p">)</span>
</span></code></pre></div>
</div>
<div class="tabbed-block">
<div class="language-bash highlight"><pre><span></span><code><span id="__span-12-1"><a id="__codelineno-12-1" name="__codelineno-12-1" href="#__codelineno-12-1"></a>&gt;<span class="w"> </span>Entering<span class="w"> </span>new<span class="w"> </span>ConversationChain<span class="w"> </span>chain...
</span><span id="__span-12-2"><a id="__codelineno-12-2" name="__codelineno-12-2" href="#__codelineno-12-2"></a>Prompt<span class="w"> </span>after<span class="w"> </span>formatting:
</span><span id="__span-12-3"><a id="__codelineno-12-3" name="__codelineno-12-3" href="#__codelineno-12-3"></a>The<span class="w"> </span>following<span class="w"> </span>is<span class="w"> </span>a<span class="w"> </span>friendly<span class="w"> </span>conversation<span class="w"> </span>between<span class="w"> </span>a<span class="w"> </span>human<span class="w"> </span>and<span class="w"> </span>an<span class="w"> </span>AI.<span class="w"> </span>The<span class="w"> </span>AI<span class="w"> </span>is<span class="w"> </span>talkative<span class="w"> </span>and<span class="w"> </span>provides<span class="w"> </span>lots<span class="w"> </span>of<span class="w"> </span>specific<span class="w"> </span>details<span class="w"> </span>from<span class="w"> </span>its<span class="w"> </span>context.<span class="w"> </span>If<span class="w"> </span>the<span class="w"> </span>AI<span class="w"> </span>does<span class="w"> </span>not<span class="w"> </span>know<span class="w"> </span>the<span class="w"> </span>answer<span class="w"> </span>to<span class="w"> </span>a<span class="w"> </span>question,<span class="w"> </span>it<span class="w"> </span>truthfully<span class="w"> </span>says<span class="w"> </span>it<span class="w"> </span>does<span class="w"> </span>not<span class="w"> </span>know.
</span><span id="__span-12-4"><a id="__codelineno-12-4" name="__codelineno-12-4" href="#__codelineno-12-4"></a>
</span><span id="__span-12-5"><a id="__codelineno-12-5" name="__codelineno-12-5" href="#__codelineno-12-5"></a>Current<span class="w"> </span>conversation:
</span><span id="__span-12-6"><a id="__codelineno-12-6" name="__codelineno-12-6" href="#__codelineno-12-6"></a>
</span><span id="__span-12-7"><a id="__codelineno-12-7" name="__codelineno-12-7" href="#__codelineno-12-7"></a>Human:<span class="w"> </span>Hola,<span class="w"> </span>mi<span class="w"> </span>nombre<span class="w"> </span>es<span class="w"> </span>Juan,<span class="w"> </span>¿cómo<span class="w"> </span>estás?
</span><span id="__span-12-8"><a id="__codelineno-12-8" name="__codelineno-12-8" href="#__codelineno-12-8"></a>AI:
</span><span id="__span-12-9"><a id="__codelineno-12-9" name="__codelineno-12-9" href="#__codelineno-12-9"></a>
</span><span id="__span-12-10"><a id="__codelineno-12-10" name="__codelineno-12-10" href="#__codelineno-12-10"></a>&gt;<span class="w"> </span>Finished<span class="w"> </span>chain.
</span><span id="__span-12-11"><a id="__codelineno-12-11" name="__codelineno-12-11" href="#__codelineno-12-11"></a>
</span><span id="__span-12-12"><a id="__codelineno-12-12" name="__codelineno-12-12" href="#__codelineno-12-12"></a><span class="w">    </span><span class="o">{</span><span class="s1">&#39;input&#39;</span>:<span class="w"> </span><span class="s1">&#39;Hola, mi nombre es Juan, ¿cómo estás?&#39;</span>,
</span><span id="__span-12-13"><a id="__codelineno-12-13" name="__codelineno-12-13" href="#__codelineno-12-13"></a><span class="s1">&#39;history&#39;</span>:<span class="w"> </span><span class="s1">&#39;&#39;</span>,
</span><span id="__span-12-14"><a id="__codelineno-12-14" name="__codelineno-12-14" href="#__codelineno-12-14"></a><span class="s1">&#39;response&#39;</span>:<span class="w"> </span><span class="s1">&#39;¡Hola, Juan! Estoy muy bien, gracias por preguntar. Soy una inteligencia artificial, así que no tengo emociones como los humanos, pero estoy aquí para ayudarte y conversar contigo. ¿En qué puedo asistirte hoy?&#39;</span><span class="o">}</span>
</span></code></pre></div>
</div>
</div>
</div>
<p>Si pregunto aluna cosa adicional, por ejeplo cuando es 2 + 2. No olvidará mi nombre:</p>
<div class="tabbed-set tabbed-alternate" data-tabs="3:2"><input checked="checked" id="__tabbed_3_1" name="__tabbed_3" type="radio" /><input id="__tabbed_3_2" name="__tabbed_3" type="radio" /><div class="tabbed-labels"><label for="__tabbed_3_1">Código</label><label for="__tabbed_3_2">Salida</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<div class="language-python highlight"><pre><span></span><code><span id="__span-13-1"><a id="__codelineno-13-1" name="__codelineno-13-1" href="#__codelineno-13-1"></a><span class="n">chain</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="s2">&quot;cuanto es 2 + 2?&quot;</span><span class="p">)</span>
</span><span id="__span-13-2"><a id="__codelineno-13-2" name="__codelineno-13-2" href="#__codelineno-13-2"></a><span class="n">chain</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="s2">&quot;cual es mi nombre?&quot;</span><span class="p">)</span>
</span></code></pre></div>
</div>
<div class="tabbed-block">
<div class="language-bash highlight"><pre><span></span><code><span id="__span-14-1"><a id="__codelineno-14-1" name="__codelineno-14-1" href="#__codelineno-14-1"></a>&gt;<span class="w"> </span>Entering<span class="w"> </span>new<span class="w"> </span>ConversationChain<span class="w"> </span>chain...
</span><span id="__span-14-2"><a id="__codelineno-14-2" name="__codelineno-14-2" href="#__codelineno-14-2"></a>Prompt<span class="w"> </span>after<span class="w"> </span>formatting:
</span><span id="__span-14-3"><a id="__codelineno-14-3" name="__codelineno-14-3" href="#__codelineno-14-3"></a>The<span class="w"> </span>following<span class="w"> </span>is<span class="w"> </span>a<span class="w"> </span>friendly<span class="w"> </span>conversation<span class="w"> </span>between<span class="w"> </span>a<span class="w"> </span>human<span class="w"> </span>and<span class="w"> </span>an<span class="w"> </span>AI.<span class="w"> </span>The<span class="w"> </span>AI<span class="w"> </span>is<span class="w"> </span>talkative<span class="w"> </span>and<span class="w"> </span>provides<span class="w"> </span>lots<span class="w"> </span>of<span class="w"> </span>specific<span class="w"> </span>details<span class="w"> </span>from<span class="w"> </span>its<span class="w"> </span>context.<span class="w"> </span>If<span class="w"> </span>the<span class="w"> </span>AI<span class="w"> </span>does<span class="w"> </span>not<span class="w"> </span>know<span class="w"> </span>the<span class="w"> </span>answer<span class="w"> </span>to<span class="w"> </span>a<span class="w"> </span>question,<span class="w"> </span>it<span class="w"> </span>truthfully<span class="w"> </span>says<span class="w"> </span>it<span class="w"> </span>does<span class="w"> </span>not<span class="w"> </span>know.
</span><span id="__span-14-4"><a id="__codelineno-14-4" name="__codelineno-14-4" href="#__codelineno-14-4"></a>
</span><span id="__span-14-5"><a id="__codelineno-14-5" name="__codelineno-14-5" href="#__codelineno-14-5"></a>Current<span class="w"> </span>conversation:
</span><span id="__span-14-6"><a id="__codelineno-14-6" name="__codelineno-14-6" href="#__codelineno-14-6"></a>Human:<span class="w"> </span>Hola,<span class="w"> </span>mi<span class="w"> </span>nombre<span class="w"> </span>es<span class="w"> </span>Juan,<span class="w"> </span>¿cómo<span class="w"> </span>estás?
</span><span id="__span-14-7"><a id="__codelineno-14-7" name="__codelineno-14-7" href="#__codelineno-14-7"></a>AI:<span class="w"> </span>¡Hola,<span class="w"> </span>Juan!<span class="w"> </span>Estoy<span class="w"> </span>muy<span class="w"> </span>bien,<span class="w"> </span>gracias<span class="w"> </span>por<span class="w"> </span>preguntar.<span class="w"> </span>Soy<span class="w"> </span>una<span class="w"> </span>inteligencia<span class="w"> </span>artificial,<span class="w"> </span>así<span class="w"> </span>que<span class="w"> </span>no<span class="w"> </span>tengo<span class="w"> </span>emociones<span class="w"> </span>como<span class="w"> </span>los<span class="w"> </span>humanos,<span class="w"> </span>pero<span class="w"> </span>estoy<span class="w"> </span>aquí<span class="w"> </span>para<span class="w"> </span>ayudarte<span class="w"> </span>y<span class="w"> </span>conversar<span class="w"> </span>contigo.<span class="w"> </span>¿En<span class="w"> </span>qué<span class="w"> </span>puedo<span class="w"> </span>asistirte<span class="w"> </span>hoy?
</span><span id="__span-14-8"><a id="__codelineno-14-8" name="__codelineno-14-8" href="#__codelineno-14-8"></a>Human:<span class="w"> </span>cuanto<span class="w"> </span>es<span class="w"> </span><span class="m">2</span><span class="w"> </span>+<span class="w"> </span><span class="m">2</span>?
</span><span id="__span-14-9"><a id="__codelineno-14-9" name="__codelineno-14-9" href="#__codelineno-14-9"></a>AI:<span class="w"> </span><span class="m">2</span><span class="w"> </span>+<span class="w"> </span><span class="m">2</span><span class="w"> </span>es<span class="w"> </span>igual<span class="w"> </span>a<span class="w"> </span><span class="m">4</span>.<span class="w"> </span>Es<span class="w"> </span>una<span class="w"> </span>de<span class="w"> </span>las<span class="w"> </span>operaciones<span class="w"> </span>matemáticas<span class="w"> </span>más<span class="w"> </span>básicas<span class="w"> </span>y<span class="w"> </span>es<span class="w"> </span>un<span class="w"> </span>buen<span class="w"> </span>ejemplo<span class="w"> </span>de<span class="w"> </span>cómo<span class="w"> </span>funcionan<span class="w"> </span>las<span class="w"> </span>sumas.<span class="w"> </span>Si<span class="w"> </span>tienes<span class="w"> </span>más<span class="w"> </span>preguntas<span class="w"> </span>de<span class="w"> </span>matemáticas<span class="w"> </span>o<span class="w"> </span>cualquier<span class="w"> </span>otro<span class="w"> </span>tema,<span class="w"> </span>estaré<span class="w"> </span>encantado<span class="w"> </span>de<span class="w"> </span>ayudarte.
</span><span id="__span-14-10"><a id="__codelineno-14-10" name="__codelineno-14-10" href="#__codelineno-14-10"></a>Human:<span class="w"> </span>¿Cuál<span class="w"> </span>es<span class="w"> </span>mi<span class="w"> </span>nombre?
</span><span id="__span-14-11"><a id="__codelineno-14-11" name="__codelineno-14-11" href="#__codelineno-14-11"></a>AI:
</span><span id="__span-14-12"><a id="__codelineno-14-12" name="__codelineno-14-12" href="#__codelineno-14-12"></a>
</span><span id="__span-14-13"><a id="__codelineno-14-13" name="__codelineno-14-13" href="#__codelineno-14-13"></a><span class="o">{</span><span class="s1">&#39;input&#39;</span>:<span class="w"> </span><span class="s1">&#39;¿Cuál es mi nombre?&#39;</span>,
</span><span id="__span-14-14"><a id="__codelineno-14-14" name="__codelineno-14-14" href="#__codelineno-14-14"></a><span class="s1">&#39;history&#39;</span>:<span class="w"> </span><span class="s1">&#39;Human: Hola, mi nombre es Juan, ¿cómo estás?\nAI: ¡Hola, Juan! Estoy muy bien, gracias por preguntar. Soy una inteligencia artificial, así que no tengo emociones como los humanos, pero estoy aquí para ayudarte y conversar contigo. ¿En qué puedo asistirte hoy?\nHuman: cuanto es 2 + 2?\nAI: 2 + 2 es igual a 4. Es una de las operaciones matemáticas más básicas y es un buen ejemplo de cómo funcionan las sumas. Si tienes más preguntas de matemáticas o cualquier otro tema, estaré encantado de ayudarte.&#39;</span>,
</span><span id="__span-14-15"><a id="__codelineno-14-15" name="__codelineno-14-15" href="#__codelineno-14-15"></a><span class="s1">&#39;response&#39;</span>:<span class="w"> </span><span class="s1">&#39;Tu nombre es Juan. Me lo dijiste al comienzo de nuestra conversación. Si tienes más preguntas o necesitas ayuda con algo más, no dudes en decírmelo.&#39;</span><span class="o">}</span>
</span></code></pre></div>
</div>
</div>
</div>
<p>La IA responde que <em>Tu nombre es Juan. Me lo dijiste al comienzo de nuestra conversación</em>. Lo cual no era posible en el módulo 1 cuando invocábamos el modelo sin memoria.</p>
<h2 id="la-ventana-de-contexto">La ventana de contexto</h2>
<p>La memoria es un recurso costoso, pues los modelos tienen una capacidad limitada para guardar el contexto de las conversaciones. Esta característica es llamada <strong>la ventana de contexto</strong>. A medida que los modelos se han vuelto más avanzados, las ventanas de contexto ofrecidas son cada vez más grandes. </p>
<ul>
<li>GPT-3: 4096 tokens</li>
<li>GPT-3.5-turbo: 4096 tokens</li>
<li>GPT-4: 8192 tokens (con una versión extendida de 32768 tokens)</li>
</ul>
<p>Sin embargo, debemos tener en cuenta que el modelo siempre tratará de relacionar sus respuestas con el contexto de la conversación. Esto puede ser beneficioso, pero al mismo tiempo, puede introducir ruido a la conversación, y el modelo puede generar respuestas menos precisas si está distraído en un contexto muy amplio. Por este motivo, resulta conveniente tener la posibilidad de limitar la memoria que queremos dar a las aplicaciones asistidas por IA. Veremos ahora algunos tipos de memoria y sus usos.</p>
<h2 id="tipos-de-memoria-en-langchain">Tipos de Memoria en LangChain</h2>
<p>LangChain ofrece varios tipos de memoria que se pueden utilizar para gestionar el contexto en las aplicaciones de inteligencia artificial</p>
<p>crearemos una pequena encapsulación para ilistrar el uso de las dos clases principales:</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-15-1"><a id="__codelineno-15-1" name="__codelineno-15-1" href="#__codelineno-15-1"></a><span class="k">class</span><span class="w"> </span><span class="nc">ChatBot</span><span class="p">:</span>
</span><span id="__span-15-2"><a id="__codelineno-15-2" name="__codelineno-15-2" href="#__codelineno-15-2"></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">memory</span><span class="p">):</span>
</span><span id="__span-15-3"><a id="__codelineno-15-3" name="__codelineno-15-3" href="#__codelineno-15-3"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">chat_model</span>  <span class="o">=</span>  <span class="n">ChatOpenAI</span><span class="p">(</span><span class="n">model_name</span><span class="o">=</span> <span class="s2">&quot;gpt-4o&quot;</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">streaming</span><span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
</span><span id="__span-15-4"><a id="__codelineno-15-4" name="__codelineno-15-4" href="#__codelineno-15-4"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">memory</span> <span class="o">=</span> <span class="n">memory</span>
</span><span id="__span-15-5"><a id="__codelineno-15-5" name="__codelineno-15-5" href="#__codelineno-15-5"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">chain</span> <span class="o">=</span> <span class="n">ConversationChain</span><span class="p">(</span><span class="n">llm</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">chat_model</span><span class="p">,</span> <span class="n">memory</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">memory</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="__span-15-6"><a id="__codelineno-15-6" name="__codelineno-15-6" href="#__codelineno-15-6"></a>
</span><span id="__span-15-7"><a id="__codelineno-15-7" name="__codelineno-15-7" href="#__codelineno-15-7"></a><span class="c1"># metodo para responder</span>
</span><span id="__span-15-8"><a id="__codelineno-15-8" name="__codelineno-15-8" href="#__codelineno-15-8"></a>    <span class="k">def</span><span class="w"> </span><span class="nf">pregunta</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pregunta</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>        
</span><span id="__span-15-9"><a id="__codelineno-15-9" name="__codelineno-15-9" href="#__codelineno-15-9"></a>        <span class="n">response</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">chain</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="nb">input</span> <span class="o">=</span> <span class="n">pregunta</span><span class="p">)</span>
</span><span id="__span-15-10"><a id="__codelineno-15-10" name="__codelineno-15-10" href="#__codelineno-15-10"></a>        <span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
</span><span id="__span-15-11"><a id="__codelineno-15-11" name="__codelineno-15-11" href="#__codelineno-15-11"></a>        <span class="k">return</span> <span class="mi">0</span>
</span></code></pre></div>
<h2 id="tipos-de-memoria-en-langchain_1">Tipos de Memoria en LangChain</h2>
<p><strong>ConversationBufferMemory</strong>: 
   - <strong>Uso</strong>: Esta memoria almacena todo el historial de la conversación sin ningún límite. Es útil cuando se desea mantener un registro completo de todas las interacciones previas, como lo hicimos en el ejemplo anterior.</p>
<p>Por ejemplo, un chat con memoria ilimitada se crearía de la siguiente manera:</p>
<p><div class="language-python highlight"><pre><span></span><code><span id="__span-16-1"><a id="__codelineno-16-1" name="__codelineno-16-1" href="#__codelineno-16-1"></a><span class="kn">from</span><span class="w"> </span><span class="nn">langchain.memory</span><span class="w"> </span><span class="kn">import</span> <span class="n">ConversationBufferMemory</span>
</span><span id="__span-16-2"><a id="__codelineno-16-2" name="__codelineno-16-2" href="#__codelineno-16-2"></a>
</span><span id="__span-16-3"><a id="__codelineno-16-3" name="__codelineno-16-3" href="#__codelineno-16-3"></a><span class="n">mucha_memoria</span> <span class="o">=</span> <span class="n">ConversationBufferMemory</span><span class="p">()</span>
</span><span id="__span-16-4"><a id="__codelineno-16-4" name="__codelineno-16-4" href="#__codelineno-16-4"></a><span class="n">genioBot</span> <span class="o">=</span> <span class="n">ChatBot</span><span class="p">(</span><span class="n">mucha_memoria</span><span class="p">)</span>
</span></code></pre></div>
<strong>ConversationBufferWindowMemory</strong>:
   - <strong>Uso</strong>: Similar a <code>ConversationBufferMemory</code>, pero con un parámetro llamado <code>window_size</code> que permite recordar solo un número fijo de interacciones recientes.</p>
<p>Creamos un bot muy inteligente pero con memoria limitada usando <code>ConversationBufferWindowMemory</code>:</p>
<div class="tabbed-set tabbed-alternate" data-tabs="4:2"><input checked="checked" id="__tabbed_4_1" name="__tabbed_4" type="radio" /><input id="__tabbed_4_2" name="__tabbed_4" type="radio" /><div class="tabbed-labels"><label for="__tabbed_4_1">Código</label><label for="__tabbed_4_2">Salida</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<div class="language-python highlight"><pre><span></span><code><span id="__span-17-1"><a id="__codelineno-17-1" name="__codelineno-17-1" href="#__codelineno-17-1"></a><span class="kn">from</span><span class="w"> </span><span class="nn">langchain.memory</span><span class="w"> </span><span class="kn">import</span> <span class="n">ConversationBufferWindowMemory</span>
</span><span id="__span-17-2"><a id="__codelineno-17-2" name="__codelineno-17-2" href="#__codelineno-17-2"></a>
</span><span id="__span-17-3"><a id="__codelineno-17-3" name="__codelineno-17-3" href="#__codelineno-17-3"></a><span class="c1"># Memoria con ventana de 1</span>
</span><span id="__span-17-4"><a id="__codelineno-17-4" name="__codelineno-17-4" href="#__codelineno-17-4"></a><span class="n">poca_memoria</span> <span class="o">=</span> <span class="n">ConversationBufferWindowMemory</span><span class="p">(</span><span class="n">k</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span><span id="__span-17-5"><a id="__codelineno-17-5" name="__codelineno-17-5" href="#__codelineno-17-5"></a>
</span><span id="__span-17-6"><a id="__codelineno-17-6" name="__codelineno-17-6" href="#__codelineno-17-6"></a><span class="c1"># Instanciamos un bot con poca memoria</span>
</span><span id="__span-17-7"><a id="__codelineno-17-7" name="__codelineno-17-7" href="#__codelineno-17-7"></a><span class="n">olvidoBot</span> <span class="o">=</span> <span class="n">ChatBot</span><span class="p">(</span><span class="n">poca_memoria</span><span class="p">)</span>
</span><span id="__span-17-8"><a id="__codelineno-17-8" name="__codelineno-17-8" href="#__codelineno-17-8"></a><span class="n">olvidoBot</span><span class="o">.</span><span class="n">pregunta</span><span class="p">(</span><span class="s2">&quot;Hola, mi nombre es Juan, ¿cómo estás?&quot;</span><span class="p">)</span>
</span><span id="__span-17-9"><a id="__codelineno-17-9" name="__codelineno-17-9" href="#__codelineno-17-9"></a><span class="n">olvidoBot</span><span class="o">.</span><span class="n">pregunta</span><span class="p">(</span><span class="s2">&quot;¿Cuánto es 2 + 5?&quot;</span><span class="p">)</span>
</span><span id="__span-17-10"><a id="__codelineno-17-10" name="__codelineno-17-10" href="#__codelineno-17-10"></a><span class="n">olvidoBot</span><span class="o">.</span><span class="n">pregunta</span><span class="p">(</span><span class="s2">&quot;¿Cuál es mi nombre?&quot;</span><span class="p">)</span> <span class="c1"># No sabe el nombre</span>
</span></code></pre></div>
</div>
<div class="tabbed-block">
<div class="language-bash highlight"><pre><span></span><code><span id="__span-18-1"><a id="__codelineno-18-1" name="__codelineno-18-1" href="#__codelineno-18-1"></a>&gt;<span class="w"> </span>Entering<span class="w"> </span>new<span class="w"> </span>ChatBot<span class="w"> </span>session...
</span><span id="__span-18-2"><a id="__codelineno-18-2" name="__codelineno-18-2" href="#__codelineno-18-2"></a>Human:<span class="w"> </span>Hola,<span class="w"> </span>mi<span class="w"> </span>nombre<span class="w"> </span>es<span class="w"> </span>Juan,<span class="w"> </span>¿cómo<span class="w"> </span>estás?
</span><span id="__span-18-3"><a id="__codelineno-18-3" name="__codelineno-18-3" href="#__codelineno-18-3"></a>AI:<span class="w"> </span>¡Hola,<span class="w"> </span>Juan!<span class="w"> </span>Estoy<span class="w"> </span>muy<span class="w"> </span>bien,<span class="w"> </span>gracias<span class="w"> </span>por<span class="w"> </span>preguntar.<span class="w"> </span>Soy<span class="w"> </span>una<span class="w"> </span>inteligencia<span class="w"> </span>artificial<span class="w"> </span>diseñada<span class="w"> </span>para<span class="w"> </span>ayudarte<span class="w"> </span>con<span class="w"> </span>información<span class="w"> </span>y<span class="w"> </span>responder<span class="w"> </span>a<span class="w"> </span>tus<span class="w"> </span>preguntas.<span class="w"> </span>¿En<span class="w"> </span>qué<span class="w"> </span>puedo<span class="w"> </span>asistirte<span class="w"> </span>hoy?
</span><span id="__span-18-4"><a id="__codelineno-18-4" name="__codelineno-18-4" href="#__codelineno-18-4"></a>
</span><span id="__span-18-5"><a id="__codelineno-18-5" name="__codelineno-18-5" href="#__codelineno-18-5"></a>Human:<span class="w"> </span>¿Cuánto<span class="w"> </span>es<span class="w"> </span><span class="m">2</span><span class="w"> </span>+<span class="w"> </span><span class="m">5</span>?
</span><span id="__span-18-6"><a id="__codelineno-18-6" name="__codelineno-18-6" href="#__codelineno-18-6"></a>AI:<span class="w"> </span><span class="m">2</span><span class="w"> </span>+<span class="w"> </span><span class="m">5</span><span class="w"> </span>es<span class="w"> </span>igual<span class="w"> </span>a<span class="w"> </span><span class="m">7</span>.<span class="w"> </span>Si<span class="w"> </span>tienes<span class="w"> </span>más<span class="w"> </span>preguntas<span class="w"> </span>de<span class="w"> </span>matemáticas<span class="w"> </span>o<span class="w"> </span>cualquier<span class="w"> </span>otra<span class="w"> </span>cosa<span class="w"> </span>en<span class="w"> </span>mente,<span class="w"> </span>¡estaré<span class="w"> </span>encantado<span class="w"> </span>de<span class="w"> </span>ayudarte!
</span><span id="__span-18-7"><a id="__codelineno-18-7" name="__codelineno-18-7" href="#__codelineno-18-7"></a>
</span><span id="__span-18-8"><a id="__codelineno-18-8" name="__codelineno-18-8" href="#__codelineno-18-8"></a>Human:<span class="w"> </span>¿Cuál<span class="w"> </span>es<span class="w"> </span>mi<span class="w"> </span>nombre?
</span><span id="__span-18-9"><a id="__codelineno-18-9" name="__codelineno-18-9" href="#__codelineno-18-9"></a><span class="hll">AI:<span class="w"> </span>Lo<span class="w"> </span>siento,<span class="w"> </span>no<span class="w"> </span>tengo<span class="w"> </span>la<span class="w"> </span>capacidad<span class="w"> </span>de<span class="w"> </span>saber<span class="w"> </span>tu<span class="w"> </span>nombre<span class="w"> </span>a<span class="w"> </span>menos<span class="w"> </span>que<span class="w"> </span>me<span class="w"> </span>lo<span class="w"> </span>hayas<span class="w"> </span>dicho<span class="w"> </span>antes<span class="w"> </span>en<span class="w"> </span>esta<span class="w"> </span>conversación.<span class="w"> </span>Si<span class="w"> </span>quieres,<span class="w"> </span>puedes<span class="w"> </span>decírmelo<span class="w"> </span>ahora<span class="w"> </span>y<span class="w"> </span>lo<span class="w"> </span>recordaré<span class="w"> </span>para<span class="w"> </span>el<span class="w"> </span>resto<span class="w"> </span>de<span class="w"> </span>nuestra<span class="w"> </span>charla.
</span></span></code></pre></div>
</div>
</div>
</div>
<p><code>olvidoBot</code> solo recordará una interacción a la vez debido a <code>window_size=1</code>. Cuando preguntamos "¿Cuál es mi nombre?", ya no recuerda la interacción donde le dijimos nuestro nombre, por lo que no puede recordarlo.</p>
<p><code>ConversationSummaryMemory</code> almacena un resumen de la conversación, ideal para interacciones muy largas donde no es necesario retener todos los detalles. <!--  NOta terminar esta parte, poner un ejemplo aquí--></p>
<div class="admonition warning">
<p class="admonition-title">Para tener en cuenta</p>
<p>Las guías más recientes recomiendan utilizar la función <code>trim_messages</code>, que proporciona una forma flexible de gestionar el historial de la conversación. Las funcionalidades aquí expuestas están siendo migradas a la plataforma de LangGraph, por lo que están fuera del alcance de este curso. Verás el mensaje <code>LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/</code>.</p>
</div>
<h1 id="entendiendo-la-funcion-trim_messages">Entendiendo la Función <code>trim_messages</code></h1>
<p>La función <code>trim_messages</code> es una utilidad en LangChain diseñada para reducir el tamaño de un historial de chat a un número específico de tokens o mensajes, asegurando que el historial recortado siga siendo válido para los modelos de chat. Un historial de chat válido típicamente:</p>
<ul>
<li><strong>Comienza con:</strong></li>
<li>Un <code>HumanMessage</code>, o</li>
<li>
<p>Un <code>SystemMessage</code> seguido de un <code>HumanMessage</code>.</p>
</li>
<li>
<p><strong>Termina con:</strong></p>
</li>
<li>Un <code>HumanMessage</code>, o</li>
<li>Un <code>ToolMessage</code> (común en conversaciones basadas en agentes).</li>
</ul>
<p>Al recortar mensajes más antiguos o menos relevantes, <code>trim_messages</code> ayuda a enfocar el modelo en el contexto reciente y pertinente, evitanto información que que pueda distraer al modelo (no simepre un contexto grande es mejor).</p>
<p>La función <code>trim_messages</code> opera tomando varios parámetros para controlar cómo se realiza el recorte:</p>
<table>
<thead>
<tr>
<th>Parámetro</th>
<th>Descripción</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>messages</code></td>
<td>La secuencia de mensajes (por ejemplo, <code>HumanMessage</code>, <code>AIMessage</code>, <code>SystemMessage</code>) a recortar.</td>
</tr>
<tr>
<td><code>max_tokens</code></td>
<td>El número máximo de tokens que deben tener los mensajes recortados.</td>
</tr>
<tr>
<td><code>strategy</code></td>
<td>La estrategia de recorte: "first" (mantiene los primeros mensajes) o "last" (mantiene los más recientes, a menudo preferido para conversaciones).</td>
</tr>
<tr>
<td><code>token_counter</code></td>
<td>Una función o LLM utilizado para contar tokens en los mensajes, como el método de conteo de tokens incorporado de un LLM.</td>
</tr>
<tr>
<td><code>include_system</code></td>
<td>Un booleano (por defecto: <code>False</code>) que especifica si se debe mantener el <code>SystemMessage</code> al principio si está presente.</td>
</tr>
<tr>
<td><code>allow_partial</code></td>
<td>Un booleano (por defecto: <code>False</code>) que permite dividir un mensaje si solo parte de él puede incluirse para cumplir con el límite de tokens.</td>
</tr>
</tbody>
</table>
<p>La función devuelve una lista de mensajes recortados que se ajustan a los límites especificados mientras mantienen la coherencia del contexto.</p>
<p>Consideremos un ejemplo práctico donde tenemos un chatbot que utiliza una cadena para procesar consultas de usuario. El chatbot ha estado funcionando durante varias interacciones, y el historial de chat se ha alargado. Necesitamos recortar el historial para ajustarlo a un límite de 100 tokens, manteniendo los mensajes más recientes y el <code>SystemMessage</code>.</p>
<p>Primero, definamos un historial de chat de ejemplo:</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-19-1"><a id="__codelineno-19-1" name="__codelineno-19-1" href="#__codelineno-19-1"></a><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.messages</span><span class="w"> </span><span class="kn">import</span> <span class="n">AIMessage</span><span class="p">,</span> <span class="n">HumanMessage</span><span class="p">,</span> <span class="n">SystemMessage</span>
</span><span id="__span-19-2"><a id="__codelineno-19-2" name="__codelineno-19-2" href="#__codelineno-19-2"></a>
</span><span id="__span-19-3"><a id="__codelineno-19-3" name="__codelineno-19-3" href="#__codelineno-19-3"></a><span class="n">chat_history</span> <span class="o">=</span> <span class="p">[</span>
</span><span id="__span-19-4"><a id="__codelineno-19-4" name="__codelineno-19-4" href="#__codelineno-19-4"></a>    <span class="n">SystemMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">&quot;You are a helpful assistant.&quot;</span><span class="p">),</span>
</span><span id="__span-19-5"><a id="__codelineno-19-5" name="__codelineno-19-5" href="#__codelineno-19-5"></a>    <span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">&quot;Hello, my name is Juan how are you?&quot;</span><span class="p">),</span>
</span><span id="__span-19-6"><a id="__codelineno-19-6" name="__codelineno-19-6" href="#__codelineno-19-6"></a>    <span class="n">AIMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">&quot;I&#39;m doing well, thank you. How can I help you today?&quot;</span><span class="p">),</span>
</span><span id="__span-19-7"><a id="__codelineno-19-7" name="__codelineno-19-7" href="#__codelineno-19-7"></a>    <span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">&quot;Can you tell me about the weather today?&quot;</span><span class="p">),</span>
</span><span id="__span-19-8"><a id="__codelineno-19-8" name="__codelineno-19-8" href="#__codelineno-19-8"></a>    <span class="n">AIMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">&quot;Sure, let me check that for you. [checks weather] It&#39;s sunny with a high of 75 degrees.&quot;</span><span class="p">),</span>
</span><span id="__span-19-9"><a id="__codelineno-19-9" name="__codelineno-19-9" href="#__codelineno-19-9"></a>    <span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">&quot;What&#39;s the capital of France?&quot;</span><span class="p">),</span>
</span><span id="__span-19-10"><a id="__codelineno-19-10" name="__codelineno-19-10" href="#__codelineno-19-10"></a>    <span class="n">AIMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">&quot;The capital of France is Paris.&quot;</span><span class="p">),</span>
</span><span id="__span-19-11"><a id="__codelineno-19-11" name="__codelineno-19-11" href="#__codelineno-19-11"></a><span class="p">]</span>
</span></code></pre></div>
<p>Ahora, usaremos <code>trim_messages</code> para recortar este historial a 100 tokens, manteniendo los mensajes más recientes e incluyendo el <code>SystemMessage</code>:</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-20-1"><a id="__codelineno-20-1" name="__codelineno-20-1" href="#__codelineno-20-1"></a><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.messages.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">trim_messages</span>
</span><span id="__span-20-2"><a id="__codelineno-20-2" name="__codelineno-20-2" href="#__codelineno-20-2"></a><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_openai</span><span class="w"> </span><span class="kn">import</span> <span class="n">OpenAI</span>
</span><span id="__span-20-3"><a id="__codelineno-20-3" name="__codelineno-20-3" href="#__codelineno-20-3"></a>
</span><span id="__span-20-4"><a id="__codelineno-20-4" name="__codelineno-20-4" href="#__codelineno-20-4"></a><span class="c1"># Inicializar un LLM para el conteo de tokens</span>
</span><span id="__span-20-5"><a id="__codelineno-20-5" name="__codelineno-20-5" href="#__codelineno-20-5"></a><span class="n">llm</span> <span class="o">=</span> <span class="n">OpenAI</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-3.5-turbo-1106&quot;</span><span class="p">)</span>
</span><span id="__span-20-6"><a id="__codelineno-20-6" name="__codelineno-20-6" href="#__codelineno-20-6"></a>
</span><span id="__span-20-7"><a id="__codelineno-20-7" name="__codelineno-20-7" href="#__codelineno-20-7"></a><span class="c1"># Recortar los mensajes</span>
</span><span id="__span-20-8"><a id="__codelineno-20-8" name="__codelineno-20-8" href="#__codelineno-20-8"></a><span class="n">trimmed_history</span> <span class="o">=</span> <span class="n">trim_messages</span><span class="p">(</span>
</span><span id="__span-20-9"><a id="__codelineno-20-9" name="__codelineno-20-9" href="#__codelineno-20-9"></a>    <span class="n">messages</span><span class="o">=</span><span class="n">chat_history</span><span class="p">,</span>
</span><span id="__span-20-10"><a id="__codelineno-20-10" name="__codelineno-20-10" href="#__codelineno-20-10"></a>    <span class="n">max_tokens</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
</span><span id="__span-20-11"><a id="__codelineno-20-11" name="__codelineno-20-11" href="#__codelineno-20-11"></a>    <span class="n">strategy</span><span class="o">=</span><span class="s2">&quot;last&quot;</span><span class="p">,</span>
</span><span id="__span-20-12"><a id="__codelineno-20-12" name="__codelineno-20-12" href="#__codelineno-20-12"></a>    <span class="n">token_counter</span><span class="o">=</span><span class="n">llm</span><span class="o">.</span><span class="n">get_num_tokens_from_messages</span><span class="p">,</span>
</span><span id="__span-20-13"><a id="__codelineno-20-13" name="__codelineno-20-13" href="#__codelineno-20-13"></a>    <span class="n">include_system</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="c1"># para mantener el SystemMessage en la memoria</span>
</span><span id="__span-20-14"><a id="__codelineno-20-14" name="__codelineno-20-14" href="#__codelineno-20-14"></a><span class="p">)</span>
</span></code></pre></div>
<p>Si el conteo total de tokens del historial original excede los 100, el historial recortado podría verse así:</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-21-1"><a id="__codelineno-21-1" name="__codelineno-21-1" href="#__codelineno-21-1"></a><span class="p">[</span>
</span><span id="__span-21-2"><a id="__codelineno-21-2" name="__codelineno-21-2" href="#__codelineno-21-2"></a>    <span class="n">SystemMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">&quot;You are a helpful assistant.&quot;</span><span class="p">),</span>
</span><span id="__span-21-3"><a id="__codelineno-21-3" name="__codelineno-21-3" href="#__codelineno-21-3"></a>    <span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">&quot;What&#39;s the capital of France?&quot;</span><span class="p">),</span>
</span><span id="__span-21-4"><a id="__codelineno-21-4" name="__codelineno-21-4" href="#__codelineno-21-4"></a>    <span class="n">AIMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">&quot;The capital of France is Paris.&quot;</span><span class="p">),</span>
</span><span id="__span-21-5"><a id="__codelineno-21-5" name="__codelineno-21-5" href="#__codelineno-21-5"></a><span class="p">]</span>
</span></code></pre></div>
<p>Esto mantiene la conversación enfocada y dentro de la ventana de contexto especificada, pero no recordará nuestro nombre. A continuación, se presenta el código completo:</p>
<div class="tabbed-set tabbed-alternate" data-tabs="5:2"><input checked="checked" id="__tabbed_5_1" name="__tabbed_5" type="radio" /><input id="__tabbed_5_2" name="__tabbed_5" type="radio" /><div class="tabbed-labels"><label for="__tabbed_5_1">Código</label><label for="__tabbed_5_2">Salida</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<div class="language-python highlight"><pre><span></span><code><span id="__span-22-1"><a id="__codelineno-22-1" name="__codelineno-22-1" href="#__codelineno-22-1"></a><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.messages</span><span class="w"> </span><span class="kn">import</span> <span class="n">AIMessage</span><span class="p">,</span> <span class="n">HumanMessage</span><span class="p">,</span> <span class="n">SystemMessage</span>
</span><span id="__span-22-2"><a id="__codelineno-22-2" name="__codelineno-22-2" href="#__codelineno-22-2"></a><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.messages.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">trim_messages</span>
</span><span id="__span-22-3"><a id="__codelineno-22-3" name="__codelineno-22-3" href="#__codelineno-22-3"></a><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.runnables</span><span class="w"> </span><span class="kn">import</span> <span class="n">RunnableLambda</span>
</span><span id="__span-22-4"><a id="__codelineno-22-4" name="__codelineno-22-4" href="#__codelineno-22-4"></a><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.prompts</span><span class="w"> </span><span class="kn">import</span> <span class="n">ChatPromptTemplate</span>
</span><span id="__span-22-5"><a id="__codelineno-22-5" name="__codelineno-22-5" href="#__codelineno-22-5"></a><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_openai</span><span class="w"> </span><span class="kn">import</span> <span class="n">ChatOpenAI</span>
</span><span id="__span-22-6"><a id="__codelineno-22-6" name="__codelineno-22-6" href="#__codelineno-22-6"></a>
</span><span id="__span-22-7"><a id="__codelineno-22-7" name="__codelineno-22-7" href="#__codelineno-22-7"></a><span class="c1"># Inicializar el LLM</span>
</span><span id="__span-22-8"><a id="__codelineno-22-8" name="__codelineno-22-8" href="#__codelineno-22-8"></a><span class="c1"># Reemplazar &#39;your-openai-api-key&#39; con tu clave API real de OpenAI</span>
</span><span id="__span-22-9"><a id="__codelineno-22-9" name="__codelineno-22-9" href="#__codelineno-22-9"></a><span class="n">llm</span> <span class="o">=</span> <span class="n">ChatOpenAI</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-3.5-turbo-1106&quot;</span><span class="p">)</span>
</span><span id="__span-22-10"><a id="__codelineno-22-10" name="__codelineno-22-10" href="#__codelineno-22-10"></a>
</span><span id="__span-22-11"><a id="__codelineno-22-11" name="__codelineno-22-11" href="#__codelineno-22-11"></a><span class="c1"># Definir la plantilla de prompt</span>
</span><span id="__span-22-12"><a id="__codelineno-22-12" name="__codelineno-22-12" href="#__codelineno-22-12"></a><span class="c1"># La plantilla incluye un mensaje de sistema, un marcador de posición para el historial de chat y la entrada del usuario</span>
</span><span id="__span-22-13"><a id="__codelineno-22-13" name="__codelineno-22-13" href="#__codelineno-22-13"></a><span class="n">prompt</span> <span class="o">=</span> <span class="n">ChatPromptTemplate</span><span class="o">.</span><span class="n">from_messages</span><span class="p">([</span>
</span><span id="__span-22-14"><a id="__codelineno-22-14" name="__codelineno-22-14" href="#__codelineno-22-14"></a>    <span class="p">(</span><span class="s2">&quot;system&quot;</span><span class="p">,</span> <span class="s2">&quot;You are a helpful assistant.&quot;</span><span class="p">),</span>
</span><span id="__span-22-15"><a id="__codelineno-22-15" name="__codelineno-22-15" href="#__codelineno-22-15"></a>    <span class="p">(</span><span class="s2">&quot;placeholder&quot;</span><span class="p">,</span> <span class="s2">&quot;</span><span class="si">{chat_history}</span><span class="s2">&quot;</span><span class="p">),</span>  <span class="c1"># Importante: este es el marcador de posición para el historial de chat</span>
</span><span id="__span-22-16"><a id="__codelineno-22-16" name="__codelineno-22-16" href="#__codelineno-22-16"></a>    <span class="p">(</span><span class="s2">&quot;human&quot;</span><span class="p">,</span> <span class="s2">&quot;</span><span class="si">{input}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="__span-22-17"><a id="__codelineno-22-17" name="__codelineno-22-17" href="#__codelineno-22-17"></a><span class="p">])</span>
</span><span id="__span-22-18"><a id="__codelineno-22-18" name="__codelineno-22-18" href="#__codelineno-22-18"></a>
</span><span id="__span-22-19"><a id="__codelineno-22-19" name="__codelineno-22-19" href="#__codelineno-22-19"></a><span class="c1"># Definir una función para recortar mensajes</span>
</span><span id="__span-22-20"><a id="__codelineno-22-20" name="__codelineno-22-20" href="#__codelineno-22-20"></a><span class="c1"># Esta función recorta el historial de chat para ajustarse a un límite de tokens especificado</span>
</span><span id="__span-22-21"><a id="__codelineno-22-21" name="__codelineno-22-21" href="#__codelineno-22-21"></a><span class="k">def</span><span class="w"> </span><span class="nf">trim_chat_history</span><span class="p">(</span><span class="n">messages</span><span class="p">):</span>
</span><span id="__span-22-22"><a id="__codelineno-22-22" name="__codelineno-22-22" href="#__codelineno-22-22"></a>    <span class="k">return</span> <span class="n">trim_messages</span><span class="p">(</span>
</span><span id="__span-22-23"><a id="__codelineno-22-23" name="__codelineno-22-23" href="#__codelineno-22-23"></a>        <span class="n">messages</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span>
</span><span id="__span-22-24"><a id="__codelineno-22-24" name="__codelineno-22-24" href="#__codelineno-22-24"></a>        <span class="n">max_tokens</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>  <span class="c1"># Limitar a 100 tokens para ajustarse al contexto del modelo</span>
</span><span id="__span-22-25"><a id="__codelineno-22-25" name="__codelineno-22-25" href="#__codelineno-22-25"></a>        <span class="n">strategy</span><span class="o">=</span><span class="s2">&quot;last&quot;</span><span class="p">,</span>  <span class="c1"># Conservar los mensajes más recientes</span>
</span><span id="__span-22-26"><a id="__codelineno-22-26" name="__codelineno-22-26" href="#__codelineno-22-26"></a>        <span class="n">token_counter</span><span class="o">=</span><span class="n">llm</span><span class="o">.</span><span class="n">get_num_tokens_from_messages</span><span class="p">,</span>  <span class="c1"># Usar el contador de tokens del LLM</span>
</span><span id="__span-22-27"><a id="__codelineno-22-27" name="__codelineno-22-27" href="#__codelineno-22-27"></a>        <span class="n">include_system</span><span class="o">=</span><span class="kc">True</span>  <span class="c1"># Preservar el mensaje del sistema</span>
</span><span id="__span-22-28"><a id="__codelineno-22-28" name="__codelineno-22-28" href="#__codelineno-22-28"></a>    <span class="p">)</span>
</span></code></pre></div>
</div>
<div class="tabbed-block">
<div class="language-bash highlight"><pre><span></span><code><span id="__span-23-1"><a id="__codelineno-23-1" name="__codelineno-23-1" href="#__codelineno-23-1"></a>LLM<span class="w"> </span>Response:<span class="w"> </span>You<span class="w"> </span>haven<span class="err">&#39;</span>t<span class="w"> </span>provided<span class="w"> </span>your<span class="w"> </span>name.<span class="w"> </span>Can<span class="w"> </span>you<span class="w"> </span>please<span class="w"> </span>share<span class="w"> </span>it<span class="w"> </span>with<span class="w"> </span>me?
</span></code></pre></div>
</div>
</div>
</div>
<div class="admonition tip">
<p class="admonition-title">📖 Para aprender más</p>
<p>El <code>("placeholder", "{chat_history}")</code> en el <code>ChatPromptTemplate</code> es un componente clave del sistema de plantillas de prompt de LangChain, utilizado para definir un espacio en el prompt donde se insertará una secuencia de mensajes (por ejemplo, el historial de la conversación).</p>
<div class="tabbed-set tabbed-alternate" data-tabs="6:1"><input checked="checked" id="__tabbed_6_1" name="__tabbed_6" type="radio" /><div class="tabbed-labels"><label for="__tabbed_6_1">Detalles</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<p>message type and its content or template.</p>
<p>Tipos comunes de mensajes incluyen:
- <code>("system", "...")</code>: Un mensaje del sistema que define el rol o las instrucciones del asistente.
- <code>("human", "...")</code>: Un mensaje de entrada del usuario.
- <code>("ai", "...")</code>: Un mensaje de respuesta del asistente.
- <code>("placeholder", "{variable_name}")</code>: Un marcador de posición para una secuencia de mensajes o datos que se proporcionarán más tarde.</p>
<p>La tupla <code>("placeholder", "{chat_history}")</code> indica que la variable llamada <code>chat_history</code> contendrá una lista de mensajes (por ejemplo, <code>SystemMessage</code>, <code>HumanMessage</code>, <code>AIMessage</code>) que se insertarán en esa posición en el prompt.</p>
</div>
</div>
</div>
</div>
<p>Esta cadena procesa una lista única de mensajes como contexto, pero no es propiamente una memoria de la conversación, ya que es fija. Idealmente, queremos que la memoria sea persistente, es decir, que recuerde un cierto número de interacciones. Para este propósito, podemos integrar <code>RunnableWithMessageHistory</code> a nuestra cadena para almacenar el historial de la conversación.</p>
<h2 id="anadiendo-memoria-persistente-para-chatbots">Añadiendo Memoria Persistente para Chatbots</h2>
<p>En el ejemplo anterior, la cadena procesa una lista plana de mensajes sin mantener el estado entre invocaciones:</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-24-1"><a id="__codelineno-24-1" name="__codelineno-24-1" href="#__codelineno-24-1"></a><span class="n">chain</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="__span-24-2"><a id="__codelineno-24-2" name="__codelineno-24-2" href="#__codelineno-24-2"></a>    <span class="n">RunnableLambda</span><span class="p">(</span><span class="n">trim_chat_history</span><span class="p">)</span>  <span class="c1"># Recorta los mensajes directamente</span>
</span><span id="__span-24-3"><a id="__codelineno-24-3" name="__codelineno-24-3" href="#__codelineno-24-3"></a>    <span class="o">|</span> <span class="n">prompt</span>  <span class="c1"># Formatea los mensajes recortados en el prompt</span>
</span><span id="__span-24-4"><a id="__codelineno-24-4" name="__codelineno-24-4" href="#__codelineno-24-4"></a>    <span class="o">|</span> <span class="n">llm</span>  <span class="c1"># Genera una respuesta usando el LLM</span>
</span><span id="__span-24-5"><a id="__codelineno-24-5" name="__codelineno-24-5" href="#__codelineno-24-5"></a><span class="p">)</span>
</span></code></pre></div>
<p>Para añadir memoria persistente, envolveremos esta cadena con <code>RunnableWithMessageHistory</code>. Al implementar <code>RunnableWithMessageHistory</code> en LCEL, necesitamos definir una pipeline de base, y esta a su vez se define a partir de una plantilla de prompt y un LLM. Comencemos definiendo un nuevo prompt template; en este caso, usaremos <code>from_message</code>, para lo cual importaremos los módulos:</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-25-1"><a id="__codelineno-25-1" name="__codelineno-25-1" href="#__codelineno-25-1"></a><span class="kn">from</span><span class="w"> </span><span class="nn">langchain.prompts</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
</span><span id="__span-25-2"><a id="__codelineno-25-2" name="__codelineno-25-2" href="#__codelineno-25-2"></a>    <span class="n">SystemMessagePromptTemplate</span><span class="p">,</span> 
</span><span id="__span-25-3"><a id="__codelineno-25-3" name="__codelineno-25-3" href="#__codelineno-25-3"></a>    <span class="n">HumanMessagePromptTemplate</span><span class="p">,</span>
</span><span id="__span-25-4"><a id="__codelineno-25-4" name="__codelineno-25-4" href="#__codelineno-25-4"></a>    <span class="n">MessagesPlaceholder</span><span class="p">,</span>
</span><span id="__span-25-5"><a id="__codelineno-25-5" name="__codelineno-25-5" href="#__codelineno-25-5"></a>    <span class="n">ChatPromptTemplate</span>
</span><span id="__span-25-6"><a id="__codelineno-25-6" name="__codelineno-25-6" href="#__codelineno-25-6"></a><span class="p">)</span>
</span><span id="__span-25-7"><a id="__codelineno-25-7" name="__codelineno-25-7" href="#__codelineno-25-7"></a><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_openai</span><span class="w"> </span><span class="kn">import</span> <span class="n">ChatOpenAI</span>
</span><span id="__span-25-8"><a id="__codelineno-25-8" name="__codelineno-25-8" href="#__codelineno-25-8"></a><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.messages</span><span class="w"> </span><span class="kn">import</span> <span class="n">AIMessage</span><span class="p">,</span> <span class="n">HumanMessage</span>
</span><span id="__span-25-9"><a id="__codelineno-25-9" name="__codelineno-25-9" href="#__codelineno-25-9"></a><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.chat_history</span><span class="w"> </span><span class="kn">import</span> <span class="n">InMemoryChatMessageHistory</span>
</span><span id="__span-25-10"><a id="__codelineno-25-10" name="__codelineno-25-10" href="#__codelineno-25-10"></a><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.runnables.history</span><span class="w"> </span><span class="kn">import</span> <span class="n">RunnableWithMessageHistory</span>
</span></code></pre></div>
<p>Dividiremos el proceso en los siguientes pasos:</p>
<ol>
<li>
<p><strong>Crear la pipeline base.</strong> Como lo hemos hecho antes, definimos el prompt y un LLM. Por ejemplo:</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-26-1"><a id="__codelineno-26-1" name="__codelineno-26-1" href="#__codelineno-26-1"></a><span class="c1"># Definir el prompt del sistema al estilo de Don Quijote</span>
</span><span id="__span-26-2"><a id="__codelineno-26-2" name="__codelineno-26-2" href="#__codelineno-26-2"></a><span class="n">system_prompt</span> <span class="o">=</span> <span class="s2">&quot;Eres un asistente útil que responde en una sola oración en español, al estilo de Don Quijote de la Mancha.&quot;</span>
</span><span id="__span-26-3"><a id="__codelineno-26-3" name="__codelineno-26-3" href="#__codelineno-26-3"></a>
</span><span id="__span-26-4"><a id="__codelineno-26-4" name="__codelineno-26-4" href="#__codelineno-26-4"></a><span class="c1"># Crear la plantilla de prompt para el chat</span>
</span><span id="__span-26-5"><a id="__codelineno-26-5" name="__codelineno-26-5" href="#__codelineno-26-5"></a><span class="n">prompt_template</span> <span class="o">=</span> <span class="n">ChatPromptTemplate</span><span class="o">.</span><span class="n">from_messages</span><span class="p">([</span>
</span><span id="__span-26-6"><a id="__codelineno-26-6" name="__codelineno-26-6" href="#__codelineno-26-6"></a>    <span class="n">SystemMessagePromptTemplate</span><span class="o">.</span><span class="n">from_template</span><span class="p">(</span><span class="n">system_prompt</span><span class="p">),</span>
</span><span id="__span-26-7"><a id="__codelineno-26-7" name="__codelineno-26-7" href="#__codelineno-26-7"></a>    <span class="n">MessagesPlaceholder</span><span class="p">(</span><span class="n">variable_name</span><span class="o">=</span><span class="s2">&quot;history&quot;</span><span class="p">),</span>
</span><span id="__span-26-8"><a id="__codelineno-26-8" name="__codelineno-26-8" href="#__codelineno-26-8"></a>    <span class="n">HumanMessagePromptTemplate</span><span class="o">.</span><span class="n">from_template</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{query}</span><span class="s2">&quot;</span><span class="p">),</span>
</span><span id="__span-26-9"><a id="__codelineno-26-9" name="__codelineno-26-9" href="#__codelineno-26-9"></a><span class="p">])</span>
</span><span id="__span-26-10"><a id="__codelineno-26-10" name="__codelineno-26-10" href="#__codelineno-26-10"></a>
</span><span id="__span-26-11"><a id="__codelineno-26-11" name="__codelineno-26-11" href="#__codelineno-26-11"></a><span class="c1"># Inicializar el LLM</span>
</span><span id="__span-26-12"><a id="__codelineno-26-12" name="__codelineno-26-12" href="#__codelineno-26-12"></a><span class="n">llm</span> <span class="o">=</span> <span class="n">ChatOpenAI</span><span class="p">(</span><span class="n">model_name</span><span class="o">=</span><span class="s2">&quot;gpt-3.5-turbo&quot;</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span><span id="__span-26-13"><a id="__codelineno-26-13" name="__codelineno-26-13" href="#__codelineno-26-13"></a>
</span><span id="__span-26-14"><a id="__codelineno-26-14" name="__codelineno-26-14" href="#__codelineno-26-14"></a><span class="c1"># Crear la pipeline base</span>
</span><span id="__span-26-15"><a id="__codelineno-26-15" name="__codelineno-26-15" href="#__codelineno-26-15"></a><span class="n">pipeline</span> <span class="o">=</span> <span class="n">prompt_template</span> <span class="o">|</span> <span class="n">llm</span>
</span></code></pre></div>
<p>Esta es nuestra pipeline base sobre la cual añadiremos memoria usando <code>RunnableWithMessageHistory</code> y de esta forma retener el historial de conversaciones. Necesitaremos configurar un almacén de historial de chat, puede ser un simple diccionario:</p>
</li>
<li>
<p><strong>Configurar la Gestión del Historial de Chat.</strong> Para añadir memoria, necesitamos un mecanismo para almacenar y recuperar el historial de conversaciones para diferentes sesiones de usuario. Nuestro <code>RunnableWithMessageHistory</code> requiere que nuestra pipeline esté envuelta en un objeto <code>RunnableWithMessageHistory</code>. Este objeto necesita algunos parámetros de entrada. Uno de ellos es <code>get_session_history</code>, que requiere una función que devuelva un objeto <code>ChatMessageHistory</code> basado en un ID de sesión. Definimos esta función nosotros mismos:</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-27-1"><a id="__codelineno-27-1" name="__codelineno-27-1" href="#__codelineno-27-1"></a><span class="c1"># Definir el historial de chat</span>
</span><span id="__span-27-2"><a id="__codelineno-27-2" name="__codelineno-27-2" href="#__codelineno-27-2"></a><span class="n">chat_history</span> <span class="o">=</span> <span class="p">[</span>
</span><span id="__span-27-3"><a id="__codelineno-27-3" name="__codelineno-27-3" href="#__codelineno-27-3"></a>    <span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">&quot;Saludos, mi nombre es Juan, ¿cómo te hallas?&quot;</span><span class="p">),</span>
</span><span id="__span-27-4"><a id="__codelineno-27-4" name="__codelineno-27-4" href="#__codelineno-27-4"></a>    <span class="n">AIMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">&quot;En verdad, me hallo en buen estado, y estoy presto a servirte, ¿cuál es tu deseo?&quot;</span><span class="p">),</span>
</span><span id="__span-27-5"><a id="__codelineno-27-5" name="__codelineno-27-5" href="#__codelineno-27-5"></a>    <span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">&quot;¿Puedes hablarme del tiempo que hoy nos acompaña?&quot;</span><span class="p">),</span>
</span><span id="__span-27-6"><a id="__codelineno-27-6" name="__codelineno-27-6" href="#__codelineno-27-6"></a>    <span class="n">AIMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">&quot;Ciertamente, el día se muestra soleado con un calor apacible que alcanza los 24 grados.&quot;</span><span class="p">),</span>
</span><span id="__span-27-7"><a id="__codelineno-27-7" name="__codelineno-27-7" href="#__codelineno-27-7"></a>    <span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">&quot;¿Cuál es la capital de Francia?&quot;</span><span class="p">),</span>
</span><span id="__span-27-8"><a id="__codelineno-27-8" name="__codelineno-27-8" href="#__codelineno-27-8"></a>    <span class="n">AIMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">&quot;La capital de Francia es París, noble villa de gran renombre.&quot;</span><span class="p">),</span>
</span><span id="__span-27-9"><a id="__codelineno-27-9" name="__codelineno-27-9" href="#__codelineno-27-9"></a><span class="p">]</span>
</span><span id="__span-27-10"><a id="__codelineno-27-10" name="__codelineno-27-10" href="#__codelineno-27-10"></a>
</span><span id="__span-27-11"><a id="__codelineno-27-11" name="__codelineno-27-11" href="#__codelineno-27-11"></a><span class="c1"># Configurar la gestión del historial de chat con un historial predefinido</span>
</span><span id="__span-27-12"><a id="__codelineno-27-12" name="__codelineno-27-12" href="#__codelineno-27-12"></a><span class="n">chat_dict</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="__span-27-13"><a id="__codelineno-27-13" name="__codelineno-27-13" href="#__codelineno-27-13"></a><span class="n">session_id</span> <span class="o">=</span> <span class="s2">&quot;id_123&quot;</span>
</span><span id="__span-27-14"><a id="__codelineno-27-14" name="__codelineno-27-14" href="#__codelineno-27-14"></a><span class="n">chat_dict</span><span class="p">[</span><span class="n">session_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">InMemoryChatMessageHistory</span><span class="p">()</span>
</span><span id="__span-27-15"><a id="__codelineno-27-15" name="__codelineno-27-15" href="#__codelineno-27-15"></a><span class="n">chat_dict</span><span class="p">[</span><span class="n">session_id</span><span class="p">]</span><span class="o">.</span><span class="n">add_messages</span><span class="p">(</span><span class="n">chat_history</span><span class="p">)</span>
</span><span id="__span-27-16"><a id="__codelineno-27-16" name="__codelineno-27-16" href="#__codelineno-27-16"></a>
</span><span id="__span-27-17"><a id="__codelineno-27-17" name="__codelineno-27-17" href="#__codelineno-27-17"></a><span class="k">def</span><span class="w"> </span><span class="nf">get_chat_history</span><span class="p">(</span><span class="n">session_id</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">InMemoryChatMessageHistory</span><span class="p">:</span>
</span><span id="__span-27-18"><a id="__codelineno-27-18" name="__codelineno-27-18" href="#__codelineno-27-18"></a>    <span class="k">if</span> <span class="n">session_id</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">chat_dict</span><span class="p">:</span>
</span><span id="__span-27-19"><a id="__codelineno-27-19" name="__codelineno-27-19" href="#__codelineno-27-19"></a>        <span class="n">chat_dict</span><span class="p">[</span><span class="n">session_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">InMemoryChatMessageHistory</span><span class="p">()</span>
</span><span id="__span-27-20"><a id="__codelineno-27-20" name="__codelineno-27-20" href="#__codelineno-27-20"></a>    <span class="k">return</span> <span class="n">chat_dict</span><span class="p">[</span><span class="n">session_id</span><span class="p">]</span>
</span></code></pre></div>
<p><code>chat_dict</code> es un diccionario que asigna IDs de sesión a objetos <code>InMemoryChatMessageHistory</code>, los cuales almacenan secuencias de mensajes (por ejemplo, mensajes del usuario y respuestas del asistente).</p>
<p><code>get_chat_history</code> es una función que:
- Toma un <code>session_id</code> (por ejemplo, "id_123") como entrada.
- Verifica si existe un historial de chat para ese <code>session_id</code> en <code>chat_dict</code>.
- Si no, crea un nuevo <code>InMemoryChatMessageHistory</code> y lo almacena en <code>chat_dict</code>.
- Luego, devuelve el objeto de historial de chat correspondiente.</p>
<div class="admonition tip">
<p class="admonition-title">Para aprender más</p>
<p>Esta configuración asegura que cada sesión de usuario tenga su propio historial de chat aislado, permitiendo que múltiples usuarios interactúen simultáneamente sin mezclar conversaciones. Sin embargo, para entornos de producción, el almacenamiento en memoria no es ideal debido a su volatilidad (los datos se pierden al reiniciar la aplicación). Las alternativas incluyen:
- <code>RedisChatMessageHistory</code> para almacenamiento escalable y persistente con Redis.
- <code>PostgresChatMessageHistory</code> para almacenamiento respaldado por bases de datos.
- <code>FileChatMessageHistory</code> para persistencia basada en archivos.</p>
<p>Por ejemplo, para usar Redis, debes usar el paquete Redis (<code>%pip install --upgrade --quiet redis</code>), iniciar un servidor Redis (por ejemplo, a través de Docker) y definir un <code>get_message_history</code> con <code>RedisChatMessageHistory</code>, como se muestra en la documentación de LangChain sobre Integraciones de Memoria.</p>
</div>
</li>
<li>
<p><strong>Envolver la Pipeline Base con <code>RunnableWithMessageHistory</code>.</strong> Para añadir memoria, envolvemos la pipeline base con <code>RunnableWithMessageHistory</code>, que gestiona el historial de chat para el <code>Runnable</code>:</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-28-1"><a id="__codelineno-28-1" name="__codelineno-28-1" href="#__codelineno-28-1"></a><span class="c1"># Envolver la pipeline con RunnableWithMessageHistory</span>
</span><span id="__span-28-2"><a id="__codelineno-28-2" name="__codelineno-28-2" href="#__codelineno-28-2"></a><span class="n">pipeline_with_history</span> <span class="o">=</span> <span class="n">RunnableWithMessageHistory</span><span class="p">(</span>
</span><span id="__span-28-3"><a id="__codelineno-28-3" name="__codelineno-28-3" href="#__codelineno-28-3"></a>    <span class="n">pipeline</span><span class="p">,</span>
</span><span id="__span-28-4"><a id="__codelineno-28-4" name="__codelineno-28-4" href="#__codelineno-28-4"></a>    <span class="n">get_session_history</span><span class="o">=</span><span class="n">get_chat_history</span><span class="p">,</span>
</span><span id="__span-28-5"><a id="__codelineno-28-5" name="__codelineno-28-5" href="#__codelineno-28-5"></a>    <span class="n">input_messages_key</span><span class="o">=</span><span class="s2">&quot;query&quot;</span><span class="p">,</span>
</span><span id="__span-28-6"><a id="__codelineno-28-6" name="__codelineno-28-6" href="#__codelineno-28-6"></a>    <span class="n">history_messages_key</span><span class="o">=</span><span class="s2">&quot;history&quot;</span>
</span><span id="__span-28-7"><a id="__codelineno-28-7" name="__codelineno-28-7" href="#__codelineno-28-7"></a><span class="p">)</span>
</span></code></pre></div>
<p>Aquí, <code>pipeline</code> es el <code>Runnable</code> base que vamos a envolver (en este caso, <code>prompt_template | llm</code>). <code>get_session_history</code> es la función (<code>get_chat_history</code>) que devuelve el historial de chat para un ID de sesión dado. Esta función debe tomar un <code>session_id</code> y devolver una instancia de <code>BaseChatMessageHistory</code>. <code>input_messages_key</code> especifica la clave en el diccionario de entrada que contiene el mensaje actual del usuario. En el código, es "query", lo que significa que la entrada espera algo como <code>{"query": "What is my name again?"}</code>. <code>history_messages_key</code> especifica la clave donde el historial de la conversación debe ser inyectado en la entrada. En el código, es "history", lo que significa que el historial (una lista de objetos <code>BaseMessage</code>) se añade bajo esta variable de entrada.</p>
<p>Al invocar, <code>RunnableWithMessageHistory</code> recupera el historial de chat para el ID de sesión usando <code>get_chat_history</code>. Aumenta el diccionario de entrada añadiendo el historial bajo la clave <code>history_messages_key</code> (por ejemplo, <code>{"query": "What is my name again?", "history": [...]}</code>). La entrada aumentada se pasa a la pipeline base, que incluye el historial en el prompt a través de <code>MessagesPlaceholder</code>. Después de que la pipeline genera una respuesta, el historial de chat se actualiza con el nuevo mensaje del usuario y la respuesta del asistente.</p>
</li>
</ol>
<p>Y listo, nuestro historial de chat ahora se memorizará y recuperará cada vez que invoquemos nuestro runnable con el mismo ID de sesión.</p>
<div class="tabbed-set tabbed-alternate" data-tabs="7:2"><input checked="checked" id="__tabbed_7_1" name="__tabbed_7" type="radio" /><input id="__tabbed_7_2" name="__tabbed_7" type="radio" /><div class="tabbed-labels"><label for="__tabbed_7_1">Código</label><label for="__tabbed_7_2">Salida</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<div class="language-python highlight"><pre><span></span><code><span id="__span-29-1"><a id="__codelineno-29-1" name="__codelineno-29-1" href="#__codelineno-29-1"></a><span class="c1"># Invocar la pipeline para demostrar la memoria</span>
</span><span id="__span-29-2"><a id="__codelineno-29-2" name="__codelineno-29-2" href="#__codelineno-29-2"></a><span class="n">result1</span> <span class="o">=</span> <span class="n">pipeline_with_history</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span>
</span><span id="__span-29-3"><a id="__codelineno-29-3" name="__codelineno-29-3" href="#__codelineno-29-3"></a>    <span class="p">{</span><span class="s2">&quot;query&quot;</span><span class="p">:</span> <span class="s2">&quot;¿Cuál es mi nombre?&quot;</span><span class="p">},</span>
</span><span id="__span-29-4"><a id="__codelineno-29-4" name="__codelineno-29-4" href="#__codelineno-29-4"></a>    <span class="n">config</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;session_id&quot;</span><span class="p">:</span> <span class="s2">&quot;id_123&quot;</span><span class="p">}</span>
</span><span id="__span-29-5"><a id="__codelineno-29-5" name="__codelineno-29-5" href="#__codelineno-29-5"></a><span class="p">)</span>
</span><span id="__span-29-6"><a id="__codelineno-29-6" name="__codelineno-29-6" href="#__codelineno-29-6"></a><span class="nb">print</span><span class="p">(</span><span class="n">result1</span><span class="o">.</span><span class="n">content</span><span class="p">)</span>  <span class="c1"># Esperado: &quot;Vuestro nombre, según me habéis dicho, es Juan.&quot;</span>
</span><span id="__span-29-7"><a id="__codelineno-29-7" name="__codelineno-29-7" href="#__codelineno-29-7"></a>
</span><span id="__span-29-8"><a id="__codelineno-29-8" name="__codelineno-29-8" href="#__codelineno-29-8"></a><span class="n">result2</span> <span class="o">=</span> <span class="n">pipeline_with_history</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span>
</span><span id="__span-29-9"><a id="__codelineno-29-9" name="__codelineno-29-9" href="#__codelineno-29-9"></a>    <span class="p">{</span><span class="s2">&quot;query&quot;</span><span class="p">:</span> <span class="s2">&quot;¿Qué más sabes de Francia?&quot;</span><span class="p">},</span>
</span><span id="__span-29-10"><a id="__codelineno-29-10" name="__codelineno-29-10" href="#__codelineno-29-10"></a>    <span class="n">config</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;session_id&quot;</span><span class="p">:</span> <span class="s2">&quot;id_123&quot;</span><span class="p">}</span>
</span><span id="__span-29-11"><a id="__codelineno-29-11" name="__codelineno-29-11" href="#__codelineno-29-11"></a><span class="p">)</span>
</span><span id="__span-29-12"><a id="__codelineno-29-12" name="__codelineno-29-12" href="#__codelineno-29-12"></a><span class="nb">print</span><span class="p">(</span><span class="n">result2</span><span class="o">.</span><span class="n">content</span><span class="p">)</span>
</span></code></pre></div>
</div>
<div class="tabbed-block">
<div class="language-bash highlight"><pre><span></span><code><span id="__span-30-1"><a id="__codelineno-30-1" name="__codelineno-30-1" href="#__codelineno-30-1"></a>Vuestro<span class="w"> </span>nombre<span class="w"> </span>es<span class="w"> </span>Juan,<span class="w"> </span>valeroso<span class="w"> </span>caballero<span class="w"> </span>que<span class="w"> </span>busca<span class="w"> </span>conocimiento<span class="w"> </span>y<span class="w"> </span>respuestas.
</span><span id="__span-30-2"><a id="__codelineno-30-2" name="__codelineno-30-2" href="#__codelineno-30-2"></a>Francia<span class="w"> </span>es<span class="w"> </span>tierra<span class="w"> </span>de<span class="w"> </span>exquisita<span class="w"> </span>gastronomía,<span class="w"> </span>arte<span class="w"> </span>refinado<span class="w"> </span>y<span class="w"> </span>hermosos<span class="w"> </span>paisajes,<span class="w"> </span>dignos<span class="w"> </span>de<span class="w"> </span>ser<span class="w"> </span>explorados<span class="w"> </span>y<span class="w"> </span>admirados.
</span></code></pre></div>
</div>
</div>
</div>
<p>Nuestro chat ahora tiene la capacidad de recordar todas las interacciones. Vemos:</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-31-1"><a id="__codelineno-31-1" name="__codelineno-31-1" href="#__codelineno-31-1"></a><span class="n">result2</span> <span class="o">=</span> <span class="n">pipeline_with_history</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span>
</span><span id="__span-31-2"><a id="__codelineno-31-2" name="__codelineno-31-2" href="#__codelineno-31-2"></a>    <span class="p">{</span><span class="s2">&quot;query&quot;</span><span class="p">:</span> <span class="s2">&quot;El nombre de mi madre es Maria&quot;</span><span class="p">},</span>
</span><span id="__span-31-3"><a id="__codelineno-31-3" name="__codelineno-31-3" href="#__codelineno-31-3"></a>    <span class="n">config</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;session_id&quot;</span><span class="p">:</span> <span class="s2">&quot;id_123&quot;</span><span class="p">}</span>
</span><span id="__span-31-4"><a id="__codelineno-31-4" name="__codelineno-31-4" href="#__codelineno-31-4"></a><span class="p">)</span>
</span><span id="__span-31-5"><a id="__codelineno-31-5" name="__codelineno-31-5" href="#__codelineno-31-5"></a><span class="nb">print</span><span class="p">(</span><span class="n">result2</span><span class="o">.</span><span class="n">content</span><span class="p">)</span> 
</span><span id="__span-31-6"><a id="__codelineno-31-6" name="__codelineno-31-6" href="#__codelineno-31-6"></a>
</span><span id="__span-31-7"><a id="__codelineno-31-7" name="__codelineno-31-7" href="#__codelineno-31-7"></a><span class="n">result2</span> <span class="o">=</span> <span class="n">pipeline_with_history</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span>
</span><span id="__span-31-8"><a id="__codelineno-31-8" name="__codelineno-31-8" href="#__codelineno-31-8"></a>    <span class="p">{</span><span class="s2">&quot;query&quot;</span><span class="p">:</span> <span class="s2">&quot;¿Quién es Maria en mi vida?&quot;</span><span class="p">},</span>
</span><span id="__span-31-9"><a id="__codelineno-31-9" name="__codelineno-31-9" href="#__codelineno-31-9"></a>    <span class="n">config</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;session_id&quot;</span><span class="p">:</span> <span class="s2">&quot;id_123&quot;</span><span class="p">}</span>
</span><span id="__span-31-10"><a id="__codelineno-31-10" name="__codelineno-31-10" href="#__codelineno-31-10"></a><span class="p">)</span>
</span><span id="__span-31-11"><a id="__codelineno-31-11" name="__codelineno-31-11" href="#__codelineno-31-11"></a>
</span><span id="__span-31-12"><a id="__codelineno-31-12" name="__codelineno-31-12" href="#__codelineno-31-12"></a><span class="n">chat</span> <span class="o">=</span> <span class="n">get_chat_history</span><span class="p">(</span><span class="s1">&#39;id_123&#39;</span><span class="p">)</span>
</span><span id="__span-31-13"><a id="__codelineno-31-13" name="__codelineno-31-13" href="#__codelineno-31-13"></a>
</span><span id="__span-31-14"><a id="__codelineno-31-14" name="__codelineno-31-14" href="#__codelineno-31-14"></a><span class="k">for</span> <span class="n">msg</span> <span class="ow">in</span> <span class="n">chat</span><span class="o">.</span><span class="n">messages</span><span class="p">:</span>
</span><span id="__span-31-15"><a id="__codelineno-31-15" name="__codelineno-31-15" href="#__codelineno-31-15"></a>    <span class="nb">print</span><span class="p">(</span><span class="n">msg</span><span class="o">.</span><span class="n">content</span><span class="p">)</span>
</span></code></pre></div>
<div class="tabbed-set tabbed-alternate" data-tabs="8:1"><input checked="checked" id="__tabbed_8_1" name="__tabbed_8" type="radio" /><div class="tabbed-labels"><label for="__tabbed_8_1">Salida</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<div class="language-bash highlight"><pre><span></span><code><span id="__span-32-1"><a id="__codelineno-32-1" name="__codelineno-32-1" href="#__codelineno-32-1"></a>Saludos,<span class="w"> </span>mi<span class="w"> </span>nombre<span class="w"> </span>es<span class="w"> </span>Juan,<span class="w"> </span>¿cómo<span class="w"> </span>te<span class="w"> </span>hallas?
</span><span id="__span-32-2"><a id="__codelineno-32-2" name="__codelineno-32-2" href="#__codelineno-32-2"></a>En<span class="w"> </span>verdad,<span class="w"> </span>me<span class="w"> </span>hallo<span class="w"> </span>en<span class="w"> </span>buen<span class="w"> </span>estado,<span class="w"> </span>y<span class="w"> </span>estoy<span class="w"> </span>presto<span class="w"> </span>a<span class="w"> </span>servirte,<span class="w"> </span>¿cuál<span class="w"> </span>es<span class="w"> </span>tu<span class="w"> </span>deseo?
</span><span id="__span-32-3"><a id="__codelineno-32-3" name="__codelineno-32-3" href="#__codelineno-32-3"></a>¿Puedes<span class="w"> </span>hablarme<span class="w"> </span>del<span class="w"> </span>tiempo<span class="w"> </span>que<span class="w"> </span>hoy<span class="w"> </span>nos<span class="w"> </span>acompaña?
</span><span id="__span-32-4"><a id="__codelineno-32-4" name="__codelineno-32-4" href="#__codelineno-32-4"></a>Ciertamente,<span class="w"> </span>el<span class="w"> </span>día<span class="w"> </span>se<span class="w"> </span>muestra<span class="w"> </span>soleado<span class="w"> </span>con<span class="w"> </span>un<span class="w"> </span>calor<span class="w"> </span>apacible<span class="w"> </span>que<span class="w"> </span>alcanza<span class="w"> </span>los<span class="w"> </span><span class="m">24</span><span class="w"> </span>grados.
</span><span id="__span-32-5"><a id="__codelineno-32-5" name="__codelineno-32-5" href="#__codelineno-32-5"></a>¿Cuál<span class="w"> </span>es<span class="w"> </span>la<span class="w"> </span>capital<span class="w"> </span>de<span class="w"> </span>Francia?
</span><span id="__span-32-6"><a id="__codelineno-32-6" name="__codelineno-32-6" href="#__codelineno-32-6"></a>La<span class="w"> </span>capital<span class="w"> </span>de<span class="w"> </span>Francia<span class="w"> </span>es<span class="w"> </span>París,<span class="w"> </span>noble<span class="w"> </span>villa<span class="w"> </span>de<span class="w"> </span>gran<span class="w"> </span>renombre.
</span><span id="__span-32-7"><a id="__codelineno-32-7" name="__codelineno-32-7" href="#__codelineno-32-7"></a>¿Cuál<span class="w"> </span>es<span class="w"> </span>mi<span class="w"> </span>nombre?
</span><span id="__span-32-8"><a id="__codelineno-32-8" name="__codelineno-32-8" href="#__codelineno-32-8"></a>Vuestro<span class="w"> </span>nombre<span class="w"> </span>es<span class="w"> </span>Juan,<span class="w"> </span>valeroso<span class="w"> </span>caballero<span class="w"> </span>que<span class="w"> </span>busca<span class="w"> </span>conocimiento<span class="w"> </span>y<span class="w"> </span>respuestas.
</span><span id="__span-32-9"><a id="__codelineno-32-9" name="__codelineno-32-9" href="#__codelineno-32-9"></a>¿Qué<span class="w"> </span>más<span class="w"> </span>sabes<span class="w"> </span>de<span class="w"> </span>Francia?
</span><span id="__span-32-10"><a id="__codelineno-32-10" name="__codelineno-32-10" href="#__codelineno-32-10"></a>Francia<span class="w"> </span>es<span class="w"> </span>tierra<span class="w"> </span>de<span class="w"> </span>exquisita<span class="w"> </span>gastronomía,<span class="w"> </span>arte<span class="w"> </span>refinado<span class="w"> </span>y<span class="w"> </span>hermosos<span class="w"> </span>paisajes,<span class="w"> </span>dignos<span class="w"> </span>de<span class="w"> </span>ser<span class="w"> </span>explorados<span class="w"> </span>y<span class="w"> </span>admirados.
</span><span id="__span-32-11"><a id="__codelineno-32-11" name="__codelineno-32-11" href="#__codelineno-32-11"></a>El<span class="w"> </span>nombre<span class="w"> </span>de<span class="w"> </span>mi<span class="w"> </span>madre<span class="w"> </span>es<span class="w"> </span>Maria
</span><span id="__span-32-12"><a id="__codelineno-32-12" name="__codelineno-32-12" href="#__codelineno-32-12"></a>Vuestra<span class="w"> </span>madre,<span class="w"> </span>María,<span class="w"> </span>posee<span class="w"> </span>un<span class="w"> </span>nombre<span class="w"> </span>tan<span class="w"> </span>puro<span class="w"> </span>y<span class="w"> </span>bello<span class="w"> </span>como<span class="w"> </span>el<span class="w"> </span>de<span class="w"> </span>la<span class="w"> </span>Virgen<span class="w"> </span>Santa.
</span><span id="__span-32-13"><a id="__codelineno-32-13" name="__codelineno-32-13" href="#__codelineno-32-13"></a>¿Quién<span class="w"> </span>es<span class="w"> </span>Maria?
</span><span id="__span-32-14"><a id="__codelineno-32-14" name="__codelineno-32-14" href="#__codelineno-32-14"></a>María<span class="w"> </span>es<span class="w"> </span>un<span class="w"> </span>nombre<span class="w"> </span>común<span class="w"> </span>entre<span class="w"> </span>las<span class="w"> </span>mujeres,<span class="w"> </span>pero<span class="w"> </span>también<span class="w"> </span>es<span class="w"> </span>el<span class="w"> </span>nombre<span class="w"> </span>de<span class="w"> </span>la<span class="w"> </span>madre<span class="w"> </span>de<span class="w"> </span>Jesús,<span class="w"> </span>la<span class="w"> </span>Virgen<span class="w"> </span>María,<span class="w"> </span>figura<span class="w"> </span>importante<span class="w"> </span>en<span class="w"> </span>la<span class="w"> </span>religión<span class="w"> </span>católica.
</span><span id="__span-32-15"><a id="__codelineno-32-15" name="__codelineno-32-15" href="#__codelineno-32-15"></a>¿Quién<span class="w"> </span>es<span class="w"> </span>Maria<span class="w"> </span>en<span class="w"> </span>mi<span class="w"> </span>vida?
</span><span id="__span-32-16"><a id="__codelineno-32-16" name="__codelineno-32-16" href="#__codelineno-32-16"></a>María,<span class="w"> </span>en<span class="w"> </span>vuestra<span class="w"> </span>vida,<span class="w"> </span>es<span class="w"> </span>la<span class="w"> </span>mujer<span class="w"> </span>que<span class="w"> </span>os<span class="w"> </span>dio<span class="w"> </span>la<span class="w"> </span>vida,<span class="w"> </span>os<span class="w"> </span>cuidó<span class="w"> </span>con<span class="w"> </span>amor<span class="w"> </span>y<span class="w"> </span>os<span class="w"> </span>guió<span class="w"> </span>en<span class="w"> </span>vuestro<span class="w"> </span>camino,<span class="w"> </span>como<span class="w"> </span>una<span class="w"> </span>luz<span class="w"> </span>en<span class="w"> </span>la<span class="w"> </span>oscuridad.
</span></code></pre></div>
</div>
</div>
</div>
<div class="admonition tip">
<p class="admonition-title">📖 Para aprender más</p>
<p>Hasta aquí hemos reproducido el comportamiento de la clase <code>ConversationBufferMemory</code> que describimos al principio. Sin embargo, esta clase será deprecada en las versiones futuras de LangChain. Para ver cómo crear <em>wrappers</em> de los demás tipos de memoria, puedes consultar el siguiente material:
<strong>Artículo</strong>: Introducción a los Tipos de Memoria en LangChain<br />
<strong>URL</strong>: <a href="https://www.aurelio.ai/learn/langchain-memory-types">https://www.aurelio.ai/learn/langchain-memory-types</a></p>
</div>
<p>¡Felicidades por llegar al final del módulo 2! Has aprendido cómo crear cadenas con memoria utilizando el LCEL. Ahora estás en capacidad de crear chatbots funcionales. Te invito a realizar la actividad de aprendizaje para que pongas en práctica lo aprendido.</p>
<p><strong>Glosario</strong></p>
<ul>
<li>
<p><strong>Configurable runnables</strong>: En LangChain, son funciones ejecutables que pueden personalizarse dinámicamente en tiempo de ejecución utilizando un objeto <code>RunnableConfig</code>. Esto permite pasar parámetros como el nombre de la ejecución, etiquetas o metadatos para controlar el comportamiento, como los límites de concurrencia o recursión.</p>
</li>
<li>
<p><strong>Context window</strong>: La cantidad máxima de tokens de entrada (texto, datos, etc.) que un modelo de chat puede procesar en una sola interacción, determinada por la arquitectura del modelo.</p>
</li>
<li>
<p><strong>langchain</strong>: Un paquete de Python que proporciona componentes de alto nivel para construir aplicaciones con modelos de lenguaje, como cadenas preconstruidas y herramientas para tareas comunes.</p>
</li>
<li>
<p><strong>langchain-community</strong>: Una colección de componentes e integraciones contribuidas por la comunidad para LangChain, que extiende su funcionalidad con herramientas de terceros.</p>
</li>
<li>
<p><strong>langchain-core</strong>: El paquete fundamental de LangChain, que contiene interfaces centrales, abstracciones base e implementaciones en memoria para construir cadenas y funciones ejecutables.</p>
</li>
<li>
<p><strong>langgraph</strong>: Una extensión de LangChain para orquestar flujos de trabajo y pipelines complejos, permitiendo una gestión avanzada de estado y procesos de múltiples pasos.</p>
</li>
<li>
<p><strong>langserve</strong>: Una herramienta para desplegar funciones ejecutables de LangChain como endpoints de API REST utilizando FastAPI. Principalmente soporta funciones ejecutables de LangChain, con compatibilidad limitada para LangGraph.</p>
</li>
<li>
<p><strong>Managing chat history</strong>: Métodos y técnicas para almacenar, recuperar y mantener el contexto conversacional a través de múltiples interacciones en una aplicación basada en chat.</p>
</li>
<li>
<p><strong>RunnableConfig</strong>: Un objeto de configuración en LangChain para pasar parámetros de tiempo de ejecución a funciones ejecutables, incluyendo <code>run_name</code>, <code>run_id</code>, etiquetas, metadatos, <code>max_concurrency</code>, <code>recursion_limit</code> y otras configuraciones personalizables.</p>
</li>
</ul>
<h2 id="evidencia-de-aprendizaje">Evidencia de Aprendizaje</h2>
<table>
<thead>
<tr>
<th><strong>Módulo 2</strong></th>
<th><strong>Cadenas y Memoria</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>EA1.</strong></td>
<td>**Generación de Informes de Salud Utilizando Archivos CSV **</td>
</tr>
</tbody>
</table>
<p>En este proyecto practicarás el uso de cadenas para desrrollar un sistema que reliza tareas secuenciales y ramificadas, cargarás los resitado de estas operacones en el bufer de memoria de un chatbot.</p>
<h2 id="instrucciones">Instrucciones</h2>
<p>Descarga el archivo <a href="../../assets/resources/healthcare_report.csv">healthcare_report.csv</a> proporcionado y, usando LECL, desarrolla un sistema capaz de:
1. <strong>Procesar un informe de salud original en español</strong>: lee el informe desde el archivo CSV.
2. <strong>Traducir el informe al inglés</strong>: usa una cadena para traducir el texto.
3. <strong>Resumir el informe traducido</strong>: genera un resumen breve en inglés.
4. <strong>Extraer indicadores clave de salud del resumen</strong>: identifica elementos clave (e.g., síntomas, duración).
5. <strong>Generar un plan de tratamiento basado en los indicadores clave</strong>: propón pasos de tratamiento.
6. <strong>Detectar el idioma original del informe</strong>: determina si el informe original está en español.
7. <strong>Generar una recomendación de seguimiento en el idioma detectado</strong>: devuelve una recomendación en español.</p>
<p>Finalmete carga el infome médico del paciente en la memoria y crea un chat bot que esté en capacidad de responder preguntas sobre el tratamiento indicado. Demuestra su uso con algunas llamas al chat</p>
<hr />
<p>Guarda los documentos con la siguiente nomenclatura:</p>
<ul>
<li><strong>Apellido_Nombre del estudiante.ipynb</strong><br />
<strong>Ejemplo:</strong>  </li>
<li>López_Karla.ipynb</li>
</ul>
<p>Finalmente, haz clic en el botón <strong>Cargar Tarea</strong>, sube tu archivo y presiona el botón <strong>Enviar</strong> para remitirlo a tu profesor con el fin de que lo evalúe y retroalimente. |</p>
<div class="admonition tip">
<p class="admonition-title">📖 Nota</p>
<p>Conoce los criterios de evaluación de esta evidencia de aprendizaje consultando la rúbrica que encontrarás a continuación.</p>
</div>
<table>
<thead>
<tr>
<th><strong>Criterios</strong></th>
<th><strong>Ponderación</strong></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th><strong>Totales</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td><strong>70</strong></td>
<td><strong>50</strong></td>
<td><strong>5</strong></td>
<td><strong>0</strong></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>Calidad de las Soluciones</strong></td>
<td>Las soluciones a los ejercicios son correctas, demostrando una implementación adecuada de los conceptos y técnicas requeridos. El estudiante muestra un dominio completo de los temas abordados.</td>
<td>Aunque las soluciones no son completamente correctas, se observa un entendimiento y aplicación adecuada de los conceptos y técnicas involucradas. Hay evidencia de esfuerzo y comprensión de los temas.</td>
<td>Las soluciones presentadas son en su mayoría incorrectas. Se percibe un intento de resolver los ejercicios, pero hay una falta de comprensión de los conceptos y técnicas esenciales.</td>
<td>No realiza la entrega</td>
<td></td>
<td><strong>70</strong></td>
</tr>
<tr>
<td><strong>Calidad de la entrega</strong></td>
<td>El notebook es claro y fácil de seguir, incluyendo comentarios detallados sobre el funcionamiento del código en las celdas Markdown, lo que facilita la comprensión de las soluciones propuestas.</td>
<td>El notebook no es particularmente fácil de leer, pero aún así incluye comentarios que explican el funcionamiento del código en las celdas Markdown, mostrando un esfuerzo por aclarar la lógica detrás del código.</td>
<td>El notebook carece de comentarios acerca del funcionamiento del código en las celdas Markdown, lo que dificulta la comprensión de las soluciones implementadas.</td>
<td>No realiza la entrega</td>
<td></td>
<td><strong>20</strong></td>
</tr>
<tr>
<td><strong>Tiempo de la entrega</strong></td>
<td>La entrega se realiza a tiempo, cumpliendo con el plazo establecido para la presentación de la actividad.</td>
<td>La entrega se realiza con una semana de atraso. Aunque fuera del plazo original, se considera adecuada para evaluar el trabajo presentado.</td>
<td>La entrega se realiza con más de una semana de atraso, lo que indica un retraso significativo en la presentación de la actividad.</td>
<td>No realiza la entrega</td>
<td></td>
<td><strong>10</strong></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td><strong>Ponderación de la actividad</strong></td>
<td><strong>100 puntos</strong></td>
</tr>
</tbody>
</table>
<h1 id="referencias">Referencias</h1>
<p>Aurelio AI. (s.f.). <em>LangChain Course</em>. Recuperado el 21 de mayo de 2025, de <a href="https://www.aurelio.ai/course/langchain">https://www.aurelio.ai/course/langchain</a></p>
<p>Chase, H., &amp; Ng, A. (2023). <em>LangChain for LLM Application Development</em> [Curso en línea]. DeepLearning.AI. Disponible en <a href="https://www.deeplearning.ai/short-courses/langchain-for-llm-application-development/">https://www.deeplearning.ai/short-courses/langchain-for-llm-application-development/</a></p>
<hr />
<h1 id="lecturas-y-material-complementario">Lecturas y material complementario</h1>
<h2 id="lecturas-recomendadas">📚 Lecturas recomendadas</h2>
<h3 id="titulo-how-trellix-uses-langchain-to-enhance-cybersecurity"><strong>Título:</strong> <em>How Trellix Uses LangChain to Enhance Cybersecurity</em></h3>
<p><strong>Autor:</strong> [LangChain]<br />
<strong>Fecha de recuperación:</strong> 21 de mayo de 2025<br />
<strong>URL:</strong> <a href="https://blog.langchain.dev/customers-trellix/">How Trellix Uses LangChain to Enhance Cybersecurity</a></p>
<h2 id="videos-recomendados">🎥 Videos recomendados</h2>
<h3 id="titulo-langchain-prompts-parsers-and-chaining-for-beginners"><strong>Título:</strong> <em>LangChain: Prompts, Parsers and Chaining | for Beginners</em></h3>
<p><strong>Autor:</strong> [Anub Gupta on Learn4Tarakki]<br />
<strong>URL:</strong> <a href="https://www.youtube.com/watch?v=FHhJYxuIIA0">LangChain: Prompts, Parsers and Chaining | for Beginners</a><br />
Este video ofrece una introducción amigable para principiantes sobre cómo crear plantillas de prompts, utilizar parsers y encadenar componentes en LangChain.</p>
<h3 id="titulo-interrupt-2025-keynote-harrison-chase-langchain">Título: Interrupt 2025 Keynote | Harrison Chase | LangChain</h3>
<p><strong>Autor:</strong> [LangChain]
<strong>URL:</strong> Interrupt 2025 Keynote | Harrison Chase | LangChain</p>
<p>Este video presenta la keynote de Harrison Chase en la conferencia Interrupt 2025 de LangChain, donde se discute la evolución de la ingeniería de agentes y la visión de la compañía para agentes inteligentes. Incluye reflexiones sobre la trayectoria de LangChain y anuncios de nuevas herramientas de desarrollo.</p>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
    
      
      <nav class="md-footer__inner md-grid" aria-label="Footer" >
        
          
          <a href="../../Unidad%201/modulo1/" class="md-footer__link md-footer__link--prev" aria-label="Previous: Módulo 1">
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
            </div>
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Previous
              </span>
              <div class="md-ellipsis">
                Módulo 1
              </div>
            </div>
          </a>
        
        
          
          <a href="../../Unidad3/modulo3/" class="md-footer__link md-footer__link--next" aria-label="Next: Módulo 3">
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Next
              </span>
              <div class="md-ellipsis">
                Módulo 3
              </div>
            </div>
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"/></svg>
            </div>
          </a>
        
      </nav>
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.footer", "content.code.copy"], "search": "../../assets/javascripts/workers/search.f8cc74c7.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.c8b220af.min.js"></script>
      
        <script src="../../javascripts/mathjax.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>